
# week_emp_pipeline.py

from pyspark.sql import SparkSession


def run_week_emp_pipeline():
    """
    Weekly Employee Alerts Pipeline
    - Single pass
    - STRUCT based
    - emp_id + week_start driven
    - Correct week_end handling
    """

    spark = (
        SparkSession.builder
        .appName("WeekEmpPipeline")
        .enableHiveSupport()
        .getOrCreate()
    )

    try:
        print("‚ñ∂ Starting Weekly Employee Pipeline")

        # ------------------------------------------------------------
        # Step 1: Build weekly alerts in ONE SQL
        # ------------------------------------------------------------

        spark.sql("""
            CREATE OR REPLACE TEMP VIEW week_emp_alerts_calc AS
            WITH emp_weeks_to_process AS (
                SELECT DISTINCT
                    emp_id,
                    week_start
                FROM dashboard.emp_reports_weekly
                WHERE updated_on > current_timestamp() - INTERVAL 4 HOURS
                  AND emp_id IS NOT NULL
                  AND week_start IS NOT NULL
            ),

            week_bounds AS (
                SELECT
                    emp_id,
                    week_start,
                    CASE
                        -- current week ‚Üí yesterday
                        WHEN week_start = DATE_TRUNC('WEEK', current_date())
                             THEN DATE_SUB(current_date(), 1)
                        -- past weeks ‚Üí Sunday
                        ELSE DATE_ADD(week_start, 6)
                    END AS week_end
                FROM emp_weeks_to_process
            ),

            week_data AS (
                SELECT
                    d.emp_id,
                    w.week_start,
                    d.day_start,
                    d.alerts.triggered_alerts AS alert_ids
                FROM dashboard.emp_reports_daily d
                JOIN week_bounds w
                  ON d.emp_id = w.emp_id
                 AND d.day_start BETWEEN w.week_start AND w.week_end
                WHERE d.alerts.triggered_alerts IS NOT NULL
                  AND SIZE(d.alerts.triggered_alerts) > 0
            ),

            exploded_data AS (
                SELECT
                    emp_id,
                    week_start,
                    day_start,
                    EXPLODE(alert_ids) AS alert_id
                FROM week_data
            ),

            streak_base AS (
                SELECT
                    emp_id,
                    week_start,
                    alert_id,
                    day_start,
                    DATE_SUB(
                        day_start,
                        ROW_NUMBER() OVER (
                            PARTITION BY emp_id, week_start, alert_id
                            ORDER BY day_start
                        )
                    ) AS grp
                FROM exploded_data
            ),

            streaks AS (
                SELECT
                    emp_id,
                    week_start,
                    alert_id,
                    COUNT(*) AS consecutive_count
                FROM streak_base
                GROUP BY emp_id, week_start, alert_id, grp
            ),

            patterns AS (
                SELECT
                    emp_id,
                    week_start,
                    alert_id,
                    COUNT(DISTINCT day_start) AS affected_days,
                    MAX(consecutive_count) AS max_consecutive
                FROM (
                    SELECT
                        e.emp_id,
                        e.week_start,
                        e.alert_id,
                        e.day_start,
                        s.consecutive_count
                    FROM exploded_data e
                    JOIN streaks s
                      ON e.emp_id = s.emp_id
                     AND e.week_start = s.week_start
                     AND e.alert_id = s.alert_id
                ) t
                GROUP BY emp_id, week_start, alert_id
            ),

            severity_assigned AS (
                SELECT
                    emp_id,
                    week_start,
                    alert_id,
                    affected_days,
                    max_consecutive,
                    CASE
                        WHEN max_consecutive >= 3 THEN 'HIGH'
                        WHEN max_consecutive >= 2 THEN 'MEDIUM'
                        WHEN affected_days >= 1 THEN 'LOW'
                        ELSE NULL
                    END AS severity
                FROM patterns
                WHERE affected_days > 0
            ),

            final_results AS (
                SELECT
                    emp_id,
                    week_start,
                    named_struct(
                        'alert_patterns',
                        collect_list(
                            named_struct(
                                'alert', alert_id,
                                'severity', severity,
                                'affected_days', affected_days,
                                'consecutive_days', max_consecutive
                            )
                        ),
                        'summary',
                        named_struct(
                            'total_alerts', COUNT(*),
                            'calculated_at', current_timestamp()
                        )
                    ) AS alerts,
                    current_timestamp() AS calculated_at
                FROM severity_assigned
                WHERE severity IS NOT NULL
                GROUP BY emp_id, week_start
            )

            SELECT * FROM final_results
        """)

        # ------------------------------------------------------------
        # Step 2: MERGE into weekly table
        # ------------------------------------------------------------

        spark.sql("""
            MERGE INTO dashboard.emp_reports_weekly target
            USING week_emp_alerts_calc source
            ON target.emp_id = source.emp_id
           AND target.week_start = source.week_start
            WHEN MATCHED THEN
              UPDATE SET
                target.alerts = source.alerts,
                target.updated_on = source.calculated_at
            WHEN NOT MATCHED THEN
              INSERT (
                emp_id,
                week_start,
                alerts,
                updated_on
              )
              VALUES (
                source.emp_id,
                source.week_start,
                source.alerts,
                source.calculated_at
              )
        """)

        # ------------------------------------------------------------
        # Step 3: Metrics
        # ------------------------------------------------------------

        processed = spark.sql(
            "SELECT COUNT(*) AS cnt FROM week_emp_alerts_calc"
        ).first().cnt

        print("‚úì Weekly Employee Pipeline completed successfully")
        print(f"‚úì Records processed: {processed}")

        return {
            "status": "success",
            "processed": processed
        }

    finally:
        spark.stop()




# week_emp_pipeline.py

from pyspark.sql import SparkSession


def run_week_emp_pipeline():
    """
    Week Employee Alerts Pipeline
    - Single pass
    - STRUCT based
    - No for-loop
    """

    spark = (
        SparkSession.builder
        .appName("WeekEmpPipeline")
        .enableHiveSupport()
        .getOrCreate()
    )

    try:
        print("‚ñ∂ Starting Week Employee Pipeline")

        # ------------------------------------------------------------
        # Step 1: Calculate weekly alerts in ONE SQL
        # ------------------------------------------------------------

        spark.sql("""
            CREATE OR REPLACE TEMP VIEW week_emp_alerts_calc AS
            WITH weeks_to_process AS (
                SELECT DISTINCT
                    week_start_date
                FROM week_emp
                WHERE updated_on > current_timestamp() - INTERVAL 4 HOURS
                  AND week_start_date IS NOT NULL
            ),

            week_bounds AS (
                SELECT
                    week_start_date,
                    LEAST(
                        DATE_ADD(week_start_date, 6),
                        DATE_SUB(current_date(), 1)
                    ) AS week_end_date
                FROM weeks_to_process
            ),

            week_data AS (
                SELECT
                    de.emp_id,
                    wb.week_start_date,
                    de.shift_date,
                    de.alerts.triggered_alerts AS alert_ids
                FROM day_emp de
                JOIN week_bounds wb
                  ON de.shift_date BETWEEN wb.week_start_date AND wb.week_end_date
                WHERE de.alerts.triggered_alerts IS NOT NULL
                  AND SIZE(de.alerts.triggered_alerts) > 0
            ),

            exploded_data AS (
                SELECT
                    emp_id,
                    week_start_date,
                    shift_date,
                    EXPLODE(alert_ids) AS alert_id
                FROM week_data
            ),

            streak_base AS (
                SELECT
                    emp_id,
                    week_start_date,
                    alert_id,
                    shift_date,
                    DATE_SUB(
                        shift_date,
                        ROW_NUMBER() OVER (
                            PARTITION BY emp_id, week_start_date, alert_id
                            ORDER BY shift_date
                        )
                    ) AS grp
                FROM exploded_data
            ),

            streaks AS (
                SELECT
                    emp_id,
                    week_start_date,
                    alert_id,
                    COUNT(*) AS consecutive_count
                FROM streak_base
                GROUP BY emp_id, week_start_date, alert_id, grp
            ),

            patterns AS (
                SELECT
                    emp_id,
                    week_start_date,
                    alert_id,
                    COUNT(DISTINCT shift_date) AS affected_days,
                    MAX(consecutive_count) AS max_consecutive
                FROM (
                    SELECT
                        e.emp_id,
                        e.week_start_date,
                        e.alert_id,
                        e.shift_date,
                        s.consecutive_count
                    FROM exploded_data e
                    JOIN streaks s
                      ON e.emp_id = s.emp_id
                     AND e.week_start_date = s.week_start_date
                     AND e.alert_id = s.alert_id
                ) t
                GROUP BY emp_id, week_start_date, alert_id
            ),

            severity_assigned AS (
                SELECT
                    emp_id,
                    week_start_date,
                    alert_id,
                    affected_days,
                    max_consecutive,
                    CASE
                        WHEN max_consecutive >= 3 THEN 'HIGH'
                        WHEN max_consecutive >= 2 THEN 'MEDIUM'
                        WHEN affected_days >= 1 THEN 'LOW'
                        ELSE NULL
                    END AS severity
                FROM patterns
                WHERE affected_days > 0
            ),

            final_results AS (
                SELECT
                    emp_id,
                    week_start_date,
                    named_struct(
                        'alert_patterns',
                        collect_list(
                            named_struct(
                                'alert', alert_id,
                                'severity', severity,
                                'affected_days', affected_days,
                                'consecutive_days', max_consecutive
                            )
                        ),
                        'summary',
                        named_struct(
                            'total_alerts', COUNT(*),
                            'calculated_at', current_timestamp()
                        )
                    ) AS alerts,
                    current_timestamp() AS calculated_at
                FROM severity_assigned
                WHERE severity IS NOT NULL
                GROUP BY emp_id, week_start_date
            )

            SELECT * FROM final_results
        """)

        # ------------------------------------------------------------
        # Step 2: Merge into week_emp
        # ------------------------------------------------------------

        spark.sql("""
            MERGE INTO week_emp target
            USING week_emp_alerts_calc source
            ON target.emp_id = source.emp_id
           AND target.week_start_date = source.week_start_date
            WHEN MATCHED THEN
              UPDATE SET
                target.alerts = source.alerts,
                target.updated_on = source.calculated_at
            WHEN NOT MATCHED THEN
              INSERT (
                emp_id,
                week_start_date,
                alerts,
                updated_on
              )
              VALUES (
                source.emp_id,
                source.week_start_date,
                source.alerts,
                source.calculated_at
              )
        """)

        # ------------------------------------------------------------
        # Step 3: Metrics
        # ------------------------------------------------------------

        processed = spark.sql(
            "SELECT COUNT(*) AS cnt FROM week_emp_alerts_calc"
        ).first().cnt

        print("‚úì Week Employee Pipeline completed successfully")
        print(f"‚úì Records processed: {processed}")

        return {
            "status": "success",
            "processed": processed
        }

    finally:
        spark.stop()



ALTER TABLE week_emp
ADD COLUMNS (
  alerts STRUCT<
    alert_patterns: ARRAY<STRUCT<
      alert: INT,
      severity: STRING,
      affected_days: INT,
      consecutive_days: INT
    >>,
    summary: STRUCT<
      total_alerts: INT,
      calculated_at: TIMESTAMP
    >
  >
);




# day_mgr_pipeline_query.py

from pyspark.sql import SparkSession


def run_day_mgr_pipeline_query():
    """
    Day Manager Alerts Pipeline
    - Single pass
    - No per-date loop
    - Processes day_alerts updated in last 4 hours
    """

    spark = (
        SparkSession.builder
        .appName("DayMgrPipelineQuery")
        .enableHiveSupport()
        .getOrCreate()
    )

    try:
        print("‚ñ∂ Starting Day Manager Alerts Pipeline")

        # ------------------------------------------------------------------
        # Step 1: Compute manager alerts (single SQL)
        # ------------------------------------------------------------------

        spark.sql("""
            CREATE OR REPLACE TEMP VIEW day_mgr_alerts_calc AS
            WITH employee_alerts AS (
                SELECT
                    da.emp_id,
                    da.shift_date,
                    de.manager_id,
                    FROM_JSON(
                        GET_JSON_OBJECT(da.alerts_json, '$.triggered_alerts'),
                        'array<int>'
                    ) AS alert_ids
                FROM day_alerts da
                JOIN day_emp de
                  ON da.emp_id = de.emp_id
                 AND da.shift_date = de.shift_date
                WHERE da.updated_on > current_timestamp() - INTERVAL 4 HOURS
                  AND de.manager_id IS NOT NULL
            ),

            exploded_alerts AS (
                SELECT
                    manager_id,
                    shift_date,
                    emp_id,
                    EXPLODE(alert_ids) AS alert_id
                FROM employee_alerts
                WHERE alert_ids IS NOT NULL
                  AND SIZE(alert_ids) > 0
            ),

            team_sizes AS (
                SELECT
                    manager_id,
                    shift_date,
                    COUNT(DISTINCT emp_id) AS team_size
                FROM day_emp
                WHERE manager_id IS NOT NULL
                GROUP BY manager_id, shift_date
            ),

            alert_counts AS (
                SELECT
                    ea.manager_id,
                    ea.shift_date,
                    ea.alert_id,
                    COUNT(DISTINCT ea.emp_id) AS affected_count,
                    ts.team_size
                FROM exploded_alerts ea
                JOIN team_sizes ts
                  ON ea.manager_id = ts.manager_id
                 AND ea.shift_date = ts.shift_date
                GROUP BY
                    ea.manager_id,
                    ea.shift_date,
                    ea.alert_id,
                    ts.team_size
            ),

            alert_metrics AS (
                SELECT
                    manager_id,
                    shift_date,
                    alert_id,
                    affected_count,
                    team_size,
                    (affected_count * 100.0 / team_size) AS percentage,
                    CASE
                        WHEN (affected_count * 100.0 / team_size) >= 60 THEN 'HIGH'
                        WHEN (affected_count * 100.0 / team_size) >= 40 THEN 'MEDIUM'
                        WHEN (affected_count * 100.0 / team_size) >= 20 THEN 'LOW'
                        ELSE NULL
                    END AS severity
                FROM alert_counts
            ),

            manager_alerts AS (
                SELECT
                    manager_id,
                    shift_date,
                    MAX(team_size) AS team_size,
                    to_json(
                        named_struct(
                            'team_alerts',
                            collect_list(
                                named_struct(
                                    'alert', alert_id,
                                    'severity', severity,
                                    'affected_employees', affected_count,
                                    'team_size', team_size,
                                    'percentage', ROUND(percentage, 1)
                                )
                            ),
                            'summary',
                            named_struct(
                                'total_alerts', COUNT(*),
                                'calculated_at', current_timestamp()
                            )
                        )
                    ) AS alerts_json,
                    current_timestamp() AS calculated_at
                FROM alert_metrics
                WHERE severity IS NOT NULL
                GROUP BY manager_id, shift_date
            )

            SELECT * FROM manager_alerts
        """)

        # ------------------------------------------------------------------
        # Step 2: Merge into day_mgr_alerts
        # ------------------------------------------------------------------

        spark.sql("""
            MERGE INTO day_mgr_alerts target
            USING day_mgr_alerts_calc source
            ON target.manager_id = source.manager_id
           AND target.shift_date = source.shift_date
            WHEN MATCHED THEN
                UPDATE SET
                    target.alerts_json = source.alerts_json,
                    target.team_size = source.team_size,
                    target.updated_on = source.calculated_at
            WHEN NOT MATCHED THEN
                INSERT (
                    manager_id,
                    shift_date,
                    team_size,
                    alerts_json,
                    updated_on
                )
                VALUES (
                    source.manager_id,
                    source.shift_date,
                    source.team_size,
                    source.alerts_json,
                    source.calculated_at
                )
        """)

        # ------------------------------------------------------------------
        # Step 3: Metrics
        # ------------------------------------------------------------------

        processed_count = spark.sql(
            "SELECT COUNT(*) AS cnt FROM day_mgr_alerts_calc"
        ).first().cnt

        print("‚úì Day Manager Pipeline completed successfully")
        print(f"‚úì Manager records processed: {processed_count}")

        return {
            "status": "success",
            "processed": processed_count
        }

    finally:
        spark.stop()




Got it üëç
Since here also the column is a STRUCT, the rule is the same as before:

> Do NOT use to_json() or JSON parsing functions at all.
Work with the STRUCT directly.



Below is exactly what to change and what the final command should look like.


---

‚úÖ Assumption (important)

Your table looks like this:

day_alerts.alerts STRUCT<
  triggered_alerts: ARRAY<INT>,
  summary: STRUCT<
    total_alerts: INT,
    calculated_at: TIMESTAMP
  >
>

And day_mgr_alerts.alerts is also a STRUCT (recommended).


---

‚ùå What you must NOT use anymore

Remove both of these:

GET_JSON_OBJECT(...)
FROM_JSON(...)
to_json(...)
CONCAT(...)

They are only for STRING JSON.


---

‚úÖ Correct command when column is STRUCT

1Ô∏è‚É£ Reading employee alerts (STRUCT access)

‚ùå Old (JSON)

FROM_JSON(
    GET_JSON_OBJECT(da.alerts_json, '$.triggered_alerts'),
    'array<int>'
) AS alert_ids

‚úÖ New (STRUCT)

da.alerts.triggered_alerts AS alert_ids


---

2Ô∏è‚É£ Building manager alerts (STRUCT, not JSON)

‚ùå Old (string concat / to_json)

to_json(
    named_struct(
        'team_alerts', ...
    )
) AS alerts_json

‚úÖ New (STRUCT)

named_struct(
    'team_alerts',
    collect_list(
        named_struct(
            'alert', alert_id,
            'severity', severity,
            'affected_employees', affected_count,
            'team_size', team_size,
            'percentage', ROUND(percentage, 1)
        )
    ),
    'summary',
    named_struct(
        'total_alerts', COUNT(*),
        'calculated_at', current_timestamp()
    )
) AS alerts


---

‚úÖ Final corrected core SQL (STRUCT ‚Üí STRUCT)

CREATE OR REPLACE TEMP VIEW day_mgr_alerts_calc AS
WITH employee_alerts AS (
    SELECT
        da.emp_id,
        da.shift_date,
        de.manager_id,
        da.alerts.triggered_alerts AS alert_ids
    FROM day_alerts da
    JOIN day_emp de
      ON da.emp_id = de.emp_id
     AND da.shift_date = de.shift_date
    WHERE da.updated_on > current_timestamp() - INTERVAL 4 HOURS
      AND de.manager_id IS NOT NULL
),

exploded_alerts AS (
    SELECT
        manager_id,
        shift_date,
        emp_id,
        EXPLODE(alert_ids) AS alert_id
    FROM employee_alerts
    WHERE alert_ids IS NOT NULL
      AND SIZE(alert_ids) > 0
),

team_sizes AS (
    SELECT
        manager_id,
        shift_date,
        COUNT(DISTINCT emp_id) AS team_size
    FROM day_emp
    WHERE manager_id IS NOT NULL
    GROUP BY manager_id, shift_date
),

alert_counts AS (
    SELECT
        ea.manager_id,
        ea.shift_date,
        ea.alert_id,
        COUNT(DISTINCT ea.emp_id) AS affected_count,
        ts.team_size
    FROM exploded_alerts ea
    JOIN team_sizes ts
      ON ea.manager_id = ts.manager_id
     AND ea.shift_date = ts.shift_date
    GROUP BY
        ea.manager_id,
        ea.shift_date,
        ea.alert_id,
        ts.team_size
),

alert_metrics AS (
    SELECT
        manager_id,
        shift_date,
        alert_id,
        affected_count,
        team_size,
        (affected_count * 100.0 / team_size) AS percentage,
        CASE
            WHEN (affected_count * 100.0 / team_size) >= 60 THEN 'HIGH'
            WHEN (affected_count * 100.0 / team_size) >= 40 THEN 'MEDIUM'
            WHEN (affected_count * 100.0 / team_size) >= 20 THEN 'LOW'
            ELSE NULL
        END AS severity
    FROM alert_counts
),

manager_alerts AS (
    SELECT
        manager_id,
        shift_date,
        MAX(team_size) AS team_size,
        named_struct(
            'team_alerts',
            collect_list(
                named_struct(
                    'alert', alert_id,
                    'severity', severity,
                    'affected_employees', affected_count,
                    'team_size', team_size,
                    'percentage', ROUND(percentage, 1)
                )
            ),
            'summary',
            named_struct(
                'total_alerts', COUNT(*),
                'calculated_at', current_timestamp()
            )
        ) AS alerts,
        current_timestamp() AS calculated_at
    FROM alert_metrics
    WHERE severity IS NOT NULL
    GROUP BY manager_id, shift_date
)

SELECT * FROM manager_alerts;


---

‚úÖ MERGE command (STRUCT column)

MERGE INTO day_mgr_alerts target
USING day_mgr_alerts_calc source
ON target.manager_id = source.manager_id
AND target.shift_date = source.shift_date
WHEN MATCHED THEN
  UPDATE SET
    target.alerts = source.alerts,
    target.team_size = source.team_size,
    target.updated_on = source.calculated_at
WHEN NOT MATCHED THEN
  INSERT (
    manager_id,
    shift_date,
    team_size,
    alerts,
    updated_on
  )
  VALUES (
    source.manager_id,
    source.shift_date,
    source.team_size,
    source.alerts,
    source.calculated_at
  );


---

üèÅ Final takeaway

‚úî STRUCT ‚Üí access with dot notation
‚úî STRUCT ‚Üí build with named_struct
‚úî NO to_json, from_json, get_json_object
‚úî Cleaner, faster, safer

If you want, next I can:

Show how to query STRUCT alerts

Convert existing STRING JSON ‚Üí STRUCT

Add Z-ORDER optimizations

Validate schema mismatches


Just tell me üëç







from pyspark.sql.functions import udf
from pyspark.sql.types import StringType, ArrayType, IntegerType
import json
from datetime import datetime

# UDF for Employee Day Alert Calculation
def calculate_employee_day_alerts(active_time, idle_time, lock_time, 
                                 expected_shift_time, expected_break_time):
    """
    Calculate day alerts for an employee and return JSON string
    """
    # Calculate active percentage
    total_time = active_time + idle_time + lock_time
    active_percent = (active_time * 100.0 / total_time) if total_time > 0 else 0
    
    triggered_alerts = []
    
    # Alert 1: Few Breaks
    if (idle_time + lock_time) < expected_break_time:
        triggered_alerts.append(1)
    
    # Alert 2: Low Active Time
    if active_time < expected_shift_time:
        triggered_alerts.append(2)
    
    # Alert 3: High Active Time
    if active_time > expected_shift_time:
        triggered_alerts.append(3)
    
    # Alert 4: Low Active & High Idle Time
    if active_time < (0.9 * expected_shift_time) and active_percent < 90:
        triggered_alerts.append(4)
    
    # Alert 5: Low Active & Low Idle Time
    if active_time < (0.9 * expected_shift_time) and active_percent > 90:
        triggered_alerts.append(5)
    
    # Alert 6: High Active & High Idle Time
    if active_time > (1.1 * expected_shift_time) and idle_time > 1.0:
        triggered_alerts.append(6)
    
    # Alert 7: Few Breaks + Low Active & High Idle Time
    if ((idle_time + lock_time) < expected_break_time and 
        active_time < (0.9 * expected_shift_time) and 
        active_percent < 90):
        triggered_alerts.append(7)
    
    # Alert 8: Few Breaks + High Active & High Idle Time
    if ((idle_time + lock_time) < expected_break_time and 
        active_time > (1.1 * expected_shift_time) and 
        idle_time > 1.0):
        triggered_alerts.append(8)
    
    # Alert 9: Few Breaks + Low Active & Low Idle Time
    if ((idle_time + lock_time) < expected_break_time and 
        active_time < (0.9 * expected_shift_time) and 
        active_percent > 90):
        triggered_alerts.append(9)
    
    # Alert 10: Low Active + High Idle Time
    if (active_time < expected_shift_time and 
        active_time < (0.9 * expected_shift_time) and 
        active_percent < 90):
        triggered_alerts.append(10)
    
    # Alert 11: Low Active + Low Idle Time
    if (active_time < expected_shift_time and 
        active_time < (0.9 * expected_shift_time) and 
        active_percent > 90):
        triggered_alerts.append(11)
    
    # Alert 12: High Active + High Idle Time
    if (active_time > (1.1 * expected_shift_time) and 
        idle_time > 1.0 and 
        active_time > expected_shift_time):
        triggered_alerts.append(12)
    
    # Alert 13: Low Active + Few Breaks
    if ((idle_time + lock_time) < expected_break_time and 
        active_time < expected_shift_time):
        triggered_alerts.append(13)
    
    # Alert 14: High Active + Few Breaks
    if ((idle_time + lock_time) < expected_break_time and 
        active_time > expected_shift_time):
        triggered_alerts.append(14)
    
    # Create JSON structure exactly as you specified
    alerts_json = {
        "triggered_alerts": triggered_alerts,
        "summary": {
            "total_alerts": len(triggered_alerts),
            "calculated_at": datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ")
        }
    }
    
    return json.dumps(alerts_json)

# Register the UDF
employee_day_alerts_udf = udf(calculate_employee_day_alerts, StringType())

# Alternative: UDF that returns just alert IDs array
def get_employee_day_alert_ids(active_time, idle_time, lock_time, 
                              expected_shift_time, expected_break_time):
    """
    Return just the alert IDs array without JSON
    """
    # Calculate active percentage
    total_time = active_time + idle_time + lock_time
    active_percent = (active_time * 100.0 / total_time) if total_time > 0 else 0
    
    triggered_alerts = []
    
    # Alert 1: Few Breaks
    if (idle_time + lock_time) < expected_break_time:
        triggered_alerts.append(1)
    
    # Alert 2: Low Active Time
    if active_time < expected_shift_time:
        triggered_alerts.append(2)
    
    # Alert 3: High Active Time
    if active_time > expected_shift_time:
        triggered_alerts.append(3)
    
    # Alert 4: Low Active & High Idle Time
    if active_time < (0.9 * expected_shift_time) and active_percent < 90:
        triggered_alerts.append(4)
    
    # Alert 5: Low Active & Low Idle Time
    if active_time < (0.9 * expected_shift_time) and active_percent > 90:
        triggered_alerts.append(5)
    
    # Alert 6: High Active & High Idle Time
    if active_time > (1.1 * expected_shift_time) and idle_time > 1.0:
        triggered_alerts.append(6)
    
    # Alert 7: Few Breaks + Low Active & High Idle Time
    if ((idle_time + lock_time) < expected_break_time and 
        active_time < (0.9 * expected_shift_time) and 
        active_percent < 90):
        triggered_alerts.append(7)
    
    # Alert 8: Few Breaks + High Active & High Idle Time
    if ((idle_time + lock_time) < expected_break_time and 
        active_time > (1.1 * expected_shift_time) and 
        idle_time > 1.0):
        triggered_alerts.append(8)
    
    # Alert 9: Few Breaks + Low Active & Low Idle Time
    if ((idle_time + lock_time) < expected_break_time and 
        active_time < (0.9 * expected_shift_time) and 
        active_percent > 90):
        triggered_alerts.append(9)
    
    # Alert 10: Low Active + High Idle Time
    if (active_time < expected_shift_time and 
        active_time < (0.9 * expected_shift_time) and 
        active_percent < 90):
        triggered_alerts.append(10)
    
    # Alert 11: Low Active + Low Idle Time
    if (active_time < expected_shift_time and 
        active_time < (0.9 * expected_shift_time) and 
        active_percent > 90):
        triggered_alerts.append(11)
    
    # Alert 12: High Active + High Idle Time
    if (active_time > (1.1 * expected_shift_time) and 
        idle_time > 1.0 and 
        active_time > expected_shift_time):
        triggered_alerts.append(12)
    
    # Alert 13: Low Active + Few Breaks
    if ((idle_time + lock_time) < expected_break_time and 
        active_time < expected_shift_time):
        triggered_alerts.append(13)
    
    # Alert 14: High Active + Few Breaks
    if ((idle_time + lock_time) < expected_break_time and 
        active_time > expected_shift_time):
        triggered_alerts.append(14)
    
    return triggered_alerts

alert_ids_udf = udf(get_employee_day_alert_ids, ArrayType(IntegerType()))


from pyspark.sql.functions import udf
from pyspark.sql.types import StringType, MapType, StringType, IntegerType, FloatType
import json
from datetime import datetime

def calculate_manager_day_alerts(alert_ids_list, team_size):
    """
    Calculate manager day alerts from employee alert IDs
    alert_ids_list: list of alert ID arrays for all team members
    team_size: total number of team members
    """
    if not alert_ids_list or team_size == 0:
        return json.dumps({"team_alerts": [], "summary": {"total_alerts": 0, "calculated_at": datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ")}})
    
    # Count occurrences of each alert across team
    alert_counts = {}
    for alert_ids in alert_ids_list:
        if alert_ids:
            for alert_id in alert_ids:
                alert_counts[alert_id] = alert_counts.get(alert_id, 0) + 1
    
    team_alerts = []
    
    for alert_id, affected_count in alert_counts.items():
        percentage = (affected_count * 100.0) / team_size
        
        # Determine severity based on percentage
        if percentage >= 60:
            severity = "HIGH"
        elif percentage >= 40:
            severity = "MEDIUM"
        elif percentage >= 20:
            severity = "LOW"
        else:
            continue  # Skip alerts affecting <20% of team
        
        team_alerts.append({
            "alert": int(alert_id),
            "severity": severity,
            "affected_employees": int(affected_count),
            "team_size": int(team_size),
            "percentage": round(float(percentage), 1)
        })
    
    # Sort by alert ID
    team_alerts.sort(key=lambda x: x["alert"])
    
    # Create JSON structure exactly as you specified
    manager_json = {
        "team_alerts": team_alerts,
        "summary": {
            "total_alerts": len(team_alerts),
            "calculated_at": datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ")
        }
    }
    
    return json.dumps(manager_json)

# Register the UDF
manager_day_alerts_udf = udf(calculate_manager_day_alerts, StringType())

# Alternative: Simplified version if you have aggregated data
def calculate_manager_simple(alert_id_counts, team_size):
    """
    Simplified version where alert_id_counts is a map of {alert_id: count}
    """
    team_alerts = []
    
    for alert_id_str, count in alert_id_counts.items():
        if count:
            alert_id = int(alert_id_str)
            affected_count = int(count)
            percentage = (affected_count * 100.0) / team_size
            
            if percentage >= 60:
                severity = "HIGH"
            elif percentage >= 40:
                severity = "MEDIUM"
            elif percentage >= 20:
                severity = "LOW"
            else:
                continue
            
            team_alerts.append({
                "alert": alert_id,
                "severity": severity,
                "affected_employees": affected_count,
                "team_size": int(team_size),
                "percentage": round(float(percentage), 1)
            })
    
    manager_json = {
        "team_alerts": team_alerts,
        "summary": {
            "total_alerts": len(team_alerts),
            "calculated_at": datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ")
        }
    }
    
    return json.dumps(manager_json)

manager_simple_udf = udf(calculate_manager_simple, StringType())


from pyspark.sql import SparkSession
from pyspark.sql.functions import col, collect_list, current_timestamp
from datetime import datetime, timedelta

def run_day_pipeline_with_udf():
    spark = SparkSession.builder.appName("AlertPipelineUDF").getOrCreate()
    
    # 1. Employee Day Alerts
    print("Processing Employee Day Alerts...")
    
    # Get data
    employee_data = spark.sql("""
        SELECT 
            emp_id,
            shift_date,
            manager_id,
            active_time,
            idle_time,
            lock_time,
            expected_shift_time,
            expected_break_time
        FROM day_emp
        WHERE shift_date = CURRENT_DATE() - INTERVAL 1 DAY
    """)
    
    # Apply employee UDF
    employee_alerts = employee_data.withColumn(
        "alerts_json",
        employee_day_alerts_udf(
            col("active_time"),
            col("idle_time"),
            col("lock_time"),
            col("expected_shift_time"),
            col("expected_break_time")
        )
    ).withColumn(
        "updated_on",
        current_timestamp()
    )
    
    # Save to day_alerts table
    employee_alerts.select("emp_id", "shift_date", "alerts_json", "updated_on") \
        .write.mode("overwrite") \
        .saveAsTable("day_alerts")
    
    print(f"Employee alerts processed: {employee_alerts.count()}")
    
    # 2. Manager Day Alerts
    print("\nProcessing Manager Day Alerts...")
    
    # Aggregate by manager
    manager_data = spark.sql("""
        WITH employee_alerts AS (
            SELECT 
                de.emp_id,
                de.manager_id,
                de.shift_date,
                FROM_JSON(da.alerts_json, 'STRUCT<triggered_alerts:ARRAY<INT>, summary:STRUCT<total_alerts:INT, calculated_at:STRING>>') as alerts
            FROM day_alerts da
            JOIN day_emp de ON da.emp_id = de.emp_id AND da.shift_date = de.shift_date
            WHERE da.shift_date = CURRENT_DATE() - INTERVAL 1 DAY
            AND de.manager_id IS NOT NULL
        ),
        team_sizes AS (
            SELECT 
                manager_id,
                COUNT(DISTINCT emp_id) as team_size
            FROM day_emp
            WHERE shift_date = CURRENT_DATE() - INTERVAL 1 DAY
            GROUP BY manager_id
        )
        SELECT 
            ea.manager_id,
            ea.shift_date,
            COLLECT_LIST(alerts.triggered_alerts) as alert_arrays,
            ts.team_size
        FROM employee_alerts ea
        JOIN team_sizes ts ON ea.manager_id = ts.manager_id
        GROUP BY ea.manager_id, ea.shift_date, ts.team_size
    """)
    
    # Apply manager UDF
    manager_alerts = manager_data.withColumn(
        "alerts_json",
        manager_day_alerts_udf(col("alert_arrays"), col("team_size"))
    ).withColumn(
        "updated_on",
        current_timestamp()
    )
    
    # Save to day_mgr_alerts table
    manager_alerts.select("manager_id", "shift_date", "team_size", "alerts_json", "updated_on") \
        .write.mode("overwrite") \
        .saveAsTable("day_mgr_alerts")
    
    print(f"Manager alerts processed: {manager_alerts.count()}")
    
    spark.stop()
    return {
        "employee_records": employee_alerts.count(),
        "manager_records": manager_alerts.count()
    }
