


def safe_div(n, d):
    return round((n / d) * 100, 2) if d else 0.0


def shift_delta(actual, expected):
    if actual is None or expected is None:
        return 0.0
    return round(abs(actual - expected), 2)


def shift_trend(actual, expected):
    if actual is None or expected is None:
        return "No Change"
    if actual > expected:
        return "Up"
    if actual < expected:
        return "Down"
    return "No Change"


def trend(curr, prev):
    if prev in (None, 0):
        return "No Change"
    if curr > prev:
        return "Up"
    if curr < prev:
        return "Down"
    return "No Change"




def build_work_summary(row):
    # Current
    total_work = row.get("current_total_work_time", 0)
    active = row.get("current_total_active_time", 0)
    idle = row.get("current_total_idle_time", 0)
    lock = row.get("current_total_window_lock_time", 0)

    within_shift = row.get("current_within_shift", 0)
    out_shift = row.get("current_outof_shift", 0)

    # Expected / pulse
    expected_work = row.get("current_total_pulse_shift_time_seconds", 0)
    expected_active = row.get("current_total_adjusted_active_time_seconds", 0)
    expected_lock = row.get("current_total_adjusted_lock_time_seconds", 0)

    # Previous
    prev_work = row.get("previous_total_work_time", 0)
    prev_active = row.get("previous_total_active_time", 0)
    prev_idle = row.get("previous_total_idle_time", 0)
    prev_lock = row.get("previous_total_window_lock_time", 0)

    return {
        # ── Core ─────────────────────────────
        "total_work_time": total_work,
        "active_time": active,
        "idle_time": idle,
        "work_lock_time": lock,

        # ── Stretch ──────────────────────────
        "total_in_stretch": within_shift,
        "total_out_stretch": out_shift,

        # ── Shift deviation (NEW) ────────────
        "total_work_shift_time": shift_delta(total_work, expected_work),
        "total_work_shift_trend": shift_trend(total_work, expected_work),

        "active_shift_time": shift_delta(active, expected_active),
        "active_shift_trend": shift_trend(active, expected_active),

        "window_lock_shift_time": shift_delta(lock, expected_lock),
        "window_lock_shift_trend": shift_trend(lock, expected_lock),

        # ── Previous ─────────────────────────
        "prev_total_work_time": prev_work,
        "prev_active_time": prev_active,
        "prev_idle_time": prev_idle,
        "prev_work_lock_time": prev_lock,

        # ── Period trends ────────────────────
        "total_work_trend": trend(total_work, prev_work),
        "active_trend": trend(active, prev_active),
        "idle_trend": trend(idle, prev_idle),
        "work_lock_trend": trend(lock, prev_lock),

        # ── Pulse percentages (NEW) ──────────
        "pulse_active_percent": safe_div(active, total_work),
        "pulse_idle_percent": safe_div(idle, total_work),
        "pulse_window_lock_percent": safe_div(lock, total_work),
    }


from collections import defaultdict

def build_app_usage(row):
    curr_apps = safe_json(row["current_application_info_totals"])
    prev_apps = safe_json(row["previous_tot_application_info"])

    # Build unified mapping
    app_tag_map = build_app_tag_mapping(
        row.get("custom_app_mappings", [])
    )

    def aggregate(apps):
        agg = defaultdict(float)

        for app, v in apps.items():
            if not isinstance(v, dict):
                continue

            norm_app = normalize_app_name(app)
            tag_info = app_tag_map.get(
                norm_app,
                {"tag": "Unlabelled", "color": DEFAULT_TAG_COLORS["Unlabelled"]}
            )

            agg[tag_info["tag"]] += v.get("active", 0) + v.get("idle", 0)

        return agg

    curr = aggregate(curr_apps)
    prev = aggregate(prev_apps)

    total_curr = sum(curr.values()) or 1
    total_prev = sum(prev.values()) or 1

    app_usage = []

    for tag, curr_val in curr.items():
        prev_val = prev.get(tag, 0)

        app_usage.append({
            "tag_name": tag,
            "percentage": round((curr_val / total_curr) * 100, 1),
            "percentage_prev": round((prev_val / total_prev) * 100, 1),
            "prev_trend": trend(curr_val, prev_val),
            "prev_percent": percentage_change(curr_val, prev_val),
            "tag_color": DEFAULT_TAG_COLORS.get(tag, "#CCCCCC")
        })

    return app_usage



def build_app_tag_mapping(custom_mappings):
    app_tag_map = {}

    # 1️⃣ Custom mappings (highest priority)
    for m in custom_mappings:
        if not isinstance(m, dict):
            continue

        app = m.get("app_name")
        tag = m.get("tag_name")
        color = m.get("tag_color")

        if not app or not tag:
            continue

        norm_app = normalize_app_name(app)

        app_tag_map[norm_app] = {
            "tag": tag,
            "color": color or DEFAULT_TAG_COLORS.get(tag, "#CCCCCC")
        }

    # 2️⃣ Default mappings (only if not already mapped)
    for tag, apps in DEFAULT_TAG_APPS.items():
        for app in apps:
            norm_app = normalize_app_name(app)

            if norm_app not in app_tag_map:
                app_tag_map[norm_app] = {
                    "tag": tag,
                    "color": DEFAULT_TAG_COLORS.get(tag, "#CCCCCC")
                }

    return app_tag_map



def normalize_app_name(app: str) -> str:
    return app.lower().replace(".exe", "")



ALTER TABLE dashboard.manager_reports_monthly
ADD COLUMNS (
  alerts_json STRUCT<
    month_team_patterns: ARRAY<STRUCT<
      alert: INT,
      severity: STRING,
      metrics: STRUCT<
        team_affected_days: INT,
        team_size: INT,
        percentage_affected: DOUBLE,
        individuals_with_3_consecutive: INT
      >
    >>,
    summary: STRUCT<
      total_alerts: INT,
      calculated_at: TIMESTAMP
    >
  >
);




# month_mgr_pipeline.py

from pyspark.sql import SparkSession


def run_month_mgr_pipeline():
    """
    Month Manager Alerts Pipeline
    - Single pass
    - STRUCT based (alerts_json)
    - Uses dashboard.emp_reports_daily
    - Uses dashboard.manager_reports_monthly
    """

    spark = (
        SparkSession.builder
        .appName("MonthMgrPipeline")
        .enableHiveSupport()
        .getOrCreate()
    )

    try:
        print("▶ Starting Month Manager Pipeline")

        # ------------------------------------------------------------
        # Step 1: Build monthly manager alerts (single SQL)
        # ------------------------------------------------------------

        spark.sql("""
            CREATE OR REPLACE TEMP VIEW month_mgr_alerts_calc AS
            WITH mgr_months_to_process AS (
                SELECT DISTINCT
                    manager_id,
                    month_start
                FROM dashboard.manager_reports_monthly
                WHERE updated_on > current_timestamp() - INTERVAL 4 HOURS
                  AND manager_id IS NOT NULL
                  AND month_start IS NOT NULL
            ),

            month_bounds AS (
                SELECT
                    manager_id,
                    month_start,
                    CASE
                        -- current month → yesterday
                        WHEN month_start = DATE_TRUNC('MONTH', current_date())
                             THEN DATE_SUB(current_date(), 1)
                        -- past months → last day of month
                        ELSE LAST_DAY(month_start)
                    END AS month_end
                FROM mgr_months_to_process
            ),

            month_data AS (
                SELECT
                    d.manager_id,
                    d.emp_id,
                    m.month_start,
                    d.day_start,
                    DATE_TRUNC('WEEK', d.day_start) AS week_start,
                    d.alerts_json.triggered_alerts AS alert_ids
                FROM dashboard.emp_reports_daily d
                JOIN month_bounds m
                  ON d.manager_id = m.manager_id
                 AND d.day_start BETWEEN m.month_start AND m.month_end
                WHERE d.alerts_json.triggered_alerts IS NOT NULL
                  AND SIZE(d.alerts_json.triggered_alerts) > 0
            ),

            exploded_data AS (
                SELECT
                    manager_id,
                    emp_id,
                    month_start,
                    day_start,
                    week_start,
                    EXPLODE(alert_ids) AS alert_id
                FROM month_data
            ),

            team_sizes AS (
                SELECT
                    manager_id,
                    month_start,
                    COUNT(DISTINCT emp_id) AS team_size
                FROM month_data
                GROUP BY manager_id, month_start
            ),

            team_days_affected AS (
                SELECT
                    manager_id,
                    month_start,
                    alert_id,
                    COUNT(DISTINCT day_start) AS team_affected_days
                FROM exploded_data
                GROUP BY manager_id, month_start, alert_id
            ),

            team_percentage AS (
                SELECT
                    tda.manager_id,
                    tda.month_start,
                    tda.alert_id,
                    tda.team_affected_days,
                    ts.team_size,
                    (tda.team_affected_days * 100.0 / ts.team_size) AS percentage_affected
                FROM team_days_affected tda
                JOIN team_sizes ts
                  ON tda.manager_id = ts.manager_id
                 AND tda.month_start = ts.month_start
                WHERE tda.team_affected_days >= 6
            ),

            individual_streak_base AS (
                SELECT
                    manager_id,
                    emp_id,
                    month_start,
                    alert_id,
                    week_start,
                    day_start,
                    DATE_SUB(
                        day_start,
                        ROW_NUMBER() OVER (
                            PARTITION BY manager_id, emp_id, alert_id, week_start
                            ORDER BY day_start
                        )
                    ) AS grp
                FROM exploded_data
            ),

            individual_streaks AS (
                SELECT
                    manager_id,
                    month_start,
                    emp_id,
                    alert_id,
                    week_start,
                    COUNT(*) AS consecutive_days
                FROM individual_streak_base
                GROUP BY manager_id, month_start, emp_id, alert_id, week_start, grp
            ),

            individuals_across_weeks AS (
                SELECT
                    manager_id,
                    month_start,
                    alert_id,
                    COUNT(DISTINCT emp_id) AS individuals_with_3_consecutive
                FROM individual_streaks
                WHERE consecutive_days >= 3
                GROUP BY manager_id, month_start, alert_id
                HAVING COUNT(DISTINCT week_start) >= 2
            ),

            combined_metrics AS (
                SELECT
                    COALESCE(tp.manager_id, iw.manager_id) AS manager_id,
                    COALESCE(tp.month_start, iw.month_start) AS month_start,
                    COALESCE(tp.alert_id, iw.alert_id) AS alert_id,
                    COALESCE(tp.team_affected_days, 0) AS team_affected_days,
                    COALESCE(tp.team_size, 0) AS team_size,
                    COALESCE(tp.percentage_affected, 0) AS percentage_affected,
                    COALESCE(iw.individuals_with_3_consecutive, 0)
                        AS individuals_with_3_consecutive
                FROM team_percentage tp
                FULL OUTER JOIN individuals_across_weeks iw
                  ON tp.manager_id = iw.manager_id
                 AND tp.month_start = iw.month_start
                 AND tp.alert_id = iw.alert_id
            ),

            severity_assigned AS (
                SELECT
                    manager_id,
                    month_start,
                    alert_id,
                    team_affected_days,
                    team_size,
                    ROUND(percentage_affected, 1) AS percentage_affected,
                    individuals_with_3_consecutive,
                    CASE
                        WHEN (percentage_affected >= 60 AND team_affected_days >= 6)
                             OR individuals_with_3_consecutive >= 3 THEN 'HIGH'
                        WHEN (percentage_affected >= 40 AND percentage_affected < 60 AND team_affected_days >= 6)
                             OR individuals_with_3_consecutive >= 2 THEN 'MEDIUM'
                        WHEN (percentage_affected >= 20 AND percentage_affected < 40 AND team_affected_days >= 6)
                             OR individuals_with_3_consecutive >= 1 THEN 'LOW'
                        ELSE NULL
                    END AS severity
                FROM combined_metrics
                WHERE (percentage_affected >= 20 AND team_affected_days >= 6)
                   OR individuals_with_3_consecutive >= 1
            ),

            final_results AS (
                SELECT
                    manager_id,
                    month_start,
                    MAX(team_size) AS team_size,
                    named_struct(
                        'month_team_patterns',
                        collect_list(
                            named_struct(
                                'alert', alert_id,
                                'severity', severity,
                                'metrics',
                                named_struct(
                                    'team_affected_days', team_affected_days,
                                    'team_size', team_size,
                                    'percentage_affected', percentage_affected,
                                    'individuals_with_3_consecutive',
                                        individuals_with_3_consecutive
                                )
                            )
                        ),
                        'summary',
                        named_struct(
                            'total_alerts', COUNT(*),
                            'calculated_at', current_timestamp()
                        )
                    ) AS alerts_json,
                    current_timestamp() AS calculated_at
                FROM severity_assigned
                WHERE severity IS NOT NULL
                GROUP BY manager_id, month_start
            )

            SELECT * FROM final_results
        """)

        # ------------------------------------------------------------
        # Step 2: MERGE into dashboard.manager_reports_monthly
        # ------------------------------------------------------------

        spark.sql("""
            MERGE INTO dashboard.manager_reports_monthly target
            USING month_mgr_alerts_calc source
            ON target.manager_id = source.manager_id
           AND target.month_start = source.month_start
            WHEN MATCHED THEN
              UPDATE SET
                target.alerts_json = source.alerts_json,
                target.team_size = source.team_size,
                target.updated_on = source.calculated_at
            WHEN NOT MATCHED THEN
              INSERT (
                manager_id,
                month_start,
                team_size,
                alerts_json,
                updated_on
              )
              VALUES (
                source.manager_id,
                source.month_start,
                source.team_size,
                source.alerts_json,
                source.calculated_at
              )
        """)

        processed = spark.sql(
            "SELECT COUNT(*) AS cnt FROM month_mgr_alerts_calc"
        ).first().cnt

        print("✓ Month Manager Pipeline completed successfully")
        print(f"✓ Records processed: {processed}")

        return {
            "status": "success",
            "processed": processed
        }

    finally:
        spark.stop()





# month_mgr_pipeline.py

from pyspark.sql import SparkSession


def run_month_mgr_pipeline():
    """
    Month Manager Alerts Pipeline
    - Single pass
    - STRUCT based (alerts_json)
    - Uses dashboard.emp_reports_daily
    - Uses dashboard.manager_reports_monthly
    """

    spark = (
        SparkSession.builder
        .appName("MonthMgrPipeline")
        .enableHiveSupport()
        .getOrCreate()
    )

    try:
        print("▶ Starting Month Manager Pipeline")

        # ------------------------------------------------------------
        # Step 1: Build monthly manager alerts (single SQL)
        # ------------------------------------------------------------

        spark.sql("""
            CREATE OR REPLACE TEMP VIEW month_mgr_alerts_calc AS
            WITH mgr_months_to_process AS (
                SELECT DISTINCT
                    manager_id,
                    month_start
                FROM dashboard.manager_reports_monthly
                WHERE updated_on > current_timestamp() - INTERVAL 4 HOURS
                  AND manager_id IS NOT NULL
                  AND month_start IS NOT NULL
            ),

            month_bounds AS (
                SELECT
                    manager_id,
                    month_start,
                    CASE
                        -- current month → yesterday
                        WHEN month_start = DATE_TRUNC('MONTH', current_date())
                             THEN DATE_SUB(current_date(), 1)
                        -- past months → last day of month
                        ELSE LAST_DAY(month_start)
                    END AS month_end
                FROM mgr_months_to_process
            ),

            month_data AS (
                SELECT
                    d.manager_id,
                    d.emp_id,
                    m.month_start,
                    d.day_start,
                    DATE_TRUNC('WEEK', d.day_start) AS week_start,
                    d.alerts_json.triggered_alerts AS alert_ids
                FROM dashboard.emp_reports_daily d
                JOIN month_bounds m
                  ON d.manager_id = m.manager_id
                 AND d.day_start BETWEEN m.month_start AND m.month_end
                WHERE d.alerts_json.triggered_alerts IS NOT NULL
                  AND SIZE(d.alerts_json.triggered_alerts) > 0
            ),

            exploded_data AS (
                SELECT
                    manager_id,
                    emp_id,
                    month_start,
                    day_start,
                    week_start,
                    EXPLODE(alert_ids) AS alert_id
                FROM month_data
            ),

            team_sizes AS (
                SELECT
                    manager_id,
                    month_start,
                    COUNT(DISTINCT emp_id) AS team_size
                FROM month_data
                GROUP BY manager_id, month_start
            ),

            team_days_affected AS (
                SELECT
                    manager_id,
                    month_start,
                    alert_id,
                    COUNT(DISTINCT day_start) AS team_affected_days
                FROM exploded_data
                GROUP BY manager_id, month_start, alert_id
            ),

            team_percentage AS (
                SELECT
                    tda.manager_id,
                    tda.month_start,
                    tda.alert_id,
                    tda.team_affected_days,
                    ts.team_size,
                    (tda.team_affected_days * 100.0 / ts.team_size) AS percentage_affected
                FROM team_days_affected tda
                JOIN team_sizes ts
                  ON tda.manager_id = ts.manager_id
                 AND tda.month_start = ts.month_start
                WHERE tda.team_affected_days >= 6
            ),

            individual_streak_base AS (
                SELECT
                    manager_id,
                    emp_id,
                    month_start,
                    alert_id,
                    week_start,
                    day_start,
                    DATE_SUB(
                        day_start,
                        ROW_NUMBER() OVER (
                            PARTITION BY manager_id, emp_id, alert_id, week_start
                            ORDER BY day_start
                        )
                    ) AS grp
                FROM exploded_data
            ),

            individual_streaks AS (
                SELECT
                    manager_id,
                    month_start,
                    emp_id,
                    alert_id,
                    week_start,
                    COUNT(*) AS consecutive_days
                FROM individual_streak_base
                GROUP BY manager_id, month_start, emp_id, alert_id, week_start, grp
            ),

            individuals_across_weeks AS (
                SELECT
                    manager_id,
                    month_start,
                    alert_id,
                    COUNT(DISTINCT emp_id) AS individuals_with_3_consecutive
                FROM individual_streaks
                WHERE consecutive_days >= 3
                GROUP BY manager_id, month_start, alert_id
                HAVING COUNT(DISTINCT week_start) >= 2
            ),

            combined_metrics AS (
                SELECT
                    COALESCE(tp.manager_id, iw.manager_id) AS manager_id,
                    COALESCE(tp.month_start, iw.month_start) AS month_start,
                    COALESCE(tp.alert_id, iw.alert_id) AS alert_id,
                    COALESCE(tp.team_affected_days, 0) AS team_affected_days,
                    COALESCE(tp.team_size, 0) AS team_size,
                    COALESCE(tp.percentage_affected, 0) AS percentage_affected,
                    COALESCE(iw.individuals_with_3_consecutive, 0)
                        AS individuals_with_3_consecutive
                FROM team_percentage tp
                FULL OUTER JOIN individuals_across_weeks iw
                  ON tp.manager_id = iw.manager_id
                 AND tp.month_start = iw.month_start
                 AND tp.alert_id = iw.alert_id
            ),

            severity_assigned AS (
                SELECT
                    manager_id,
                    month_start,
                    alert_id,
                    team_affected_days,
                    team_size,
                    ROUND(percentage_affected, 1) AS percentage_affected,
                    individuals_with_3_consecutive,
                    CASE
                        WHEN (percentage_affected >= 60 AND team_affected_days >= 6)
                             OR individuals_with_3_consecutive >= 3 THEN 'HIGH'
                        WHEN (percentage_affected >= 40 AND percentage_affected < 60 AND team_affected_days >= 6)
                             OR individuals_with_3_consecutive >= 2 THEN 'MEDIUM'
                        WHEN (percentage_affected >= 20 AND percentage_affected < 40 AND team_affected_days >= 6)
                             OR individuals_with_3_consecutive >= 1 THEN 'LOW'
                        ELSE NULL
                    END AS severity
                FROM combined_metrics
                WHERE (percentage_affected >= 20 AND team_affected_days >= 6)
                   OR individuals_with_3_consecutive >= 1
            ),

            final_results AS (
                SELECT
                    manager_id,
                    month_start,
                    MAX(team_size) AS team_size,
                    named_struct(
                        'month_team_patterns',
                        collect_list(
                            named_struct(
                                'alert', alert_id,
                                'severity', severity,
                                'metrics',
                                named_struct(
                                    'team_affected_days', team_affected_days,
                                    'team_size', team_size,
                                    'percentage_affected', percentage_affected,
                                    'individuals_with_3_consecutive',
                                        individuals_with_3_consecutive
                                )
                            )
                        ),
                        'summary',
                        named_struct(
                            'total_alerts', COUNT(*),
                            'calculated_at', current_timestamp()
                        )
                    ) AS alerts_json,
                    current_timestamp() AS calculated_at
                FROM severity_assigned
                WHERE severity IS NOT NULL
                GROUP BY manager_id, month_start
            )

            SELECT * FROM final_results
        """)

        # ------------------------------------------------------------
        # Step 2: MERGE into dashboard.manager_reports_monthly
        # ------------------------------------------------------------

        spark.sql("""
            MERGE INTO dashboard.manager_reports_monthly target
            USING month_mgr_alerts_calc source
            ON target.manager_id = source.manager_id
           AND target.month_start = source.month_start
            WHEN MATCHED THEN
              UPDATE SET
                target.alerts_json = source.alerts_json,
                target.team_size = source.team_size,
                target.updated_on = source.calculated_at
            WHEN NOT MATCHED THEN
              INSERT (
                manager_id,
                month_start,
                team_size,
                alerts_json,
                updated_on
              )
              VALUES (
                source.manager_id,
                source.month_start,
                source.team_size,
                source.alerts_json,
                source.calculated_at
              )
        """)

        processed = spark.sql(
            "SELECT COUNT(*) AS cnt FROM month_mgr_alerts_calc"
        ).first().cnt

        print("✓ Month Manager Pipeline completed successfully")
        print(f"✓ Records processed: {processed}")

        return {
            "status": "success",
            "processed": processed
        }

    finally:
        spark.stop()





# month_emp_pipeline.py

from pyspark.sql import SparkSession


def run_month_emp_pipeline():
    """
    Month Employee Alerts Pipeline
    - Single pass
    - STRUCT based
    - No loops
    """

    spark = (
        SparkSession.builder
        .appName("MonthEmpPipeline")
        .enableHiveSupport()
        .getOrCreate()
    )

    try:
        print("▶ Starting Month Employee Pipeline")

        # ------------------------------------------------------------
        # Step 1: Build monthly alerts in ONE SQL
        # ------------------------------------------------------------

        spark.sql("""
            CREATE OR REPLACE TEMP VIEW month_emp_alerts_calc AS
            WITH emp_months_to_process AS (
                SELECT DISTINCT
                    emp_id,
                    month_start_date
                FROM month_emp
                WHERE updated_on > current_timestamp() - INTERVAL 4 HOURS
                  AND emp_id IS NOT NULL
                  AND month_start_date IS NOT NULL
            ),

            month_bounds AS (
                SELECT
                    emp_id,
                    month_start_date,
                    CASE
                        -- current month → yesterday
                        WHEN month_start_date = DATE_TRUNC('MONTH', current_date())
                             THEN DATE_SUB(current_date(), 1)
                        -- past months → last day of month
                        ELSE LAST_DAY(month_start_date)
                    END AS month_end_date
                FROM emp_months_to_process
            ),

            month_data AS (
                SELECT
                    d.emp_id,
                    m.month_start_date,
                    d.shift_date,
                    DATE_TRUNC('WEEK', d.shift_date) AS week_start,
                    d.alerts_json.triggered_alerts AS alert_ids
                FROM day_emp d
                JOIN month_bounds m
                  ON d.emp_id = m.emp_id
                 AND d.shift_date BETWEEN m.month_start_date AND m.month_end_date
                WHERE d.alerts_json.triggered_alerts IS NOT NULL
                  AND SIZE(d.alerts_json.triggered_alerts) > 0
            ),

            exploded_data AS (
                SELECT
                    emp_id,
                    month_start_date,
                    shift_date,
                    week_start,
                    EXPLODE(alert_ids) AS alert_id
                FROM month_data
            ),

            weekly_streak_base AS (
                SELECT
                    emp_id,
                    month_start_date,
                    alert_id,
                    week_start,
                    shift_date,
                    DATE_SUB(
                        shift_date,
                        ROW_NUMBER() OVER (
                            PARTITION BY emp_id, month_start_date, alert_id, week_start
                            ORDER BY shift_date
                        )
                    ) AS grp
                FROM exploded_data
            ),

            weekly_stats AS (
                SELECT
                    emp_id,
                    month_start_date,
                    alert_id,
                    week_start,
                    COUNT(DISTINCT shift_date) AS affected_days_in_week,
                    COUNT(*) AS max_consecutive_in_week
                FROM weekly_streak_base
                GROUP BY emp_id, month_start_date, alert_id, week_start, grp
            ),

            weekly_aggregated AS (
                SELECT
                    emp_id,
                    month_start_date,
                    alert_id,
                    week_start,
                    MAX(affected_days_in_week) AS affected_days_in_week,
                    MAX(max_consecutive_in_week) AS max_consecutive_in_week
                FROM weekly_stats
                GROUP BY emp_id, month_start_date, alert_id, week_start
            ),

            monthly_aggregates AS (
                SELECT
                    emp_id,
                    month_start_date,
                    alert_id,
                    COUNT(DISTINCT shift_date) AS total_affected_days,
                    COLLECT_LIST(
                        named_struct(
                            'week_start', week_start,
                            'affected_days', affected_days_in_week,
                            'consecutive_days', max_consecutive_in_week
                        )
                    ) AS weekly_patterns
                FROM exploded_data e
                JOIN weekly_aggregated w
                  ON e.emp_id = w.emp_id
                 AND e.month_start_date = w.month_start_date
                 AND e.alert_id = w.alert_id
                 AND e.week_start = w.week_start
                GROUP BY emp_id, month_start_date, alert_id
            ),

            severity_assigned AS (
                SELECT
                    emp_id,
                    month_start_date,
                    alert_id,
                    total_affected_days,
                    weekly_patterns,
                    CASE
                        WHEN total_affected_days >= 18 THEN 'HIGH'
                        WHEN total_affected_days >= 12 THEN 'MEDIUM'
                        WHEN total_affected_days >= 6  THEN 'LOW'
                        ELSE NULL
                    END AS severity
                FROM monthly_aggregates
                WHERE total_affected_days >= 6
            ),

            final_results AS (
                SELECT
                    emp_id,
                    month_start_date,
                    named_struct(
                        'month_patterns',
                        collect_list(
                            named_struct(
                                'alert', alert_id,
                                'severity', severity,
                                'total_days', total_affected_days,
                                'weekly_patterns', weekly_patterns
                            )
                        ),
                        'summary',
                        named_struct(
                            'total_alerts', COUNT(*),
                            'calculated_at', current_timestamp()
                        )
                    ) AS alerts_json,
                    current_timestamp() AS calculated_at
                FROM severity_assigned
                WHERE severity IS NOT NULL
                GROUP BY emp_id, month_start_date
            )

            SELECT * FROM final_results
        """)

        # ------------------------------------------------------------
        # Step 2: MERGE into month_emp
        # ------------------------------------------------------------

        spark.sql("""
            MERGE INTO month_emp target
            USING month_emp_alerts_calc source
            ON target.emp_id = source.emp_id
           AND target.month_start_date = source.month_start_date
            WHEN MATCHED THEN
              UPDATE SET
                target.alerts_json = source.alerts_json,
                target.updated_on = source.calculated_at
            WHEN NOT MATCHED THEN
              INSERT (
                emp_id,
                month_start_date,
                alerts_json,
                updated_on
              )
              VALUES (
                source.emp_id,
                source.month_start_date,
                source.alerts_json,
                source.calculated_at
              )
        """)

        processed = spark.sql(
            "SELECT COUNT(*) AS cnt FROM month_emp_alerts_calc"
        ).first().cnt

        print("✓ Month Employee Pipeline completed successfully")
        print(f"✓ Records processed: {processed}")

        return {
            "status": "success",
            "processed": processed
        }

    finally:
        spark.stop()


ALTER TABLE month_emp
ADD COLUMNS (
  alerts_json STRUCT<
    month_patterns: ARRAY<STRUCT<
      alert: INT,
      severity: STRING,
      total_days: INT,
      weekly_patterns: ARRAY<STRUCT<
        week_start: DATE,
        affected_days: INT,
        consecutive_days: INT
      >>
    >>,
    summary: STRUCT<
      total_alerts: INT,
      calculated_at: TIMESTAMP
    >
  >
);


# week_mgr_pipeline.py

from pyspark.sql import SparkSession


def run_week_mgr_pipeline():
    """
    Weekly Manager Alerts Pipeline
    - Single pass
    - STRUCT based
    - Correct week_end logic
    """

    spark = (
        SparkSession.builder
        .appName("WeekMgrPipeline")
        .enableHiveSupport()
        .getOrCreate()
    )

    try:
        print("▶ Starting Weekly Manager Pipeline")

        # ------------------------------------------------------------
        # Step 1: Compute weekly manager alerts
        # ------------------------------------------------------------

        spark.sql("""
            CREATE OR REPLACE TEMP VIEW week_mgr_alerts_calc AS
            WITH mgr_weeks_to_process AS (
                SELECT DISTINCT
                    manager_id,
                    week_start
                FROM dashboard.mgr_reports_weekly
                WHERE updated_on > current_timestamp() - INTERVAL 4 HOURS
                  AND manager_id IS NOT NULL
                  AND week_start IS NOT NULL
            ),

            week_bounds AS (
                SELECT
                    manager_id,
                    week_start,
                    CASE
                        WHEN week_start = DATE_TRUNC('WEEK', current_date())
                             THEN DATE_SUB(current_date(), 1)
                        ELSE DATE_ADD(week_start, 6)
                    END AS week_end
                FROM mgr_weeks_to_process
            ),

            week_data AS (
                SELECT
                    d.manager_id,
                    d.emp_id,
                    w.week_start,
                    d.day_start,
                    d.alerts.triggered_alerts AS alert_ids
                FROM dashboard.emp_reports_daily d
                JOIN week_bounds w
                  ON d.manager_id = w.manager_id
                 AND d.day_start BETWEEN w.week_start AND w.week_end
                WHERE d.alerts.triggered_alerts IS NOT NULL
                  AND SIZE(d.alerts.triggered_alerts) > 0
            ),

            exploded_data AS (
                SELECT
                    manager_id,
                    emp_id,
                    week_start,
                    day_start,
                    EXPLODE(alert_ids) AS alert_id
                FROM week_data
            ),

            daily_team_sizes AS (
                SELECT
                    manager_id,
                    week_start,
                    day_start,
                    COUNT(DISTINCT emp_id) AS daily_team_size
                FROM week_data
                GROUP BY manager_id, week_start, day_start
            ),

            daily_stats AS (
                SELECT
                    e.manager_id,
                    e.week_start,
                    e.day_start,
                    e.alert_id,
                    COUNT(DISTINCT e.emp_id) AS daily_affected,
                    ts.daily_team_size,
                    (COUNT(DISTINCT e.emp_id) * 100.0 / ts.daily_team_size) AS daily_percentage
                FROM exploded_data e
                JOIN daily_team_sizes ts
                  ON e.manager_id = ts.manager_id
                 AND e.week_start = ts.week_start
                 AND e.day_start = ts.day_start
                GROUP BY
                    e.manager_id,
                    e.week_start,
                    e.day_start,
                    e.alert_id,
                    ts.daily_team_size
            ),

            weekly_patterns AS (
                SELECT
                    manager_id,
                    week_start,
                    alert_id,
                    MAX(daily_percentage) AS max_daily_percent,
                    SUM(CASE WHEN daily_percentage >= 20 THEN 1 ELSE 0 END) AS days_above_20,
                    SUM(CASE WHEN daily_percentage >= 40 THEN 1 ELSE 0 END) AS days_above_40,
                    SUM(CASE WHEN daily_percentage >= 60 THEN 1 ELSE 0 END) AS days_above_60
                FROM daily_stats
                GROUP BY manager_id, week_start, alert_id
            ),

            individual_streaks AS (
                SELECT
                    manager_id,
                    alert_id,
                    emp_id,
                    MAX(consecutive_days) AS max_consecutive
                FROM (
                    SELECT
                        manager_id,
                        emp_id,
                        alert_id,
                        day_start,
                        COUNT(*) OVER (
                            PARTITION BY manager_id, emp_id, alert_id, grp
                            ORDER BY day_start
                        ) AS consecutive_days
                    FROM (
                        SELECT
                            manager_id,
                            emp_id,
                            alert_id,
                            day_start,
                            DATE_SUB(
                                day_start,
                                ROW_NUMBER() OVER (
                                    PARTITION BY manager_id, emp_id, alert_id
                                    ORDER BY day_start
                                )
                            ) AS grp
                        FROM exploded_data
                    ) t
                ) t
                GROUP BY manager_id, alert_id, emp_id
            ),

            individual_patterns AS (
                SELECT
                    manager_id,
                    alert_id,
                    COUNT(DISTINCT emp_id) AS employees_3_consecutive
                FROM individual_streaks
                WHERE max_consecutive >= 3
                GROUP BY manager_id, alert_id
            ),

            combined_metrics AS (
                SELECT
                    wp.manager_id,
                    wp.week_start,
                    wp.alert_id,
                    wp.max_daily_percent,
                    wp.days_above_20,
                    COALESCE(ip.employees_3_consecutive, 0) AS employees_3_consecutive,
                    CASE
                        WHEN (wp.max_daily_percent >= 60 AND wp.days_above_60 >= 3)
                             OR COALESCE(ip.employees_3_consecutive, 0) >= 3 THEN 'HIGH'
                        WHEN (wp.max_daily_percent >= 40 AND wp.days_above_40 >= 3)
                             OR COALESCE(ip.employees_3_consecutive, 0) >= 2 THEN 'MEDIUM'
                        WHEN (wp.max_daily_percent >= 20 AND wp.days_above_20 >= 1)
                             OR COALESCE(ip.employees_3_consecutive, 0) >= 1 THEN 'LOW'
                        ELSE NULL
                    END AS severity
                FROM weekly_patterns wp
                LEFT JOIN individual_patterns ip
                  ON wp.manager_id = ip.manager_id
                 AND wp.alert_id = ip.alert_id
            ),

            final_results AS (
                SELECT
                    manager_id,
                    week_start,
                    named_struct(
                        'team_patterns',
                        collect_list(
                            named_struct(
                                'alert', alert_id,
                                'severity', severity,
                                'max_daily_percent', ROUND(max_daily_percent, 1),
                                'days_above_20', days_above_20,
                                'employees_3_consecutive', employees_3_consecutive
                            )
                        ),
                        'summary',
                        named_struct(
                            'total_alerts', COUNT(*),
                            'calculated_at', current_timestamp()
                        )
                    ) AS alerts,
                    current_timestamp() AS calculated_at
                FROM combined_metrics
                WHERE severity IS NOT NULL
                GROUP BY manager_id, week_start
            )

            SELECT * FROM final_results
        """)

        # ------------------------------------------------------------
        # Step 2: MERGE into weekly manager table
        # ------------------------------------------------------------

        spark.sql("""
            MERGE INTO dashboard.mgr_reports_weekly target
            USING week_mgr_alerts_calc source
            ON target.manager_id = source.manager_id
           AND target.week_start = source.week_start
            WHEN MATCHED THEN
              UPDATE SET
                target.alerts = source.alerts,
                target.updated_on = source.calculated_at
            WHEN NOT MATCHED THEN
              INSERT (
                manager_id,
                week_start,
                alerts,
                updated_on
              )
              VALUES (
                source.manager_id,
                source.week_start,
                source.alerts,
                source.calculated_at
              )
        """)

        processed = spark.sql(
            "SELECT COUNT(*) AS cnt FROM week_mgr_alerts_calc"
        ).first().cnt

        print("✓ Weekly Manager Pipeline completed successfully")
        print(f"✓ Records processed: {processed}")

        return {
            "status": "success",
            "processed": processed
        }

    finally:
        spark.stop()


ALTER TABLE dashboard.mgr_reports_weekly
ADD COLUMNS (
  alerts STRUCT<
    team_patterns: ARRAY<STRUCT<
      alert: INT,
      severity: STRING,
      max_daily_percent: DOUBLE,
      days_above_20: INT,
      employees_3_consecutive: INT
    >>,
    summary: STRUCT<
      total_alerts: INT,
      calculated_at: TIMESTAMP
    >
  >
);




# week_emp_pipeline.py

from pyspark.sql import SparkSession


def run_week_emp_pipeline():
    """
    Weekly Employee Alerts Pipeline
    - Single pass
    - STRUCT based
    - emp_id + week_start driven
    - Correct week_end handling
    """

    spark = (
        SparkSession.builder
        .appName("WeekEmpPipeline")
        .enableHiveSupport()
        .getOrCreate()
    )

    try:
        print("▶ Starting Weekly Employee Pipeline")

        # ------------------------------------------------------------
        # Step 1: Build weekly alerts in ONE SQL
        # ------------------------------------------------------------

        spark.sql("""
            CREATE OR REPLACE TEMP VIEW week_emp_alerts_calc AS
            WITH emp_weeks_to_process AS (
                SELECT DISTINCT
                    emp_id,
                    week_start
                FROM dashboard.emp_reports_weekly
                WHERE updated_on > current_timestamp() - INTERVAL 4 HOURS
                  AND emp_id IS NOT NULL
                  AND week_start IS NOT NULL
            ),

            week_bounds AS (
                SELECT
                    emp_id,
                    week_start,
                    CASE
                        -- current week → yesterday
                        WHEN week_start = DATE_TRUNC('WEEK', current_date())
                             THEN DATE_SUB(current_date(), 1)
                        -- past weeks → Sunday
                        ELSE DATE_ADD(week_start, 6)
                    END AS week_end
                FROM emp_weeks_to_process
            ),

            week_data AS (
                SELECT
                    d.emp_id,
                    w.week_start,
                    d.day_start,
                    d.alerts.triggered_alerts AS alert_ids
                FROM dashboard.emp_reports_daily d
                JOIN week_bounds w
                  ON d.emp_id = w.emp_id
                 AND d.day_start BETWEEN w.week_start AND w.week_end
                WHERE d.alerts.triggered_alerts IS NOT NULL
                  AND SIZE(d.alerts.triggered_alerts) > 0
            ),

            exploded_data AS (
                SELECT
                    emp_id,
                    week_start,
                    day_start,
                    EXPLODE(alert_ids) AS alert_id
                FROM week_data
            ),

            streak_base AS (
                SELECT
                    emp_id,
                    week_start,
                    alert_id,
                    day_start,
                    DATE_SUB(
                        day_start,
                        ROW_NUMBER() OVER (
                            PARTITION BY emp_id, week_start, alert_id
                            ORDER BY day_start
                        )
                    ) AS grp
                FROM exploded_data
            ),

            streaks AS (
                SELECT
                    emp_id,
                    week_start,
                    alert_id,
                    COUNT(*) AS consecutive_count
                FROM streak_base
                GROUP BY emp_id, week_start, alert_id, grp
            ),

            patterns AS (
                SELECT
                    emp_id,
                    week_start,
                    alert_id,
                    COUNT(DISTINCT day_start) AS affected_days,
                    MAX(consecutive_count) AS max_consecutive
                FROM (
                    SELECT
                        e.emp_id,
                        e.week_start,
                        e.alert_id,
                        e.day_start,
                        s.consecutive_count
                    FROM exploded_data e
                    JOIN streaks s
                      ON e.emp_id = s.emp_id
                     AND e.week_start = s.week_start
                     AND e.alert_id = s.alert_id
                ) t
                GROUP BY emp_id, week_start, alert_id
            ),

            severity_assigned AS (
                SELECT
                    emp_id,
                    week_start,
                    alert_id,
                    affected_days,
                    max_consecutive,
                    CASE
                        WHEN max_consecutive >= 3 THEN 'HIGH'
                        WHEN max_consecutive >= 2 THEN 'MEDIUM'
                        WHEN affected_days >= 1 THEN 'LOW'
                        ELSE NULL
                    END AS severity
                FROM patterns
                WHERE affected_days > 0
            ),

            final_results AS (
                SELECT
                    emp_id,
                    week_start,
                    named_struct(
                        'alert_patterns',
                        collect_list(
                            named_struct(
                                'alert', alert_id,
                                'severity', severity,
                                'affected_days', affected_days,
                                'consecutive_days', max_consecutive
                            )
                        ),
                        'summary',
                        named_struct(
                            'total_alerts', COUNT(*),
                            'calculated_at', current_timestamp()
                        )
                    ) AS alerts,
                    current_timestamp() AS calculated_at
                FROM severity_assigned
                WHERE severity IS NOT NULL
                GROUP BY emp_id, week_start
            )

            SELECT * FROM final_results
        """)

        # ------------------------------------------------------------
        # Step 2: MERGE into weekly table
        # ------------------------------------------------------------

        spark.sql("""
            MERGE INTO dashboard.emp_reports_weekly target
            USING week_emp_alerts_calc source
            ON target.emp_id = source.emp_id
           AND target.week_start = source.week_start
            WHEN MATCHED THEN
              UPDATE SET
                target.alerts = source.alerts,
                target.updated_on = source.calculated_at
            WHEN NOT MATCHED THEN
              INSERT (
                emp_id,
                week_start,
                alerts,
                updated_on
              )
              VALUES (
                source.emp_id,
                source.week_start,
                source.alerts,
                source.calculated_at
              )
        """)

        # ------------------------------------------------------------
        # Step 3: Metrics
        # ------------------------------------------------------------

        processed = spark.sql(
            "SELECT COUNT(*) AS cnt FROM week_emp_alerts_calc"
        ).first().cnt

        print("✓ Weekly Employee Pipeline completed successfully")
        print(f"✓ Records processed: {processed}")

        return {
            "status": "success",
            "processed": processed
        }

    finally:
        spark.stop()




# week_emp_pipeline.py

from pyspark.sql import SparkSession


def run_week_emp_pipeline():
    """
    Week Employee Alerts Pipeline
    - Single pass
    - STRUCT based
    - No for-loop
    """

    spark = (
        SparkSession.builder
        .appName("WeekEmpPipeline")
        .enableHiveSupport()
        .getOrCreate()
    )

    try:
        print("▶ Starting Week Employee Pipeline")

        # ------------------------------------------------------------
        # Step 1: Calculate weekly alerts in ONE SQL
        # ------------------------------------------------------------

        spark.sql("""
            CREATE OR REPLACE TEMP VIEW week_emp_alerts_calc AS
            WITH weeks_to_process AS (
                SELECT DISTINCT
                    week_start_date
                FROM week_emp
                WHERE updated_on > current_timestamp() - INTERVAL 4 HOURS
                  AND week_start_date IS NOT NULL
            ),

            week_bounds AS (
                SELECT
                    week_start_date,
                    LEAST(
                        DATE_ADD(week_start_date, 6),
                        DATE_SUB(current_date(), 1)
                    ) AS week_end_date
                FROM weeks_to_process
            ),

            week_data AS (
                SELECT
                    de.emp_id,
                    wb.week_start_date,
                    de.shift_date,
                    de.alerts.triggered_alerts AS alert_ids
                FROM day_emp de
                JOIN week_bounds wb
                  ON de.shift_date BETWEEN wb.week_start_date AND wb.week_end_date
                WHERE de.alerts.triggered_alerts IS NOT NULL
                  AND SIZE(de.alerts.triggered_alerts) > 0
            ),

            exploded_data AS (
                SELECT
                    emp_id,
                    week_start_date,
                    shift_date,
                    EXPLODE(alert_ids) AS alert_id
                FROM week_data
            ),

            streak_base AS (
                SELECT
                    emp_id,
                    week_start_date,
                    alert_id,
                    shift_date,
                    DATE_SUB(
                        shift_date,
                        ROW_NUMBER() OVER (
                            PARTITION BY emp_id, week_start_date, alert_id
                            ORDER BY shift_date
                        )
                    ) AS grp
                FROM exploded_data
            ),

            streaks AS (
                SELECT
                    emp_id,
                    week_start_date,
                    alert_id,
                    COUNT(*) AS consecutive_count
                FROM streak_base
                GROUP BY emp_id, week_start_date, alert_id, grp
            ),

            patterns AS (
                SELECT
                    emp_id,
                    week_start_date,
                    alert_id,
                    COUNT(DISTINCT shift_date) AS affected_days,
                    MAX(consecutive_count) AS max_consecutive
                FROM (
                    SELECT
                        e.emp_id,
                        e.week_start_date,
                        e.alert_id,
                        e.shift_date,
                        s.consecutive_count
                    FROM exploded_data e
                    JOIN streaks s
                      ON e.emp_id = s.emp_id
                     AND e.week_start_date = s.week_start_date
                     AND e.alert_id = s.alert_id
                ) t
                GROUP BY emp_id, week_start_date, alert_id
            ),

            severity_assigned AS (
                SELECT
                    emp_id,
                    week_start_date,
                    alert_id,
                    affected_days,
                    max_consecutive,
                    CASE
                        WHEN max_consecutive >= 3 THEN 'HIGH'
                        WHEN max_consecutive >= 2 THEN 'MEDIUM'
                        WHEN affected_days >= 1 THEN 'LOW'
                        ELSE NULL
                    END AS severity
                FROM patterns
                WHERE affected_days > 0
            ),

            final_results AS (
                SELECT
                    emp_id,
                    week_start_date,
                    named_struct(
                        'alert_patterns',
                        collect_list(
                            named_struct(
                                'alert', alert_id,
                                'severity', severity,
                                'affected_days', affected_days,
                                'consecutive_days', max_consecutive
                            )
                        ),
                        'summary',
                        named_struct(
                            'total_alerts', COUNT(*),
                            'calculated_at', current_timestamp()
                        )
                    ) AS alerts,
                    current_timestamp() AS calculated_at
                FROM severity_assigned
                WHERE severity IS NOT NULL
                GROUP BY emp_id, week_start_date
            )

            SELECT * FROM final_results
        """)

        # ------------------------------------------------------------
        # Step 2: Merge into week_emp
        # ------------------------------------------------------------

        spark.sql("""
            MERGE INTO week_emp target
            USING week_emp_alerts_calc source
            ON target.emp_id = source.emp_id
           AND target.week_start_date = source.week_start_date
            WHEN MATCHED THEN
              UPDATE SET
                target.alerts = source.alerts,
                target.updated_on = source.calculated_at
            WHEN NOT MATCHED THEN
              INSERT (
                emp_id,
                week_start_date,
                alerts,
                updated_on
              )
              VALUES (
                source.emp_id,
                source.week_start_date,
                source.alerts,
                source.calculated_at
              )
        """)

        # ------------------------------------------------------------
        # Step 3: Metrics
        # ------------------------------------------------------------

        processed = spark.sql(
            "SELECT COUNT(*) AS cnt FROM week_emp_alerts_calc"
        ).first().cnt

        print("✓ Week Employee Pipeline completed successfully")
        print(f"✓ Records processed: {processed}")

        return {
            "status": "success",
            "processed": processed
        }

    finally:
        spark.stop()



ALTER TABLE week_emp
ADD COLUMNS (
  alerts STRUCT<
    alert_patterns: ARRAY<STRUCT<
      alert: INT,
      severity: STRING,
      affected_days: INT,
      consecutive_days: INT
    >>,
    summary: STRUCT<
      total_alerts: INT,
      calculated_at: TIMESTAMP
    >
  >
);




# day_mgr_pipeline_query.py

from pyspark.sql import SparkSession


def run_day_mgr_pipeline_query():
    """
    Day Manager Alerts Pipeline
    - Single pass
    - No per-date loop
    - Processes day_alerts updated in last 4 hours
    """

    spark = (
        SparkSession.builder
        .appName("DayMgrPipelineQuery")
        .enableHiveSupport()
        .getOrCreate()
    )

    try:
        print("▶ Starting Day Manager Alerts Pipeline")

        # ------------------------------------------------------------------
        # Step 1: Compute manager alerts (single SQL)
        # ------------------------------------------------------------------

        spark.sql("""
            CREATE OR REPLACE TEMP VIEW day_mgr_alerts_calc AS
            WITH employee_alerts AS (
                SELECT
                    da.emp_id,
                    da.shift_date,
                    de.manager_id,
                    FROM_JSON(
                        GET_JSON_OBJECT(da.alerts_json, '$.triggered_alerts'),
                        'array<int>'
                    ) AS alert_ids
                FROM day_alerts da
                JOIN day_emp de
                  ON da.emp_id = de.emp_id
                 AND da.shift_date = de.shift_date
                WHERE da.updated_on > current_timestamp() - INTERVAL 4 HOURS
                  AND de.manager_id IS NOT NULL
            ),

            exploded_alerts AS (
                SELECT
                    manager_id,
                    shift_date,
                    emp_id,
                    EXPLODE(alert_ids) AS alert_id
                FROM employee_alerts
                WHERE alert_ids IS NOT NULL
                  AND SIZE(alert_ids) > 0
            ),

            team_sizes AS (
                SELECT
                    manager_id,
                    shift_date,
                    COUNT(DISTINCT emp_id) AS team_size
                FROM day_emp
                WHERE manager_id IS NOT NULL
                GROUP BY manager_id, shift_date
            ),

            alert_counts AS (
                SELECT
                    ea.manager_id,
                    ea.shift_date,
                    ea.alert_id,
                    COUNT(DISTINCT ea.emp_id) AS affected_count,
                    ts.team_size
                FROM exploded_alerts ea
                JOIN team_sizes ts
                  ON ea.manager_id = ts.manager_id
                 AND ea.shift_date = ts.shift_date
                GROUP BY
                    ea.manager_id,
                    ea.shift_date,
                    ea.alert_id,
                    ts.team_size
            ),

            alert_metrics AS (
                SELECT
                    manager_id,
                    shift_date,
                    alert_id,
                    affected_count,
                    team_size,
                    (affected_count * 100.0 / team_size) AS percentage,
                    CASE
                        WHEN (affected_count * 100.0 / team_size) >= 60 THEN 'HIGH'
                        WHEN (affected_count * 100.0 / team_size) >= 40 THEN 'MEDIUM'
                        WHEN (affected_count * 100.0 / team_size) >= 20 THEN 'LOW'
                        ELSE NULL
                    END AS severity
                FROM alert_counts
            ),

            manager_alerts AS (
                SELECT
                    manager_id,
                    shift_date,
                    MAX(team_size) AS team_size,
                    to_json(
                        named_struct(
                            'team_alerts',
                            collect_list(
                                named_struct(
                                    'alert', alert_id,
                                    'severity', severity,
                                    'affected_employees', affected_count,
                                    'team_size', team_size,
                                    'percentage', ROUND(percentage, 1)
                                )
                            ),
                            'summary',
                            named_struct(
                                'total_alerts', COUNT(*),
                                'calculated_at', current_timestamp()
                            )
                        )
                    ) AS alerts_json,
                    current_timestamp() AS calculated_at
                FROM alert_metrics
                WHERE severity IS NOT NULL
                GROUP BY manager_id, shift_date
            )

            SELECT * FROM manager_alerts
        """)

        # ------------------------------------------------------------------
        # Step 2: Merge into day_mgr_alerts
        # ------------------------------------------------------------------

        spark.sql("""
            MERGE INTO day_mgr_alerts target
            USING day_mgr_alerts_calc source
            ON target.manager_id = source.manager_id
           AND target.shift_date = source.shift_date
            WHEN MATCHED THEN
                UPDATE SET
                    target.alerts_json = source.alerts_json,
                    target.team_size = source.team_size,
                    target.updated_on = source.calculated_at
            WHEN NOT MATCHED THEN
                INSERT (
                    manager_id,
                    shift_date,
                    team_size,
                    alerts_json,
                    updated_on
                )
                VALUES (
                    source.manager_id,
                    source.shift_date,
                    source.team_size,
                    source.alerts_json,
                    source.calculated_at
                )
        """)

        # ------------------------------------------------------------------
        # Step 3: Metrics
        # ------------------------------------------------------------------

        processed_count = spark.sql(
            "SELECT COUNT(*) AS cnt FROM day_mgr_alerts_calc"
        ).first().cnt

        print("✓ Day Manager Pipeline completed successfully")
        print(f"✓ Manager records processed: {processed_count}")

        return {
            "status": "success",
            "processed": processed_count
        }

    finally:
        spark.stop()




Got it 👍
Since here also the column is a STRUCT, the rule is the same as before:

> Do NOT use to_json() or JSON parsing functions at all.
Work with the STRUCT directly.



Below is exactly what to change and what the final command should look like.


---

✅ Assumption (important)

Your table looks like this:

day_alerts.alerts STRUCT<
  triggered_alerts: ARRAY<INT>,
  summary: STRUCT<
    total_alerts: INT,
    calculated_at: TIMESTAMP
  >
>

And day_mgr_alerts.alerts is also a STRUCT (recommended).


---

❌ What you must NOT use anymore

Remove both of these:

GET_JSON_OBJECT(...)
FROM_JSON(...)
to_json(...)
CONCAT(...)

They are only for STRING JSON.


---

✅ Correct command when column is STRUCT

1️⃣ Reading employee alerts (STRUCT access)

❌ Old (JSON)

FROM_JSON(
    GET_JSON_OBJECT(da.alerts_json, '$.triggered_alerts'),
    'array<int>'
) AS alert_ids

✅ New (STRUCT)

da.alerts.triggered_alerts AS alert_ids


---

2️⃣ Building manager alerts (STRUCT, not JSON)

❌ Old (string concat / to_json)

to_json(
    named_struct(
        'team_alerts', ...
    )
) AS alerts_json

✅ New (STRUCT)

named_struct(
    'team_alerts',
    collect_list(
        named_struct(
            'alert', alert_id,
            'severity', severity,
            'affected_employees', affected_count,
            'team_size', team_size,
            'percentage', ROUND(percentage, 1)
        )
    ),
    'summary',
    named_struct(
        'total_alerts', COUNT(*),
        'calculated_at', current_timestamp()
    )
) AS alerts


---

✅ Final corrected core SQL (STRUCT → STRUCT)

CREATE OR REPLACE TEMP VIEW day_mgr_alerts_calc AS
WITH employee_alerts AS (
    SELECT
        da.emp_id,
        da.shift_date,
        de.manager_id,
        da.alerts.triggered_alerts AS alert_ids
    FROM day_alerts da
    JOIN day_emp de
      ON da.emp_id = de.emp_id
     AND da.shift_date = de.shift_date
    WHERE da.updated_on > current_timestamp() - INTERVAL 4 HOURS
      AND de.manager_id IS NOT NULL
),

exploded_alerts AS (
    SELECT
        manager_id,
        shift_date,
        emp_id,
        EXPLODE(alert_ids) AS alert_id
    FROM employee_alerts
    WHERE alert_ids IS NOT NULL
      AND SIZE(alert_ids) > 0
),

team_sizes AS (
    SELECT
        manager_id,
        shift_date,
        COUNT(DISTINCT emp_id) AS team_size
    FROM day_emp
    WHERE manager_id IS NOT NULL
    GROUP BY manager_id, shift_date
),

alert_counts AS (
    SELECT
        ea.manager_id,
        ea.shift_date,
        ea.alert_id,
        COUNT(DISTINCT ea.emp_id) AS affected_count,
        ts.team_size
    FROM exploded_alerts ea
    JOIN team_sizes ts
      ON ea.manager_id = ts.manager_id
     AND ea.shift_date = ts.shift_date
    GROUP BY
        ea.manager_id,
        ea.shift_date,
        ea.alert_id,
        ts.team_size
),

alert_metrics AS (
    SELECT
        manager_id,
        shift_date,
        alert_id,
        affected_count,
        team_size,
        (affected_count * 100.0 / team_size) AS percentage,
        CASE
            WHEN (affected_count * 100.0 / team_size) >= 60 THEN 'HIGH'
            WHEN (affected_count * 100.0 / team_size) >= 40 THEN 'MEDIUM'
            WHEN (affected_count * 100.0 / team_size) >= 20 THEN 'LOW'
            ELSE NULL
        END AS severity
    FROM alert_counts
),

manager_alerts AS (
    SELECT
        manager_id,
        shift_date,
        MAX(team_size) AS team_size,
        named_struct(
            'team_alerts',
            collect_list(
                named_struct(
                    'alert', alert_id,
                    'severity', severity,
                    'affected_employees', affected_count,
                    'team_size', team_size,
                    'percentage', ROUND(percentage, 1)
                )
            ),
            'summary',
            named_struct(
                'total_alerts', COUNT(*),
                'calculated_at', current_timestamp()
            )
        ) AS alerts,
        current_timestamp() AS calculated_at
    FROM alert_metrics
    WHERE severity IS NOT NULL
    GROUP BY manager_id, shift_date
)

SELECT * FROM manager_alerts;


---

✅ MERGE command (STRUCT column)

MERGE INTO day_mgr_alerts target
USING day_mgr_alerts_calc source
ON target.manager_id = source.manager_id
AND target.shift_date = source.shift_date
WHEN MATCHED THEN
  UPDATE SET
    target.alerts = source.alerts,
    target.team_size = source.team_size,
    target.updated_on = source.calculated_at
WHEN NOT MATCHED THEN
  INSERT (
    manager_id,
    shift_date,
    team_size,
    alerts,
    updated_on
  )
  VALUES (
    source.manager_id,
    source.shift_date,
    source.team_size,
    source.alerts,
    source.calculated_at
  );


---

🏁 Final takeaway

✔ STRUCT → access with dot notation
✔ STRUCT → build with named_struct
✔ NO to_json, from_json, get_json_object
✔ Cleaner, faster, safer

If you want, next I can:

Show how to query STRUCT alerts

Convert existing STRING JSON → STRUCT

Add Z-ORDER optimizations

Validate schema mismatches


Just tell me 👍







from pyspark.sql.functions import udf
from pyspark.sql.types import StringType, ArrayType, IntegerType
import json
from datetime import datetime

# UDF for Employee Day Alert Calculation
def calculate_employee_day_alerts(active_time, idle_time, lock_time, 
                                 expected_shift_time, expected_break_time):
    """
    Calculate day alerts for an employee and return JSON string
    """
    # Calculate active percentage
    total_time = active_time + idle_time + lock_time
    active_percent = (active_time * 100.0 / total_time) if total_time > 0 else 0
    
    triggered_alerts = []
    
    # Alert 1: Few Breaks
    if (idle_time + lock_time) < expected_break_time:
        triggered_alerts.append(1)
    
    # Alert 2: Low Active Time
    if active_time < expected_shift_time:
        triggered_alerts.append(2)
    
    # Alert 3: High Active Time
    if active_time > expected_shift_time:
        triggered_alerts.append(3)
    
    # Alert 4: Low Active & High Idle Time
    if active_time < (0.9 * expected_shift_time) and active_percent < 90:
        triggered_alerts.append(4)
    
    # Alert 5: Low Active & Low Idle Time
    if active_time < (0.9 * expected_shift_time) and active_percent > 90:
        triggered_alerts.append(5)
    
    # Alert 6: High Active & High Idle Time
    if active_time > (1.1 * expected_shift_time) and idle_time > 1.0:
        triggered_alerts.append(6)
    
    # Alert 7: Few Breaks + Low Active & High Idle Time
    if ((idle_time + lock_time) < expected_break_time and 
        active_time < (0.9 * expected_shift_time) and 
        active_percent < 90):
        triggered_alerts.append(7)
    
    # Alert 8: Few Breaks + High Active & High Idle Time
    if ((idle_time + lock_time) < expected_break_time and 
        active_time > (1.1 * expected_shift_time) and 
        idle_time > 1.0):
        triggered_alerts.append(8)
    
    # Alert 9: Few Breaks + Low Active & Low Idle Time
    if ((idle_time + lock_time) < expected_break_time and 
        active_time < (0.9 * expected_shift_time) and 
        active_percent > 90):
        triggered_alerts.append(9)
    
    # Alert 10: Low Active + High Idle Time
    if (active_time < expected_shift_time and 
        active_time < (0.9 * expected_shift_time) and 
        active_percent < 90):
        triggered_alerts.append(10)
    
    # Alert 11: Low Active + Low Idle Time
    if (active_time < expected_shift_time and 
        active_time < (0.9 * expected_shift_time) and 
        active_percent > 90):
        triggered_alerts.append(11)
    
    # Alert 12: High Active + High Idle Time
    if (active_time > (1.1 * expected_shift_time) and 
        idle_time > 1.0 and 
        active_time > expected_shift_time):
        triggered_alerts.append(12)
    
    # Alert 13: Low Active + Few Breaks
    if ((idle_time + lock_time) < expected_break_time and 
        active_time < expected_shift_time):
        triggered_alerts.append(13)
    
    # Alert 14: High Active + Few Breaks
    if ((idle_time + lock_time) < expected_break_time and 
        active_time > expected_shift_time):
        triggered_alerts.append(14)
    
    # Create JSON structure exactly as you specified
    alerts_json = {
        "triggered_alerts": triggered_alerts,
        "summary": {
            "total_alerts": len(triggered_alerts),
            "calculated_at": datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ")
        }
    }
    
    return json.dumps(alerts_json)

# Register the UDF
employee_day_alerts_udf = udf(calculate_employee_day_alerts, StringType())

# Alternative: UDF that returns just alert IDs array
def get_employee_day_alert_ids(active_time, idle_time, lock_time, 
                              expected_shift_time, expected_break_time):
    """
    Return just the alert IDs array without JSON
    """
    # Calculate active percentage
    total_time = active_time + idle_time + lock_time
    active_percent = (active_time * 100.0 / total_time) if total_time > 0 else 0
    
    triggered_alerts = []
    
    # Alert 1: Few Breaks
    if (idle_time + lock_time) < expected_break_time:
        triggered_alerts.append(1)
    
    # Alert 2: Low Active Time
    if active_time < expected_shift_time:
        triggered_alerts.append(2)
    
    # Alert 3: High Active Time
    if active_time > expected_shift_time:
        triggered_alerts.append(3)
    
    # Alert 4: Low Active & High Idle Time
    if active_time < (0.9 * expected_shift_time) and active_percent < 90:
        triggered_alerts.append(4)
    
    # Alert 5: Low Active & Low Idle Time
    if active_time < (0.9 * expected_shift_time) and active_percent > 90:
        triggered_alerts.append(5)
    
    # Alert 6: High Active & High Idle Time
    if active_time > (1.1 * expected_shift_time) and idle_time > 1.0:
        triggered_alerts.append(6)
    
    # Alert 7: Few Breaks + Low Active & High Idle Time
    if ((idle_time + lock_time) < expected_break_time and 
        active_time < (0.9 * expected_shift_time) and 
        active_percent < 90):
        triggered_alerts.append(7)
    
    # Alert 8: Few Breaks + High Active & High Idle Time
    if ((idle_time + lock_time) < expected_break_time and 
        active_time > (1.1 * expected_shift_time) and 
        idle_time > 1.0):
        triggered_alerts.append(8)
    
    # Alert 9: Few Breaks + Low Active & Low Idle Time
    if ((idle_time + lock_time) < expected_break_time and 
        active_time < (0.9 * expected_shift_time) and 
        active_percent > 90):
        triggered_alerts.append(9)
    
    # Alert 10: Low Active + High Idle Time
    if (active_time < expected_shift_time and 
        active_time < (0.9 * expected_shift_time) and 
        active_percent < 90):
        triggered_alerts.append(10)
    
    # Alert 11: Low Active + Low Idle Time
    if (active_time < expected_shift_time and 
        active_time < (0.9 * expected_shift_time) and 
        active_percent > 90):
        triggered_alerts.append(11)
    
    # Alert 12: High Active + High Idle Time
    if (active_time > (1.1 * expected_shift_time) and 
        idle_time > 1.0 and 
        active_time > expected_shift_time):
        triggered_alerts.append(12)
    
    # Alert 13: Low Active + Few Breaks
    if ((idle_time + lock_time) < expected_break_time and 
        active_time < expected_shift_time):
        triggered_alerts.append(13)
    
    # Alert 14: High Active + Few Breaks
    if ((idle_time + lock_time) < expected_break_time and 
        active_time > expected_shift_time):
        triggered_alerts.append(14)
    
    return triggered_alerts

alert_ids_udf = udf(get_employee_day_alert_ids, ArrayType(IntegerType()))


from pyspark.sql.functions import udf
from pyspark.sql.types import StringType, MapType, StringType, IntegerType, FloatType
import json
from datetime import datetime

def calculate_manager_day_alerts(alert_ids_list, team_size):
    """
    Calculate manager day alerts from employee alert IDs
    alert_ids_list: list of alert ID arrays for all team members
    team_size: total number of team members
    """
    if not alert_ids_list or team_size == 0:
        return json.dumps({"team_alerts": [], "summary": {"total_alerts": 0, "calculated_at": datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ")}})
    
    # Count occurrences of each alert across team
    alert_counts = {}
    for alert_ids in alert_ids_list:
        if alert_ids:
            for alert_id in alert_ids:
                alert_counts[alert_id] = alert_counts.get(alert_id, 0) + 1
    
    team_alerts = []
    
    for alert_id, affected_count in alert_counts.items():
        percentage = (affected_count * 100.0) / team_size
        
        # Determine severity based on percentage
        if percentage >= 60:
            severity = "HIGH"
        elif percentage >= 40:
            severity = "MEDIUM"
        elif percentage >= 20:
            severity = "LOW"
        else:
            continue  # Skip alerts affecting <20% of team
        
        team_alerts.append({
            "alert": int(alert_id),
            "severity": severity,
            "affected_employees": int(affected_count),
            "team_size": int(team_size),
            "percentage": round(float(percentage), 1)
        })
    
    # Sort by alert ID
    team_alerts.sort(key=lambda x: x["alert"])
    
    # Create JSON structure exactly as you specified
    manager_json = {
        "team_alerts": team_alerts,
        "summary": {
            "total_alerts": len(team_alerts),
            "calculated_at": datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ")
        }
    }
    
    return json.dumps(manager_json)

# Register the UDF
manager_day_alerts_udf = udf(calculate_manager_day_alerts, StringType())

# Alternative: Simplified version if you have aggregated data
def calculate_manager_simple(alert_id_counts, team_size):
    """
    Simplified version where alert_id_counts is a map of {alert_id: count}
    """
    team_alerts = []
    
    for alert_id_str, count in alert_id_counts.items():
        if count:
            alert_id = int(alert_id_str)
            affected_count = int(count)
            percentage = (affected_count * 100.0) / team_size
            
            if percentage >= 60:
                severity = "HIGH"
            elif percentage >= 40:
                severity = "MEDIUM"
            elif percentage >= 20:
                severity = "LOW"
            else:
                continue
            
            team_alerts.append({
                "alert": alert_id,
                "severity": severity,
                "affected_employees": affected_count,
                "team_size": int(team_size),
                "percentage": round(float(percentage), 1)
            })
    
    manager_json = {
        "team_alerts": team_alerts,
        "summary": {
            "total_alerts": len(team_alerts),
            "calculated_at": datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ")
        }
    }
    
    return json.dumps(manager_json)

manager_simple_udf = udf(calculate_manager_simple, StringType())


from pyspark.sql import SparkSession
from pyspark.sql.functions import col, collect_list, current_timestamp
from datetime import datetime, timedelta

def run_day_pipeline_with_udf():
    spark = SparkSession.builder.appName("AlertPipelineUDF").getOrCreate()
    
    # 1. Employee Day Alerts
    print("Processing Employee Day Alerts...")
    
    # Get data
    employee_data = spark.sql("""
        SELECT 
            emp_id,
            shift_date,
            manager_id,
            active_time,
            idle_time,
            lock_time,
            expected_shift_time,
            expected_break_time
        FROM day_emp
        WHERE shift_date = CURRENT_DATE() - INTERVAL 1 DAY
    """)
    
    # Apply employee UDF
    employee_alerts = employee_data.withColumn(
        "alerts_json",
        employee_day_alerts_udf(
            col("active_time"),
            col("idle_time"),
            col("lock_time"),
            col("expected_shift_time"),
            col("expected_break_time")
        )
    ).withColumn(
        "updated_on",
        current_timestamp()
    )
    
    # Save to day_alerts table
    employee_alerts.select("emp_id", "shift_date", "alerts_json", "updated_on") \
        .write.mode("overwrite") \
        .saveAsTable("day_alerts")
    
    print(f"Employee alerts processed: {employee_alerts.count()}")
    
    # 2. Manager Day Alerts
    print("\nProcessing Manager Day Alerts...")
    
    # Aggregate by manager
    manager_data = spark.sql("""
        WITH employee_alerts AS (
            SELECT 
                de.emp_id,
                de.manager_id,
                de.shift_date,
                FROM_JSON(da.alerts_json, 'STRUCT<triggered_alerts:ARRAY<INT>, summary:STRUCT<total_alerts:INT, calculated_at:STRING>>') as alerts
            FROM day_alerts da
            JOIN day_emp de ON da.emp_id = de.emp_id AND da.shift_date = de.shift_date
            WHERE da.shift_date = CURRENT_DATE() - INTERVAL 1 DAY
            AND de.manager_id IS NOT NULL
        ),
        team_sizes AS (
            SELECT 
                manager_id,
                COUNT(DISTINCT emp_id) as team_size
            FROM day_emp
            WHERE shift_date = CURRENT_DATE() - INTERVAL 1 DAY
            GROUP BY manager_id
        )
        SELECT 
            ea.manager_id,
            ea.shift_date,
            COLLECT_LIST(alerts.triggered_alerts) as alert_arrays,
            ts.team_size
        FROM employee_alerts ea
        JOIN team_sizes ts ON ea.manager_id = ts.manager_id
        GROUP BY ea.manager_id, ea.shift_date, ts.team_size
    """)
    
    # Apply manager UDF
    manager_alerts = manager_data.withColumn(
        "alerts_json",
        manager_day_alerts_udf(col("alert_arrays"), col("team_size"))
    ).withColumn(
        "updated_on",
        current_timestamp()
    )
    
    # Save to day_mgr_alerts table
    manager_alerts.select("manager_id", "shift_date", "team_size", "alerts_json", "updated_on") \
        .write.mode("overwrite") \
        .saveAsTable("day_mgr_alerts")
    
    print(f"Manager alerts processed: {manager_alerts.count()}")
    
    spark.stop()
    return {
        "employee_records": employee_alerts.count(),
        "manager_records": manager_alerts.count()
    }
