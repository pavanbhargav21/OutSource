
Got it üëç
Since here also the column is a STRUCT, the rule is the same as before:

> Do NOT use to_json() or JSON parsing functions at all.
Work with the STRUCT directly.



Below is exactly what to change and what the final command should look like.


---

‚úÖ Assumption (important)

Your table looks like this:

day_alerts.alerts STRUCT<
  triggered_alerts: ARRAY<INT>,
  summary: STRUCT<
    total_alerts: INT,
    calculated_at: TIMESTAMP
  >
>

And day_mgr_alerts.alerts is also a STRUCT (recommended).


---

‚ùå What you must NOT use anymore

Remove both of these:

GET_JSON_OBJECT(...)
FROM_JSON(...)
to_json(...)
CONCAT(...)

They are only for STRING JSON.


---

‚úÖ Correct command when column is STRUCT

1Ô∏è‚É£ Reading employee alerts (STRUCT access)

‚ùå Old (JSON)

FROM_JSON(
    GET_JSON_OBJECT(da.alerts_json, '$.triggered_alerts'),
    'array<int>'
) AS alert_ids

‚úÖ New (STRUCT)

da.alerts.triggered_alerts AS alert_ids


---

2Ô∏è‚É£ Building manager alerts (STRUCT, not JSON)

‚ùå Old (string concat / to_json)

to_json(
    named_struct(
        'team_alerts', ...
    )
) AS alerts_json

‚úÖ New (STRUCT)

named_struct(
    'team_alerts',
    collect_list(
        named_struct(
            'alert', alert_id,
            'severity', severity,
            'affected_employees', affected_count,
            'team_size', team_size,
            'percentage', ROUND(percentage, 1)
        )
    ),
    'summary',
    named_struct(
        'total_alerts', COUNT(*),
        'calculated_at', current_timestamp()
    )
) AS alerts


---

‚úÖ Final corrected core SQL (STRUCT ‚Üí STRUCT)

CREATE OR REPLACE TEMP VIEW day_mgr_alerts_calc AS
WITH employee_alerts AS (
    SELECT
        da.emp_id,
        da.shift_date,
        de.manager_id,
        da.alerts.triggered_alerts AS alert_ids
    FROM day_alerts da
    JOIN day_emp de
      ON da.emp_id = de.emp_id
     AND da.shift_date = de.shift_date
    WHERE da.updated_on > current_timestamp() - INTERVAL 4 HOURS
      AND de.manager_id IS NOT NULL
),

exploded_alerts AS (
    SELECT
        manager_id,
        shift_date,
        emp_id,
        EXPLODE(alert_ids) AS alert_id
    FROM employee_alerts
    WHERE alert_ids IS NOT NULL
      AND SIZE(alert_ids) > 0
),

team_sizes AS (
    SELECT
        manager_id,
        shift_date,
        COUNT(DISTINCT emp_id) AS team_size
    FROM day_emp
    WHERE manager_id IS NOT NULL
    GROUP BY manager_id, shift_date
),

alert_counts AS (
    SELECT
        ea.manager_id,
        ea.shift_date,
        ea.alert_id,
        COUNT(DISTINCT ea.emp_id) AS affected_count,
        ts.team_size
    FROM exploded_alerts ea
    JOIN team_sizes ts
      ON ea.manager_id = ts.manager_id
     AND ea.shift_date = ts.shift_date
    GROUP BY
        ea.manager_id,
        ea.shift_date,
        ea.alert_id,
        ts.team_size
),

alert_metrics AS (
    SELECT
        manager_id,
        shift_date,
        alert_id,
        affected_count,
        team_size,
        (affected_count * 100.0 / team_size) AS percentage,
        CASE
            WHEN (affected_count * 100.0 / team_size) >= 60 THEN 'HIGH'
            WHEN (affected_count * 100.0 / team_size) >= 40 THEN 'MEDIUM'
            WHEN (affected_count * 100.0 / team_size) >= 20 THEN 'LOW'
            ELSE NULL
        END AS severity
    FROM alert_counts
),

manager_alerts AS (
    SELECT
        manager_id,
        shift_date,
        MAX(team_size) AS team_size,
        named_struct(
            'team_alerts',
            collect_list(
                named_struct(
                    'alert', alert_id,
                    'severity', severity,
                    'affected_employees', affected_count,
                    'team_size', team_size,
                    'percentage', ROUND(percentage, 1)
                )
            ),
            'summary',
            named_struct(
                'total_alerts', COUNT(*),
                'calculated_at', current_timestamp()
            )
        ) AS alerts,
        current_timestamp() AS calculated_at
    FROM alert_metrics
    WHERE severity IS NOT NULL
    GROUP BY manager_id, shift_date
)

SELECT * FROM manager_alerts;


---

‚úÖ MERGE command (STRUCT column)

MERGE INTO day_mgr_alerts target
USING day_mgr_alerts_calc source
ON target.manager_id = source.manager_id
AND target.shift_date = source.shift_date
WHEN MATCHED THEN
  UPDATE SET
    target.alerts = source.alerts,
    target.team_size = source.team_size,
    target.updated_on = source.calculated_at
WHEN NOT MATCHED THEN
  INSERT (
    manager_id,
    shift_date,
    team_size,
    alerts,
    updated_on
  )
  VALUES (
    source.manager_id,
    source.shift_date,
    source.team_size,
    source.alerts,
    source.calculated_at
  );


---

üèÅ Final takeaway

‚úî STRUCT ‚Üí access with dot notation
‚úî STRUCT ‚Üí build with named_struct
‚úî NO to_json, from_json, get_json_object
‚úî Cleaner, faster, safer

If you want, next I can:

Show how to query STRUCT alerts

Convert existing STRING JSON ‚Üí STRUCT

Add Z-ORDER optimizations

Validate schema mismatches


Just tell me üëç







from pyspark.sql.functions import udf
from pyspark.sql.types import StringType, ArrayType, IntegerType
import json
from datetime import datetime

# UDF for Employee Day Alert Calculation
def calculate_employee_day_alerts(active_time, idle_time, lock_time, 
                                 expected_shift_time, expected_break_time):
    """
    Calculate day alerts for an employee and return JSON string
    """
    # Calculate active percentage
    total_time = active_time + idle_time + lock_time
    active_percent = (active_time * 100.0 / total_time) if total_time > 0 else 0
    
    triggered_alerts = []
    
    # Alert 1: Few Breaks
    if (idle_time + lock_time) < expected_break_time:
        triggered_alerts.append(1)
    
    # Alert 2: Low Active Time
    if active_time < expected_shift_time:
        triggered_alerts.append(2)
    
    # Alert 3: High Active Time
    if active_time > expected_shift_time:
        triggered_alerts.append(3)
    
    # Alert 4: Low Active & High Idle Time
    if active_time < (0.9 * expected_shift_time) and active_percent < 90:
        triggered_alerts.append(4)
    
    # Alert 5: Low Active & Low Idle Time
    if active_time < (0.9 * expected_shift_time) and active_percent > 90:
        triggered_alerts.append(5)
    
    # Alert 6: High Active & High Idle Time
    if active_time > (1.1 * expected_shift_time) and idle_time > 1.0:
        triggered_alerts.append(6)
    
    # Alert 7: Few Breaks + Low Active & High Idle Time
    if ((idle_time + lock_time) < expected_break_time and 
        active_time < (0.9 * expected_shift_time) and 
        active_percent < 90):
        triggered_alerts.append(7)
    
    # Alert 8: Few Breaks + High Active & High Idle Time
    if ((idle_time + lock_time) < expected_break_time and 
        active_time > (1.1 * expected_shift_time) and 
        idle_time > 1.0):
        triggered_alerts.append(8)
    
    # Alert 9: Few Breaks + Low Active & Low Idle Time
    if ((idle_time + lock_time) < expected_break_time and 
        active_time < (0.9 * expected_shift_time) and 
        active_percent > 90):
        triggered_alerts.append(9)
    
    # Alert 10: Low Active + High Idle Time
    if (active_time < expected_shift_time and 
        active_time < (0.9 * expected_shift_time) and 
        active_percent < 90):
        triggered_alerts.append(10)
    
    # Alert 11: Low Active + Low Idle Time
    if (active_time < expected_shift_time and 
        active_time < (0.9 * expected_shift_time) and 
        active_percent > 90):
        triggered_alerts.append(11)
    
    # Alert 12: High Active + High Idle Time
    if (active_time > (1.1 * expected_shift_time) and 
        idle_time > 1.0 and 
        active_time > expected_shift_time):
        triggered_alerts.append(12)
    
    # Alert 13: Low Active + Few Breaks
    if ((idle_time + lock_time) < expected_break_time and 
        active_time < expected_shift_time):
        triggered_alerts.append(13)
    
    # Alert 14: High Active + Few Breaks
    if ((idle_time + lock_time) < expected_break_time and 
        active_time > expected_shift_time):
        triggered_alerts.append(14)
    
    # Create JSON structure exactly as you specified
    alerts_json = {
        "triggered_alerts": triggered_alerts,
        "summary": {
            "total_alerts": len(triggered_alerts),
            "calculated_at": datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ")
        }
    }
    
    return json.dumps(alerts_json)

# Register the UDF
employee_day_alerts_udf = udf(calculate_employee_day_alerts, StringType())

# Alternative: UDF that returns just alert IDs array
def get_employee_day_alert_ids(active_time, idle_time, lock_time, 
                              expected_shift_time, expected_break_time):
    """
    Return just the alert IDs array without JSON
    """
    # Calculate active percentage
    total_time = active_time + idle_time + lock_time
    active_percent = (active_time * 100.0 / total_time) if total_time > 0 else 0
    
    triggered_alerts = []
    
    # Alert 1: Few Breaks
    if (idle_time + lock_time) < expected_break_time:
        triggered_alerts.append(1)
    
    # Alert 2: Low Active Time
    if active_time < expected_shift_time:
        triggered_alerts.append(2)
    
    # Alert 3: High Active Time
    if active_time > expected_shift_time:
        triggered_alerts.append(3)
    
    # Alert 4: Low Active & High Idle Time
    if active_time < (0.9 * expected_shift_time) and active_percent < 90:
        triggered_alerts.append(4)
    
    # Alert 5: Low Active & Low Idle Time
    if active_time < (0.9 * expected_shift_time) and active_percent > 90:
        triggered_alerts.append(5)
    
    # Alert 6: High Active & High Idle Time
    if active_time > (1.1 * expected_shift_time) and idle_time > 1.0:
        triggered_alerts.append(6)
    
    # Alert 7: Few Breaks + Low Active & High Idle Time
    if ((idle_time + lock_time) < expected_break_time and 
        active_time < (0.9 * expected_shift_time) and 
        active_percent < 90):
        triggered_alerts.append(7)
    
    # Alert 8: Few Breaks + High Active & High Idle Time
    if ((idle_time + lock_time) < expected_break_time and 
        active_time > (1.1 * expected_shift_time) and 
        idle_time > 1.0):
        triggered_alerts.append(8)
    
    # Alert 9: Few Breaks + Low Active & Low Idle Time
    if ((idle_time + lock_time) < expected_break_time and 
        active_time < (0.9 * expected_shift_time) and 
        active_percent > 90):
        triggered_alerts.append(9)
    
    # Alert 10: Low Active + High Idle Time
    if (active_time < expected_shift_time and 
        active_time < (0.9 * expected_shift_time) and 
        active_percent < 90):
        triggered_alerts.append(10)
    
    # Alert 11: Low Active + Low Idle Time
    if (active_time < expected_shift_time and 
        active_time < (0.9 * expected_shift_time) and 
        active_percent > 90):
        triggered_alerts.append(11)
    
    # Alert 12: High Active + High Idle Time
    if (active_time > (1.1 * expected_shift_time) and 
        idle_time > 1.0 and 
        active_time > expected_shift_time):
        triggered_alerts.append(12)
    
    # Alert 13: Low Active + Few Breaks
    if ((idle_time + lock_time) < expected_break_time and 
        active_time < expected_shift_time):
        triggered_alerts.append(13)
    
    # Alert 14: High Active + Few Breaks
    if ((idle_time + lock_time) < expected_break_time and 
        active_time > expected_shift_time):
        triggered_alerts.append(14)
    
    return triggered_alerts

alert_ids_udf = udf(get_employee_day_alert_ids, ArrayType(IntegerType()))


from pyspark.sql.functions import udf
from pyspark.sql.types import StringType, MapType, StringType, IntegerType, FloatType
import json
from datetime import datetime

def calculate_manager_day_alerts(alert_ids_list, team_size):
    """
    Calculate manager day alerts from employee alert IDs
    alert_ids_list: list of alert ID arrays for all team members
    team_size: total number of team members
    """
    if not alert_ids_list or team_size == 0:
        return json.dumps({"team_alerts": [], "summary": {"total_alerts": 0, "calculated_at": datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ")}})
    
    # Count occurrences of each alert across team
    alert_counts = {}
    for alert_ids in alert_ids_list:
        if alert_ids:
            for alert_id in alert_ids:
                alert_counts[alert_id] = alert_counts.get(alert_id, 0) + 1
    
    team_alerts = []
    
    for alert_id, affected_count in alert_counts.items():
        percentage = (affected_count * 100.0) / team_size
        
        # Determine severity based on percentage
        if percentage >= 60:
            severity = "HIGH"
        elif percentage >= 40:
            severity = "MEDIUM"
        elif percentage >= 20:
            severity = "LOW"
        else:
            continue  # Skip alerts affecting <20% of team
        
        team_alerts.append({
            "alert": int(alert_id),
            "severity": severity,
            "affected_employees": int(affected_count),
            "team_size": int(team_size),
            "percentage": round(float(percentage), 1)
        })
    
    # Sort by alert ID
    team_alerts.sort(key=lambda x: x["alert"])
    
    # Create JSON structure exactly as you specified
    manager_json = {
        "team_alerts": team_alerts,
        "summary": {
            "total_alerts": len(team_alerts),
            "calculated_at": datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ")
        }
    }
    
    return json.dumps(manager_json)

# Register the UDF
manager_day_alerts_udf = udf(calculate_manager_day_alerts, StringType())

# Alternative: Simplified version if you have aggregated data
def calculate_manager_simple(alert_id_counts, team_size):
    """
    Simplified version where alert_id_counts is a map of {alert_id: count}
    """
    team_alerts = []
    
    for alert_id_str, count in alert_id_counts.items():
        if count:
            alert_id = int(alert_id_str)
            affected_count = int(count)
            percentage = (affected_count * 100.0) / team_size
            
            if percentage >= 60:
                severity = "HIGH"
            elif percentage >= 40:
                severity = "MEDIUM"
            elif percentage >= 20:
                severity = "LOW"
            else:
                continue
            
            team_alerts.append({
                "alert": alert_id,
                "severity": severity,
                "affected_employees": affected_count,
                "team_size": int(team_size),
                "percentage": round(float(percentage), 1)
            })
    
    manager_json = {
        "team_alerts": team_alerts,
        "summary": {
            "total_alerts": len(team_alerts),
            "calculated_at": datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ")
        }
    }
    
    return json.dumps(manager_json)

manager_simple_udf = udf(calculate_manager_simple, StringType())


from pyspark.sql import SparkSession
from pyspark.sql.functions import col, collect_list, current_timestamp
from datetime import datetime, timedelta

def run_day_pipeline_with_udf():
    spark = SparkSession.builder.appName("AlertPipelineUDF").getOrCreate()
    
    # 1. Employee Day Alerts
    print("Processing Employee Day Alerts...")
    
    # Get data
    employee_data = spark.sql("""
        SELECT 
            emp_id,
            shift_date,
            manager_id,
            active_time,
            idle_time,
            lock_time,
            expected_shift_time,
            expected_break_time
        FROM day_emp
        WHERE shift_date = CURRENT_DATE() - INTERVAL 1 DAY
    """)
    
    # Apply employee UDF
    employee_alerts = employee_data.withColumn(
        "alerts_json",
        employee_day_alerts_udf(
            col("active_time"),
            col("idle_time"),
            col("lock_time"),
            col("expected_shift_time"),
            col("expected_break_time")
        )
    ).withColumn(
        "updated_on",
        current_timestamp()
    )
    
    # Save to day_alerts table
    employee_alerts.select("emp_id", "shift_date", "alerts_json", "updated_on") \
        .write.mode("overwrite") \
        .saveAsTable("day_alerts")
    
    print(f"Employee alerts processed: {employee_alerts.count()}")
    
    # 2. Manager Day Alerts
    print("\nProcessing Manager Day Alerts...")
    
    # Aggregate by manager
    manager_data = spark.sql("""
        WITH employee_alerts AS (
            SELECT 
                de.emp_id,
                de.manager_id,
                de.shift_date,
                FROM_JSON(da.alerts_json, 'STRUCT<triggered_alerts:ARRAY<INT>, summary:STRUCT<total_alerts:INT, calculated_at:STRING>>') as alerts
            FROM day_alerts da
            JOIN day_emp de ON da.emp_id = de.emp_id AND da.shift_date = de.shift_date
            WHERE da.shift_date = CURRENT_DATE() - INTERVAL 1 DAY
            AND de.manager_id IS NOT NULL
        ),
        team_sizes AS (
            SELECT 
                manager_id,
                COUNT(DISTINCT emp_id) as team_size
            FROM day_emp
            WHERE shift_date = CURRENT_DATE() - INTERVAL 1 DAY
            GROUP BY manager_id
        )
        SELECT 
            ea.manager_id,
            ea.shift_date,
            COLLECT_LIST(alerts.triggered_alerts) as alert_arrays,
            ts.team_size
        FROM employee_alerts ea
        JOIN team_sizes ts ON ea.manager_id = ts.manager_id
        GROUP BY ea.manager_id, ea.shift_date, ts.team_size
    """)
    
    # Apply manager UDF
    manager_alerts = manager_data.withColumn(
        "alerts_json",
        manager_day_alerts_udf(col("alert_arrays"), col("team_size"))
    ).withColumn(
        "updated_on",
        current_timestamp()
    )
    
    # Save to day_mgr_alerts table
    manager_alerts.select("manager_id", "shift_date", "team_size", "alerts_json", "updated_on") \
        .write.mode("overwrite") \
        .saveAsTable("day_mgr_alerts")
    
    print(f"Manager alerts processed: {manager_alerts.count()}")
    
    spark.stop()
    return {
        "employee_records": employee_alerts.count(),
        "manager_records": manager_alerts.count()
    }
