

Perfect — here’s the revised and polished version of your incident report with all your updates applied, in a professional and production-ready format:


---

Incident Summary

During the login process to the Pulse Dashboard, the application incorrectly referenced the gold_dashboard.analytics.LoginLogout_BKP table instead of the main gold_analytics.empLoginLogout table while fetching the PulseAgentCount.
This misconfiguration led to incorrect or missing data being fetched during user login in the production environment, resulting in login failures.


---

Impact Analysis

Duration: From initial deployment of the incorrect code until the hotfix was applied.

Affected Users: All users attempting to log in during the affected period.

System Behavior: The application attempted to fetch data from the backup table, which does not exist in the production environment, causing login failures.

Detection: The issue was identified through user reports and validation checks by the support team.



---

Business Impact

Users whose tokens had expired after 24 hours experienced login failures.

Temporary restriction in user workflow and operational tracking for reactivations of expired sessions.

Partial disruption in Pulse Dashboard accessibility and reporting during the incident window.



---

Technical Impact

The production codebase referenced the backup table (gold_dashboard.analytics.LoginLogout_BKP) instead of the main table (gold_analytics.empLoginLogout) due to a merge issue between DEV, UAT, and PROD branches.

The environment code versions were not aligned, leading to inconsistent database references.



---

Root Cause

Code inconsistency across environments (DEV, UAT, and PROD).

During a merge from DEV to PROD, an outdated configuration referencing the backup table was unintentionally deployed.

Lack of pre-deployment validation for database table references in configuration or code review.



---

Resolution Followed

1. Immediately restricted affected users to prevent further login failures.


2. Identified and confirmed the incorrect table reference in the production code.


3. Updated the configuration to point to the correct gold_analytics.empLoginLogout table.


4. Performed an emergency hotfix deployment to production.


5. Verified functionality with multiple production users, including managers and employees.


6. Monitored logs and user sessions post-fix to confirm successful resolution and stability.




---

Preventive Measures

Ensure environment code parity before deployments (DEV, UAT, PROD synchronization).

Implement a pre-deployment validation checklist for database table references and configuration values.

Introduce automated regression and configuration consistency checks.

Strengthen merge and code review processes to detect outdated references before release.

Schedule periodic environment alignment reviews between development, QA, and release teams.



---

Would you like me to make a shorter version (1–2 paragraphs) for including as a Jira comment, while keeping this as the full RCA document?






Root Cause Analysis / Incident Explanation:
During the Login-Logout process, the application was expected to fetch the PulseAgentCount from the empLoginLogout table. However, due to a branch version mismatch, the deployed code in PROD was referencing the PulseLoginLogout_BKP (backup) table instead of the main table.

This occurred because different environments (UAT, DEV, and PROD) had inconsistent versions of the codebase. While merging the DEV branch into PROD, the reference to the backup table was unintentionally included. As a result, the application retrieved incorrect data from the backup table.

Resolution:
Once the issue was identified, we immediately disabled affected users from accessing the Login module to prevent further data inconsistencies. The code was then hotfixed and redeployed to reference the correct empLoginLogout table.

Verification:
Post-deployment, validation was performed with production users, including managers and three employees, confirming that the Login-Logout functionality and PulseAgentCount values were working correctly.

Current Status:
✅ Issue resolved
✅ Functionality verified in PROD
✅ Code base aligned across environments




Here’s a clean and well-structured draft of your concept, written as a clear functional and technical explanation.


---

Feature: Manager-Based Dashboard Access Control

Objective

Restrict dashboard access for managers based on the number of employees actively running the agent in the past 30 days. This ensures that only managers with a minimum level of agent activity within their team can access the dashboard.


---

Data Source: LoginLogout Table

Column Name	Description

EmployeeID	Unique identifier of the employee running the agent
ManagerID	Unique identifier of the employee’s reporting manager
ShiftDate	Date on which the employee ran the agent
LoginTime	Timestamp of agent start/login
LogoutTime	Timestamp of agent stop/logout


> Each record represents an instance of an employee running the agent on a given day.




---

Logic Flow

1. Add Manager Reference

Include a ManagerID column in the LoginLogout table.

On each agent run (login event), determine the employee’s manager and insert that ManagerID along with the record.



2. Identify Active Employees

For each manager, identify all unique employees (EmployeeID) who have at least one LoginLogout entry in the past 30 days.



3. Calculate Agent Run Count

For each manager:

Count how many distinct employees under them have logged agent runs within the past 30 days.

Example:

SELECT 
    ManagerID, 
    COUNT(DISTINCT EmployeeID) AS ActiveEmployeeCount
FROM LoginLogout
WHERE ShiftDate >= CURDATE() - INTERVAL 30 DAY
GROUP BY ManagerID;




4. Apply Access Restriction

Define the minimum active employee threshold: 4 employees.

If a manager’s ActiveEmployeeCount < 4, block their access to the dashboard.

If the count is 4 or more, grant access as usual.



5. Notify Restricted Managers

When a manager with insufficient active employees attempts to access the dashboard:

Display a message such as:

> “Dashboard access is temporarily restricted. Fewer than 4 employees in your team have been active in the past 30 days.”








---

Example Scenario

ManagerID	Employee Count (Last 30 Days)	Access

MGR101	8	✅ Granted
MGR102	3	❌ Blocked
MGR103	5	✅ Granted



---

Implementation Notes

This logic can be implemented:

In backend (Python/Flask) as part of the dashboard API route — check access before returning data.

In frontend (Vue.js) — handle the access flag in API response and display a notification popup if blocked.


The 30-day check should be rolling, always calculated from the current system date.

To optimize performance, consider caching results daily since the data changes minimally.



---

Would you like me to include a sample Flask backend implementation (SQLAlchemy + Flask route) that performs this check and returns an access flag to the dashboard frontend?




Perfect — that adds an important layer of client-side state handling. Here’s a revised and properly structured draft that incorporates your cookie-based logic for maintaining and validating counts.


---

Feature: Dashboard Access Control Based on Active Employee Count (Cookie-Based Logic)

Objective

Restrict dashboard access for managers based on the number of employees actively running the agent within the past 30 days.
Access is dynamically checked and maintained using cookies to reduce database load and ensure smooth user experience.


---

Data Source: LoginLogout Table

Column Name	Description

EmployeeID	Unique identifier of the employee running the agent
ManagerID	Manager to whom the employee reports
ShiftDate	Date when the employee ran the agent
LoginTime	Timestamp when the agent was started
LogoutTime	Timestamp when the agent was stopped


Each entry indicates that the agent was active for that employee on the specified date.


---

System Flow

1. Daily Cookie Validation

On each day when a manager logs in or accesses the dashboard:

The system checks for two cookies:

HRCount → total number of employees mapped under the manager.

PulseAgentCount → number of employees who ran the agent in the last 30 days.


If the cookies do not exist or are older than 24 hours, they are refreshed.




---

2. Cookie Refresh Logic

When a cookie refresh is triggered:

1. Backend query runs to compute the agent count for the manager:

SELECT 
    ManagerID, 
    COUNT(DISTINCT EmployeeID) AS ActiveEmployeeCount
FROM LoginLogout
WHERE ShiftDate >= CURDATE() - INTERVAL 30 DAY
GROUP BY ManagerID;


2. The backend returns:

HRCount: total employees under the manager (from HR or mapping table)

PulseAgentCount: number of employees who ran the agent in the last 30 days



3. The frontend updates cookies:

document.cookie = `HRCount=${hrCount}; max-age=86400`; // 1 day
document.cookie = `PulseAgentCount=${activeCount}; max-age=86400`;





---

3. Access Decision

On dashboard load or login click:

The system compares the stored cookie values:

if (parseInt(getCookie('PulseAgentCount')) < 4) {
    showAccessBlockedMessage();
    redirectToLoginPage();
} else {
    grantDashboardAccess();
}


If the PulseAgentCount is less than 4, access is blocked, and the manager is redirected to the login page with a notification.



---

4. Notification Message

When access is restricted:

> “Access Denied: Dashboard is unavailable because fewer than 4 employees in your team have been active (agent run) in the past 30 days.”




---

5. Example Flow

1. Manager logs in → system checks cookies


2. Cookies missing or outdated → backend query triggered, cookies updated


3. Counts evaluated:

HRCount = 10

PulseAgentCount = 3



4. Access Result: Blocked


5. Next day, if new employees run the agent → count updates, access unlocked.




---

6. Benefits of Cookie-Based Approach

✅ Reduces frequent backend calls
✅ Provides daily snapshot for consistent behavior
✅ Keeps logic lightweight on the client side
✅ Enables smooth redirection and notification handling


---

Would you like me to extend this draft to show a sample implementation — for example,

a Flask route (/check-access) that returns HR and agent counts, and

a corresponding Vue.js or JavaScript snippet that updates cookies and checks access?





Hi [Team/Name],

Currently, our backend web application connects to the SQL Warehouse to execute all dashboard queries. These queries involve operations like joins and aggregations, and the processed data is stored in Delta Lake tables.

However, since all queries are executed through the SQL Warehouse, the response time is sometimes delayed — especially when the clusters are fully utilized or take time to scale up.

I wanted to check if there’s a way to directly access and query the Delta tables (bypassing SQL Warehouse) to achieve faster, more transactional-style query execution — similar to how traditional RDBMS systems operate.

In short:

Can we query the Delta tables directly without going through SQL Warehouse?

If yes, what would be the recommended approach or architecture to support such access efficiently and securely?


Appreciate your inputs on this.

Best regards,
[Your Name]



I wanted to share some context regarding the current issue and the upcoming change request scheduled for Monday or Tuesday.

At present, the login-logout recalculation is based on a delete and insert process. This approach leads to inaccurate data being returned if queries are made while the job is still running. The current process recalculates login-logout data for employees whose records fall within the past 72 hours, regardless of whether their actual login or logout times have changed. This is especially problematic with shift changes.

In the upcoming CR, this will be improved. The login-logout recalculation will occur only when there is an actual change in the employee’s login or logout time. Additionally, the process will be handled using an upsert approach instead of delete and insert. This means new data will be appended or updated without removing existing records, ensuring data consistency and accuracy.

With these changes, the issue of querying data during job execution will no longer occur.

Let me know if you have any questions.

import os
import logging
from dotenv import load_dotenv
import urllib.parse
import threading
from databricks import sql
from databricks.sdk.core import Config, oauth_service_principal
from queue import Queue
import time
from contextlib import contextmanager
from typing import Dict, Any
import json
from fastapi.responses import JSONResponse

# Load environment variables from .env file
load_dotenv()

# Configure Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Connection Pool Configuration
POOL_SIZE = 120  # Max number of connections in the pool
CONNECTION_TTL = 1800  # 30 minutes in seconds
connection_pool = Queue(maxsize=POOL_SIZE)
pool_lock = threading.Lock()

# Connection monitoring class
class ConnectionMonitor:
    def __init__(self):
        self.active_connections = 0
        self.total_connections_created = 0
        self.connections_reused = 0
        self.connections_closed = 0
        self.monitor_lock = threading.Lock()
    
    def connection_created(self):
        with self.monitor_lock:
            self.active_connections += 1
            self.total_connections_created += 1
            logger.info(f"New connection created. Active: {self.active_connections}, Total created: {self.total_connections_created}")
    
    def connection_reused(self):
        with self.monitor_lock:
            self.connections_reused += 1
            logger.info(f"Connection reused from pool. Reused count: {self.connections_reused}")
    
    def connection_returned(self):
        with self.monitor_lock:
            logger.info(f"Connection returned to pool. Pool size: {connection_pool.qsize()}")
    
    def connection_closed(self, reason: str):
        with self.monitor_lock:
            self.active_connections -= 1
            self.connections_closed += 1
            logger.info(f"Connection closed ({reason}). Active: {self.active_connections}, Closed: {self.connections_closed}")
    
    def get_stats(self) -> Dict[str, Any]:
        with self.monitor_lock:
            return {
                "active_connections": self.active_connections,
                "total_connections_created": self.total_connections_created,
                "connections_reused": self.connections_reused,
                "connections_closed": self.connections_closed,
                "pool_size": connection_pool.qsize(),
                "pool_capacity": POOL_SIZE
            }

# Initialize connection monitor
connection_monitor = ConnectionMonitor()

# Function to create a new Databricks connection with logging
def create_connection():
    try:
        server_hostname = os.getenv('DATABRICKS_SERVER_HOST')
        config = Config(
            host=f"https://{server_hostname}",
            client_id=os.getenv('DATABRICKS_CLIENT_ID'),
            client_secret=os.getenv('DATABRICKS_CLIENT_SECRET')
        )
        
        logger.info(f"Creating new connection to {server_hostname}")
        conn = sql.connect(
            server_hostname=server_hostname,
            http_path=os.getenv('DATABRICKS_HTTP_PATH'),
            credentials_provider=lambda: oauth_service_principal(config)
        )
        
        connection_monitor.connection_created()
        return conn
        
    except Exception as e:
        logger.error(f"Failed to create connection: {str(e)}")
        raise

# Context Manager for Connection Pooling with enhanced logging
class DatabricksSession:
    def __enter__(self):
        now = time.time()
        connection_obtained = False
        
        with pool_lock:
            # Try to get a connection from the pool
            while not connection_pool.empty() and not connection_obtained:
                conn, last_used = connection_pool.get()
                if now - last_used < CONNECTION_TTL:
                    self.conn = conn
                    connection_obtained = True
                    connection_monitor.connection_reused()
                    logger.info(f"Reused connection from pool. Time since last use: {now - last_used:.2f}s")
                else:
                    try:
                        conn.close()
                        connection_monitor.connection_closed("stale")
                        logger.info("Closed stale connection from pool")
                    except Exception as e:
                        logger.error(f"Error closing stale connection: {e}")
            
            # If no valid connection found in pool, create a new one
            if not connection_obtained:
                self.conn = create_connection()
                logger.info("Created new connection (pool empty or no valid connections)")
        
        logger.info(f"Connection obtained. Pool status: {connection_pool.qsize()}/{POOL_SIZE}")
        return self.conn

    def __exit__(self, exc_type, exc_val, exc_tb):
        try:
            # Check if connection is still valid before returning to pool
            if hasattr(self, 'conn') and self.conn:
                if exc_type is not None:
                    # If there was an exception, don't return to pool
                    logger.warning(f"Exception occurred in session, closing connection: {exc_val}")
                    self.conn.close()
                    connection_monitor.connection_closed("exception")
                else:
                    # Return connection to pool if it's not full
                    with pool_lock:
                        if connection_pool.full():
                            logger.warning("Connection pool full, closing connection")
                            self.conn.close()
                            connection_monitor.connection_closed("pool_full")
                        else:
                            connection_pool.put((self.conn, time.time()))
                            connection_monitor.connection_returned()
                            logger.debug(f"Connection returned to pool. New size: {connection_pool.qsize()}")
            else:
                logger.warning("No valid connection to return to pool")
                
        except Exception as e:
            logger.error(f"Error handling connection cleanup: {e}")
            try:
                if hasattr(self, 'conn') and self.conn:
                    self.conn.close()
                    connection_monitor.connection_closed("cleanup_error")
            except Exception as close_error:
                logger.error(f"Error closing connection during cleanup: {close_error}")

# Function to execute query with logging
def execute_query(conn, query: str):
    try:
        logger.info(f"Executing query: {query[:100]}...")  # Log first 100 chars
        start_time = time.time()
        
        with conn.cursor() as cursor:
            cursor.execute(query)
            result = cursor.fetchall()
        
        execution_time = time.time() - start_time
        logger.info(f"Query executed successfully in {execution_time:.2f}s. Rows returned: {len(result)}")
        return result
        
    except Exception as e:
        logger.error(f"Query execution failed: {str(e)}")
        raise

# Function to get connection pool statistics
def get_connection_stats() -> Dict[str, Any]:
    stats = connection_monitor.get_stats()
    logger.info(f"Connection pool statistics: {stats}")
    return stats

# Your existing usage pattern remains the same
def process_query(query: str):
    try:
        with DatabricksSession() as conn:
            data = execute_query(conn, query)
            
            if data and data[0]:
                json_string = data[0][0]
                json_data = json.loads(json_string)
            else:
                json_data = {"empSummary": {}, "teamSummary": {}, "empCount": None}
        
        # Log connection stats after operation
        stats = get_connection_stats()
        logger.info(f"Operation completed. Connection stats: {stats}")
        
        if not json_data:
            return JSONResponse(status_code=404, content={'message': "Execution Not Success"})
        return JSONResponse(status_code=200, content=json_data)
        
    except Exception as e:
        logger.error(f"Error in process_query: {str(e)}")
        return JSONResponse(status_code=500, content={'message': f"Internal Server Error: {str(e)}"})

# Optional: Add periodic logging of connection stats
def log_periodic_stats(interval: int = 300):  # 5 minutes
    """Periodically log connection pool statistics"""
    while True:
        time.sleep(interval)
        stats = get_connection_stats()
        logger.info(f"Periodic connection pool report: {stats}")

# Start periodic logging in a background thread (optional)
# stats_thread = threading.Thread(target=log_periodic_stats, daemon=True)
# stats_thread.start()







result_df = classified_df.groupBy("emp_id", "cal_date").agg(
    # Current login: Minimum start_time within current window and app_name != "Window Lock"
    F.min(F.when(
        (F.col("is_current_login")) & (F.col("app_name") != "WindowLock"),
        F.col("start_time")
    )).alias("current_login"),
    
    # Last activity: Maximum start_time within current window BUT exclude WindowLock
    F.max(F.when(
        (F.col("start_time") >= F.col("current_window_start")) &
        (F.col("start_time") <= F.col("current_window_end")) &
        (F.col("app_name") != "WindowLock"),  # EXCLUDE WindowLock
        F.col("start_time")
    )).alias("last_activity"),
    
    # ... keep other aggregations as they are
)

# For mouse/keyboard data, we already exclude WindowLock by nature
result_mkdf = classified_mkdf.groupBy("emp_id", "cal_date").agg(
    F.max(F.when(
        (F.col("event_time") >= F.col("current_window_start")) &
        (F.col("event_time") <= F.col("current_window_end")),
        F.col("event_time")
    )).alias("last_mk_activity"),
    # ... other aggregations
)




final_result = result_df.withColumn(
    "emp_login_time",
    F.when(
        F.col("is_week_off"),
        F.coalesce(F.col("week_off_login"), F.col("last_activity"))
    ).otherwise(
        F.coalesce(F.col("current_login"), F.col("last_activity"))
    )
).withColumn(
    "emp_logout_time",
    F.when(
        F.col("is_week_off"),
        F.coalesce(F.col("week_off_logout"), F.col("last_activity"))
    ).otherwise(
        # PREFER mouse/keyboard logout over app activities (which excludes WindowLock)
        F.coalesce(F.col("last_mk_activity"), F.col("last_activity"), F.col("current_login"))
    )
)




# Single combined merge with optimized logic
spark.sql("""
MERGE INTO gold_dashboard.analytics_emp_login_logout AS target
USING (
    -- Combined source data with operation type indicator
    SELECT ..., 1 as operation_type FROM temp_filtered_login_logout
    UNION ALL
    SELECT ..., 2 as operation_type FROM temp_prev_day_updates
) AS source
ON (
    -- Combined join conditions
    (operation_type = 1 AND target.EMP_ID = source.emp_id AND target.SHIFT_DATE = source.shift_date)
    OR
    (operation_type = 2 AND target.EMP_ID = source.update_emp_id AND target.SHIFT_DATE = source.update_date)
)
WHEN MATCHED AND source.operation_type = 1 AND (
    -- Only update if source values are better
    (unix_timestamp(source.emp_login_time) < unix_timestamp(target.EMP_LOGIN_TIME)) OR
    (unix_timestamp(source.emp_logout_time) > unix_timestamp(target.EMP_LOGOUT_TIME))
) THEN
    UPDATE SET 
        target.EMP_LOGIN_TIME = CASE 
            WHEN unix_timestamp(source.emp_login_time) < unix_timestamp(target.EMP_LOGIN_TIME)
            THEN source.emp_login_time 
            ELSE target.EMP_LOGIN_TIME 
        END,
        target.EMP_LOGOUT_TIME = CASE 
            WHEN unix_timestamp(source.emp_logout_time) > unix_timestamp(target.EMP_LOGOUT_TIME)
            THEN source.emp_logout_time 
            ELSE target.EMP_LOGOUT_TIME 
        END
WHEN MATCHED AND source.operation_type = 2 AND (
    -- Only update previous logout if source is later
    unix_timestamp(source.new_logout_time) > unix_timestamp(target.EMP_LOGOUT_TIME)
) THEN
    UPDATE SET target.EMP_LOGOUT_TIME = source.new_logout_time
WHEN NOT MATCHED AND source.operation_type = 1 THEN
    INSERT (EMP_ID, START_TIME, END_TIME, EMP_LOGIN_TIME, EMP_LOGOUT_TIME, SHIFT_DATE)
    VALUES (source.emp_id, source.start_time, source.end_time,
            source.emp_login_time, source.emp_logout_time, source.shift_date)
""")




WITH FilteredEmployees AS (
  SELECT
    emplid AS emp_id,
    COALESCE(country, 'Default') AS country
  FROM inbound.hr_employee_central
  WHERE func_mar_id = 43560069
    AND (TERMINATION_DT > CURRENT_TIMESTAMP() OR TERMINATION_DT IS NULL)
),

EmployeeShiftsWithConfig AS (
  SELECT
    l.emp_id,
    l.shift_date,
    TO_TIMESTAMP(l.start_time, 'yyyy-MM-dd HH:mm:ss') AS start_time,
    TO_TIMESTAMP(l.end_time, 'yyyy-MM-dd HH:mm:ss') AS end_time,
    TO_TIMESTAMP(l.emp_login_time, 'yyyy-MM-dd HH:mm:ss') AS emp_login_time,
    TO_TIMESTAMP(l.emp_logout_time, 'yyyy-MM-dd HH:mm:ss') AS emp_logout_time,
    (UNIX_TIMESTAMP(l.end_time) - UNIX_TIMESTAMP(l.start_time)) / 3600.0 AS pulse_shift_time,
    (COALESCE(o.shift_dur_hrs, 8) * (UNIX_TIMESTAMP(l.end_time) - UNIX_TIMESTAMP(l.start_time)) / 3600.0 / COALESCE(o.shift_length_hrs, 9)) AS adjusted_active_time,
    (COALESCE(o.break_dur_hrs, 1) * (UNIX_TIMESTAMP(l.end_time) - UNIX_TIMESTAMP(l.start_time)) / 3600.0 / COALESCE(o.shift_length_hrs, 9)) AS adjusted_lock_time
  FROM gold_dashboard.analytics_emp_login_logout l
  JOIN FilteredEmployees e ON l.emp_id = e.emp_id
  LEFT JOIN inbound.countrywise_shift o ON e.country = o.country
  WHERE (l.shift_date BETWEEN '{start_date}' AND '{end_date}' OR l.shift_date BETWEEN '{prev_start_date}' AND '{prev_end_date}')
    AND l.is_weekoff = false
    AND l.emp_login_time IS NOT NULL
    AND l.emp_logout_time IS NOT NULL
),

FilteredAppInfo AS (
  SELECT
    i.emp_id,
    i.cal_date AS shift_date,
    TO_TIMESTAMP(CONCAT(i.cal_date, ' ', SUBSTRING(i.interval, 1, 5), ':00'), 'yyyy-MM-dd HH:mm:ss') AS interval_start,
    TO_TIMESTAMP(CONCAT(i.cal_date, ' ', SUBSTRING(i.interval, 7, 5), ':00'), 'yyyy-MM-dd HH:mm:ss') AS interval_end,
    i.total_time_spent_active AS total_time_active,
    i.total_time_spent_idle AS total_time_idle,
    i.window_lock_time
  FROM gold_dashboard.analytics_emp_app_info i
  JOIN FilteredEmployees e ON i.emp_id = e.emp_id
  WHERE (i.cal_date BETWEEN '{start_date}' AND '{end_date}' OR i.cal_date BETWEEN '{prev_start_date}' AND '{prev_end_date}')
),

ActivityWithShiftBounds AS (
  SELECT
    i.emp_id,
    i.shift_date,
    s.start_time,
    s.end_time,
    i.interval_start,
    i.interval_end,
    s.emp_login_time,
    s.emp_logout_time,
    i.total_time_active,
    i.total_time_idle,
    i.window_lock_time,
    GREATEST(i.interval_start, s.start_time) AS overlap_start,
    LEAST(i.interval_end, s.end_time) AS overlap_end,
    GREATEST(i.interval_start, s.emp_login_time) AS activity_start,
    LEAST(i.interval_end, s.emp_logout_time) AS activity_end
  FROM FilteredAppInfo i
  JOIN EmployeeShiftsWithConfig s ON i.emp_id = s.emp_id AND i.shift_date = s.shift_date
  WHERE s.emp_login_time < i.interval_end
    AND s.emp_logout_time > i.interval_start
),

ActivityTimeAllocation AS (
  SELECT
    emp_id,
    shift_date,
    total_time_active,
    total_time_idle,
    window_lock_time,
    start_time,
    end_time,
    
    -- Calculate seconds within shift
    CASE 
      WHEN activity_start >= end_time OR activity_end <= start_time THEN 0
      WHEN activity_start >= start_time AND activity_end <= end_time THEN
        UNIX_TIMESTAMP(activity_end) - UNIX_TIMESTAMP(activity_start)
      ELSE
        UNIX_TIMESTAMP(LEAST(activity_end, end_time)) - UNIX_TIMESTAMP(GREATEST(activity_start, start_time))
    END AS seconds_within_shift,
    
    -- Calculate seconds outside shift
    CASE 
      WHEN activity_start >= end_time OR activity_end <= start_time THEN
        UNIX_TIMESTAMP(activity_end) - UNIX_TIMESTAMP(activity_start)
      WHEN activity_start >= start_time AND activity_end <= end_time THEN 0
      ELSE
        (UNIX_TIMESTAMP(activity_end) - UNIX_TIMESTAMP(activity_start)) - 
        (UNIX_TIMESTAMP(LEAST(activity_end, end_time)) - UNIX_TIMESTAMP(GREATEST(activity_start, start_time)))
    END AS seconds_outside_shift
  FROM ActivityWithShiftBounds
  WHERE activity_start < activity_end
),

CurrentShiftData AS (
  SELECT
    a.emp_id,
    a.shift_date,
    
    -- Total metrics
    SUM(total_time_active) AS total_active_time,
    SUM(total_time_idle) AS total_idle_time,
    SUM(window_lock_time) AS total_lock_time,
    SUM(total_time_active + total_time_idle + window_lock_time) AS total_work_time,
    
    -- Within shift allocation
    SUM(
      total_time_active * (seconds_within_shift / NULLIF((seconds_within_shift + seconds_outside_shift), 0))
    ) AS active_within_shift,
    
    SUM(
      total_time_idle * (seconds_within_shift / NULLIF((seconds_within_shift + seconds_outside_shift), 0))
    ) AS idle_within_shift,
    
    SUM(
      window_lock_time * (seconds_within_shift / NULLIF((seconds_within_shift + seconds_outside_shift), 0))
    ) AS lock_within_shift,
    
    -- Outside shift allocation
    SUM(
      total_time_active * (seconds_outside_shift / NULLIF((seconds_within_shift + seconds_outside_shift), 0))
    ) AS active_outside_shift,
    
    SUM(
      total_time_idle * (seconds_outside_shift / NULLIF((seconds_within_shift + seconds_outside_shift), 0))
    ) AS idle_outside_shift,
    
    -- Shift configuration
    MAX(s.pulse_shift_time) AS pulse_shift_time,
    MAX(s.adjusted_active_time) AS adjusted_active_time,
    MAX(s.adjusted_lock_time) AS adjusted_lock_time,
    
    -- Period indicator
    CASE 
      WHEN a.shift_date BETWEEN '{start_date}' AND '{end_date}' THEN 'current'
      WHEN a.shift_date BETWEEN '{prev_start_date}' AND '{prev_end_date}' THEN 'previous'
    END AS period
    
  FROM ActivityTimeAllocation a
  JOIN EmployeeShiftsWithConfig s ON a.emp_id = s.emp_id AND a.shift_date = s.shift_date
  GROUP BY a.emp_id, a.shift_date, 
    CASE 
      WHEN a.shift_date BETWEEN '{start_date}' AND '{end_date}' THEN 'current'
      WHEN a.shift_date BETWEEN '{prev_start_date}' AND '{prev_end_date}' THEN 'previous'
    END
),

PeriodAggregates AS (
  SELECT
    period,
    AVG(total_work_time) AS avg_total_work_time,
    AVG(total_active_time) AS avg_total_active_time,
    AVG(total_idle_time) AS avg_total_idle_time,
    AVG(total_lock_time) AS avg_total_lock_time,
    AVG(active_within_shift + idle_within_shift + lock_within_shift) AS avg_total_work_time_within_shift,
    AVG(active_outside_shift + idle_outside_shift) AS avg_total_work_time_outside_shift,
    AVG(pulse_shift_time * 3600) AS avg_pulse_shift_time_seconds,
    AVG(adjusted_active_time * 3600) AS avg_adjusted_active_time_seconds,
    AVG(adjusted_lock_time * 3600) AS avg_adjusted_lock_time_seconds
  FROM CurrentShiftData
  WHERE period IN ('current', 'previous')
  GROUP BY period
),

CurrentPeriod AS (
  SELECT * FROM PeriodAggregates WHERE period = 'current'
),

PreviousPeriod AS (
  SELECT * FROM PeriodAggregates WHERE period = 'previous'
)

SELECT
  -- Current period metrics (use 0 if no data exists)
  COALESCE(curr.avg_total_work_time, 0) AS current_total_work_time,
  COALESCE(curr.avg_total_active_time, 0) AS current_total_active_time,
  COALESCE(curr.avg_total_idle_time, 0) AS current_total_idle_time,
  COALESCE(curr.avg_total_lock_time, 0) AS current_total_lock_time,
  COALESCE(curr.avg_total_work_time_within_shift, 0) AS current_total_work_time_within_shift,
  COALESCE(curr.avg_total_work_time_outside_shift, 0) AS current_total_work_time_outside_shift,
  
  -- Schedule comparisons (only calculate if current data exists)
  CASE 
    WHEN curr.avg_total_work_time IS NOT NULL 
    THEN COALESCE(curr.avg_total_work_time, 0) - COALESCE(curr.avg_pulse_shift_time_seconds, 0)
    ELSE 0 
  END AS total_work_shift_schedule,
  
  CASE 
    WHEN curr.avg_total_work_time IS NULL THEN 'No Data'
    WHEN COALESCE(curr.avg_total_work_time, 0) - COALESCE(curr.avg_pulse_shift_time_seconds, 0) > 0 THEN 'Up'
    WHEN COALESCE(curr.avg_total_work_time, 0) - COALESCE(curr.avg_pulse_shift_time_seconds, 0) < 0 THEN 'Down'
    ELSE 'No Change'
  END AS total_work_shift_schedule_trend,
  
  CASE 
    WHEN curr.avg_total_active_time IS NOT NULL 
    THEN COALESCE(curr.avg_total_active_time, 0) - COALESCE(curr.avg_adjusted_active_time_seconds, 0)
    ELSE 0 
  END AS total_active_shift_schedule,
  
  CASE 
    WHEN curr.avg_total_active_time IS NULL THEN 'No Data'
    WHEN COALESCE(curr.avg_total_active_time, 0) - COALESCE(curr.avg_adjusted_active_time_seconds, 0) > 0 THEN 'Up'
    WHEN COALESCE(curr.avg_total_active_time, 0) - COALESCE(curr.avg_adjusted_active_time_seconds, 0) < 0 THEN 'Down'
    ELSE 'No Change'
  END AS total_active_shift_schedule_trend,
  
  CASE 
    WHEN curr.avg_total_lock_time IS NOT NULL 
    THEN COALESCE(curr.avg_total_lock_time, 0) - COALESCE(curr.avg_adjusted_lock_time_seconds, 0)
    ELSE 0 
  END AS total_lock_shift_schedule,
  
  CASE 
    WHEN curr.avg_total_lock_time IS NULL THEN 'No Data'
    WHEN COALESCE(curr.avg_total_lock_time, 0) - COALESCE(curr.avg_adjusted_lock_time_seconds, 0) > 0 THEN 'Up'
    WHEN COALESCE(curr.avg_total_lock_time, 0) - COALESCE(curr.avg_adjusted_lock_time_seconds, 0) < 0 THEN 'Down'
    ELSE 'No Change'
  END AS total_lock_shift_schedule_trend,
  
  -- Previous period metrics (use 0 if no data exists)
  COALESCE(prev.avg_total_work_time, 0) AS prev_total_work_time,
  COALESCE(prev.avg_total_active_time, 0) AS prev_total_active_time,
  COALESCE(prev.avg_total_idle_time, 0) AS prev_total_idle_time,
  COALESCE(prev.avg_total_lock_time, 0) AS prev_total_lock_time,
  
  -- Period comparison trends (handle missing data scenarios)
  CASE 
    WHEN curr.avg_total_work_time IS NULL OR prev.avg_total_work_time IS NULL THEN 'No Comparison Data'
    WHEN curr.avg_total_work_time > prev.avg_total_work_time THEN 'Up'
    WHEN curr.avg_total_work_time < prev.avg_total_work_time THEN 'Down'
    ELSE 'No Change'
  END AS total_work_trend,
  
  CASE 
    WHEN curr.avg_total_work_time IS NULL OR prev.avg_total_work_time IS NULL OR prev.avg_total_work_time = 0 THEN 0
    ELSE ROUND(((curr.avg_total_work_time - prev.avg_total_work_time) / prev.avg_total_work_time) * 100, 2)
  END AS total_work_percentage_change,
  
  CASE 
    WHEN curr.avg_total_active_time IS NULL OR prev.avg_total_active_time IS NULL THEN 'No Comparison Data'
    WHEN curr.avg_total_active_time > prev.avg_total_active_time THEN 'Up'
    WHEN curr.avg_total_active_time < prev.avg_total_active_time THEN 'Down'
    ELSE 'No Change'
  END AS total_active_trend,
  
  CASE 
    WHEN curr.avg_total_active_time IS NULL OR prev.avg_total_active_time IS NULL OR prev.avg_total_active_time = 0 THEN 0
    ELSE ROUND(((curr.avg_total_active_time - prev.avg_total_active_time) / prev.avg_total_active_time) * 100, 2)
  END AS total_active_percentage_change,
  
  CASE 
    WHEN curr.avg_total_idle_time IS NULL OR prev.avg_total_idle_time IS NULL THEN 'No Comparison Data'
    WHEN curr.avg_total_idle_time > prev.avg_total_idle_time THEN 'Up'
    WHEN curr.avg_total_idle_time < prev.avg_total_idle_time THEN 'Down'
    ELSE 'No Change'
  END AS total_idle_trend,
  
  CASE 
    WHEN curr.avg_total_idle_time IS NULL OR prev.avg_total_idle_time IS NULL OR prev.avg_total_idle_time = 0 THEN 0
    ELSE ROUND(((curr.avg_total_idle_time - prev.avg_total_idle_time) / prev.avg_total_idle_time) * 100, 2)
  END AS total_idle_percentage_change,
  
  CASE 
    WHEN curr.avg_total_lock_time IS NULL OR prev.avg_total_lock_time IS NULL THEN 'No Comparison Data'
    WHEN curr.avg_total_lock_time > prev.avg_total_lock_time THEN 'Up'
    WHEN curr.avg_total_lock_time < prev.avg_total_lock_time THEN 'Down'
    ELSE 'No Change'
  END AS total_lock_trend,
  
  CASE 
    WHEN curr.avg_total_lock_time IS NULL OR prev.avg_total_lock_time IS NULL OR prev.avg_total_lock_time = 0 THEN 0
    ELSE ROUND(((curr.avg_total_lock_time - prev.avg_total_lock_time) / prev.avg_total_lock_time) * 100, 2)
  END AS total_lock_percentage_change
  
FROM 
  (SELECT 1 AS join_key) AS dummy
LEFT JOIN CurrentPeriod curr ON 1=1
LEFT JOIN PreviousPeriod prev ON 1=1






WITH FilteredEmployees AS (
  SELECT
    emplid AS emp_id,
    COALESCE(country, 'Default') AS country
  FROM inbound.hr_employee_central
  WHERE func_mar_id = 43560069
    AND (TERMINATION_DT > CURRENT_TIMESTAMP() OR TERMINATION_DT IS NULL)
),

EmployeeShiftsWithConfig AS (
  SELECT
    l.emp_id,
    l.shift_date,
    TO_TIMESTAMP(l.start_time, 'yyyy-MM-dd HH:mm:ss') AS start_time,
    TO_TIMESTAMP(l.end_time, 'yyyy-MM-dd HH:mm:ss') AS end_time,
    TO_TIMESTAMP(l.emp_login_time, 'yyyy-MM-dd HH:mm:ss') AS emp_login_time,
    TO_TIMESTAMP(l.emp_logout_time, 'yyyy-MM-dd HH:mm:ss') AS emp_logout_time,
    (UNIX_TIMESTAMP(l.end_time) - UNIX_TIMESTAMP(l.start_time)) / 3600.0 AS pulse_shift_time,
    (COALESCE(o.shift_dur_hrs, 8) * (UNIX_TIMESTAMP(l.end_time) - UNIX_TIMESTAMP(l.start_time)) / 3600.0 / COALESCE(o.shift_length_hrs, 9)) AS adjusted_active_time,
    (COALESCE(o.break_dur_hrs, 1) * (UNIX_TIMESTAMP(l.end_time) - UNIX_TIMESTAMP(l.start_time)) / 3600.0 / COALESCE(o.shift_length_hrs, 9)) AS adjusted_lock_time
  FROM gold_dashboard.analytics_emp_login_logout l
  JOIN FilteredEmployees e ON l.emp_id = e.emp_id
  LEFT JOIN inbound.countrywise_shift o ON e.country = o.country
  WHERE (l.shift_date BETWEEN '{start_date}' AND '{end_date}' OR l.shift_date BETWEEN '{prev_start_date}' AND '{prev_end_date}')
    AND l.is_weekoff = false
    AND l.emp_login_time IS NOT NULL
    AND l.emp_logout_time IS NOT NULL
),

FilteredAppInfo AS (
  SELECT
    i.emp_id,
    i.cal_date AS shift_date,
    TO_TIMESTAMP(CONCAT(i.cal_date, ' ', SUBSTRING(i.interval, 1, 5), ':00'), 'yyyy-MM-dd HH:mm:ss') AS interval_start,
    TO_TIMESTAMP(CONCAT(i.cal_date, ' ', SUBSTRING(i.interval, 7, 5), ':00'), 'yyyy-MM-dd HH:mm:ss') AS interval_end,
    i.total_time_spent_active AS total_time_active,
    i.total_time_spent_idle AS total_time_idle,
    i.window_lock_time
  FROM gold_dashboard.analytics_emp_app_info i
  JOIN FilteredEmployees e ON i.emp_id = e.emp_id
  WHERE (i.cal_date BETWEEN '{start_date}' AND '{end_date}' OR i.cal_date BETWEEN '{prev_start_date}' AND '{prev_end_date}')
),

ActivityWithShiftBounds AS (
  SELECT
    i.emp_id,
    i.shift_date,
    s.start_time,
    s.end_time,
    i.interval_start,
    i.interval_end,
    s.emp_login_time,
    s.emp_logout_time,
    i.total_time_active,
    i.total_time_idle,
    i.window_lock_time,
    GREATEST(i.interval_start, s.start_time) AS overlap_start,
    LEAST(i.interval_end, s.end_time) AS overlap_end,
    GREATEST(i.interval_start, s.emp_login_time) AS activity_start,
    LEAST(i.interval_end, s.emp_logout_time) AS activity_end
  FROM FilteredAppInfo i
  JOIN EmployeeShiftsWithConfig s ON i.emp_id = s.emp_id AND i.shift_date = s.shift_date
  WHERE s.emp_login_time < i.interval_end
    AND s.emp_logout_time > i.interval_start
),

ActivityTimeAllocation AS (
  SELECT
    emp_id,
    shift_date,
    total_time_active,
    total_time_idle,
    window_lock_time,
    start_time,
    end_time,
    
    -- Calculate seconds within shift
    CASE 
      WHEN activity_start >= end_time OR activity_end <= start_time THEN 0
      WHEN activity_start >= start_time AND activity_end <= end_time THEN
        UNIX_TIMESTAMP(activity_end) - UNIX_TIMESTAMP(activity_start)
      ELSE
        UNIX_TIMESTAMP(LEAST(activity_end, end_time)) - UNIX_TIMESTAMP(GREATEST(activity_start, start_time))
    END AS seconds_within_shift,
    
    -- Calculate seconds outside shift
    CASE 
      WHEN activity_start >= end_time OR activity_end <= start_time THEN
        UNIX_TIMESTAMP(activity_end) - UNIX_TIMESTAMP(activity_start)
      WHEN activity_start >= start_time AND activity_end <= end_time THEN 0
      ELSE
        (UNIX_TIMESTAMP(activity_end) - UNIX_TIMESTAMP(activity_start)) - 
        (UNIX_TIMESTAMP(LEAST(activity_end, end_time)) - UNIX_TIMESTAMP(GREATEST(activity_start, start_time)))
    END AS seconds_outside_shift
  FROM ActivityWithShiftBounds
  WHERE activity_start < activity_end
),

CurrentShiftData AS (
  SELECT
    a.emp_id,
    a.shift_date,
    
    -- Total metrics
    SUM(total_time_active) AS total_active_time,
    SUM(total_time_idle) AS total_idle_time,
    SUM(window_lock_time) AS total_lock_time,
    SUM(total_time_active + total_time_idle + window_lock_time) AS total_work_time,
    
    -- Within shift allocation
    SUM(
      total_time_active * (seconds_within_shift / NULLIF((seconds_within_shift + seconds_outside_shift), 0))
    ) AS active_within_shift,
    
    SUM(
      total_time_idle * (seconds_within_shift / NULLIF((seconds_within_shift + seconds_outside_shift), 0))
    ) AS idle_within_shift,
    
    SUM(
      window_lock_time * (seconds_within_shift / NULLIF((seconds_within_shift + seconds_outside_shift), 0))
    ) AS lock_within_shift,
    
    -- Outside shift allocation
    SUM(
      total_time_active * (seconds_outside_shift / NULLIF((seconds_within_shift + seconds_outside_shift), 0))
    ) AS active_outside_shift,
    
    SUM(
      total_time_idle * (seconds_outside_shift / NULLIF((seconds_within_shift + seconds_outside_shift), 0))
    ) AS idle_outside_shift,
    
    -- Shift configuration
    MAX(s.pulse_shift_time) AS pulse_shift_time,
    MAX(s.adjusted_active_time) AS adjusted_active_time,
    MAX(s.adjusted_lock_time) AS adjusted_lock_time,
    
    -- Period indicator
    CASE 
      WHEN a.shift_date BETWEEN '{start_date}' AND '{end_date}' THEN 'current'
      WHEN a.shift_date BETWEEN '{prev_start_date}' AND '{prev_end_date}' THEN 'previous'
    END AS period
    
  FROM ActivityTimeAllocation a
  JOIN EmployeeShiftsWithConfig s ON a.emp_id = s.emp_id AND a.shift_date = s.shift_date
  GROUP BY a.emp_id, a.shift_date, 
    CASE 
      WHEN a.shift_date BETWEEN '{start_date}' AND '{end_date}' THEN 'current'
      WHEN a.shift_date BETWEEN '{prev_start_date}' AND '{prev_end_date}' THEN 'previous'
    END
),

PeriodAggregates AS (
  SELECT
    period,
    AVG(total_work_time) AS avg_total_work_time,
    AVG(total_active_time) AS avg_total_active_time,
    AVG(total_idle_time) AS avg_total_idle_time,
    AVG(total_lock_time) AS avg_total_lock_time,
    AVG(active_within_shift + idle_within_shift + lock_within_shift) AS avg_total_work_time_within_shift,
    AVG(active_outside_shift + idle_outside_shift) AS avg_total_work_time_outside_shift,
    AVG(pulse_shift_time * 3600) AS avg_pulse_shift_time_seconds,
    AVG(adjusted_active_time * 3600) AS avg_adjusted_active_time_seconds,
    AVG(adjusted_lock_time * 3600) AS avg_adjusted_lock_time_seconds
  FROM CurrentShiftData
  WHERE period IN ('current', 'previous')
  GROUP BY period
)

SELECT
  -- Current period metrics
  curr.avg_total_work_time AS current_total_work_time,
  curr.avg_total_active_time AS current_total_active_time,
  curr.avg_total_idle_time AS current_total_idle_time,
  curr.avg_total_lock_time AS current_total_lock_time,
  curr.avg_total_work_time_within_shift AS current_total_work_time_within_shift,
  curr.avg_total_work_time_outside_shift AS current_total_work_time_outside_shift,
  
  -- Schedule comparisons
  (curr.avg_total_work_time - curr.avg_pulse_shift_time_seconds) AS total_work_shift_schedule,
  CASE 
    WHEN (curr.avg_total_work_time - curr.avg_pulse_shift_time_seconds) > 0 THEN 'Up'
    WHEN (curr.avg_total_work_time - curr.avg_pulse_shift_time_seconds) < 0 THEN 'Down'
    ELSE 'No Change'
  END AS total_work_shift_schedule_trend,
  
  (curr.avg_total_active_time - curr.avg_adjusted_active_time_seconds) AS total_active_shift_schedule,
  CASE 
    WHEN (curr.avg_total_active_time - curr.avg_adjusted_active_time_seconds) > 0 THEN 'Up'
    WHEN (curr.avg_total_active_time - curr.avg_adjusted_active_time_seconds) < 0 THEN 'Down'
    ELSE 'No Change'
  END AS total_active_shift_schedule_trend,
  
  (curr.avg_total_lock_time - curr.avg_adjusted_lock_time_seconds) AS total_lock_shift_schedule,
  CASE 
    WHEN (curr.avg_total_lock_time - curr.avg_adjusted_lock_time_seconds) > 0 THEN 'Up'
    WHEN (curr.avg_total_lock_time - curr.avg_adjusted_lock_time_seconds) < 0 THEN 'Down'
    ELSE 'No Change'
  END AS total_lock_shift_schedule_trend,
  
  -- Previous period metrics
  prev.avg_total_work_time AS prev_total_work_time,
  prev.avg_total_active_time AS prev_total_active_time,
  prev.avg_total_idle_time AS prev_total_idle_time,
  prev.avg_total_lock_time AS prev_total_lock_time,
  
  -- Period comparison trends
  CASE 
    WHEN curr.avg_total_work_time > prev.avg_total_work_time THEN 'Up'
    WHEN curr.avg_total_work_time < prev.avg_total_work_time THEN 'Down'
    ELSE 'No Change'
  END AS total_work_trend,
  
  ROUND(((curr.avg_total_work_time - prev.avg_total_work_time) / NULLIF(prev.avg_total_work_time, 0)) * 100, 2) AS total_work_percentage_change,
  
  CASE 
    WHEN curr.avg_total_active_time > prev.avg_total_active_time THEN 'Up'
    WHEN curr.avg_total_active_time < prev.avg_total_active_time THEN 'Down'
    ELSE 'No Change'
  END AS total_active_trend,
  
  ROUND(((curr.avg_total_active_time - prev.avg_total_active_time) / NULLIF(prev.avg_total_active_time, 0)) * 100, 2) AS total_active_percentage_change,
  
  CASE 
    WHEN curr.avg_total_idle_time > prev.avg_total_idle_time THEN 'Up'
    WHEN curr.avg_total_idle_time < prev.avg_total_idle_time THEN 'Down'
    ELSE 'No Change'
  END AS total_idle_trend,
  
  ROUND(((curr.avg_total_idle_time - prev.avg_total_idle_time) / NULLIF(prev.avg_total_idle_time, 0)) * 100, 2) AS total_idle_percentage_change,
  
  CASE 
    WHEN curr.avg_total_lock_time > prev.avg_total_lock_time THEN 'Up'
    WHEN curr.avg_total_lock_time < prev.avg_total_lock_time THEN 'Down'
    ELSE 'No Change'
  END AS total_lock_trend,
  
  ROUND(((curr.avg_total_lock_time - prev.avg_total_lock_time) / NULLIF(prev.avg_total_lock_time, 0)) * 100, 2) AS total_lock_percentage_change
  
FROM 
  (SELECT * FROM PeriodAggregates WHERE period = 'current') curr
CROSS JOIN
  (SELECT * FROM PeriodAggregates WHERE period = 'previous') prev
WHERE curr.avg_total_work_time IS NOT NULL AND prev.avg_total_work_time IS NOT NULL






---------
-----------------
WITH ActivityWithShiftBounds AS (
  SELECT
    i.emp_id,
    i.shift_date,
    i.start_time,
    i.end_time,
    i.interval_start,
    i.interval_end,
    i.emp_login_time,
    i.emp_logout_time,
    i.total_time_active,
    i.total_time_idle,
    i.window_lock_time,
    greatest(i.interval_start, i.start_time) AS overlap_start,
    least(i.interval_end, i.end_time) AS overlap_end,
    greatest(i.interval_start, i.emp_login_time) AS activity_start,
    least(i.interval_end, i.emp_logout_time) AS activity_end
  FROM FilteredAppInfo i
  LEFT JOIN EmployeeShiftsWithConfig s
    ON i.emp_id = s.emp_id AND i.shift_date = s.shift_date
),

ActivityTimeAllocation AS (
  SELECT
    emp_id,
    shift_date,
    total_time_active,
    total_time_idle,
    window_lock_time,
    start_time,
    end_time,
    activity_start,
    activity_end,
    
    -- Calculate seconds within shift (considers BOTH login/logout AND shift boundaries)
    CASE 
      WHEN activity_start >= end_time OR activity_end <= start_time THEN
        0
      WHEN activity_start >= start_time AND activity_end <= end_time THEN
        unix_timestamp(activity_end) - unix_timestamp(activity_start)
      ELSE
        unix_timestamp(least(activity_end, end_time)) - unix_timestamp(greatest(activity_start, start_time))
    END AS seconds_within_shift,
    
    -- Calculate seconds outside shift
    CASE 
      WHEN activity_start >= end_time OR activity_end <= start_time THEN
        unix_timestamp(activity_end) - unix_timestamp(activity_start)
      WHEN activity_start >= start_time AND activity_end <= end_time THEN
        0
      ELSE
        (unix_timestamp(activity_end) - unix_timestamp(activity_start)) - 
        (unix_timestamp(least(activity_end, end_time)) - unix_timestamp(greatest(activity_start, start_time)))
    END AS seconds_outside_shift,
    
    -- Total activity seconds for validation
    unix_timestamp(activity_end) - unix_timestamp(activity_start) AS total_activity_seconds
    
  FROM ActivityWithShiftBounds
  WHERE activity_start < activity_end
),

CurrentShiftData AS (
  SELECT
    a.emp_id,
    a.shift_date,
    
    -- Total metrics
    SUM(total_time_active) AS total_active_time,
    SUM(total_time_idle) AS total_idle_time,
    SUM(window_lock_time) AS total_lock_time,
    SUM(total_time_active + total_time_idle + window_lock_time) AS total_work_time,
    
    -- Within shift allocation
    SUM(
      CASE 
        WHEN (seconds_within_shift + seconds_outside_shift) = 0 THEN 0
        ELSE total_time_active * (seconds_within_shift / (seconds_within_shift + seconds_outside_shift))
      END
    ) AS active_within_shift,
    
    SUM(
      CASE 
        WHEN (seconds_within_shift + seconds_outside_shift) = 0 THEN 0
        ELSE total_time_idle * (seconds_within_shift / (seconds_within_shift + seconds_outside_shift))
      END
    ) AS idle_within_shift,
    
    SUM(
      CASE 
        WHEN (seconds_within_shift + seconds_outside_shift) = 0 THEN 0
        ELSE window_lock_time * (seconds_within_shift / (seconds_within_shift + seconds_outside_shift))
      END
    ) AS lock_within_shift,
    
    -- Outside shift allocation (only active + idle, no lock time)
    SUM(
      CASE 
        WHEN (seconds_within_shift + seconds_outside_shift) = 0 THEN 0
        ELSE total_time_active * (seconds_outside_shift / (seconds_within_shift + seconds_outside_shift))
      END
    ) AS active_outside_shift,
    
    SUM(
      CASE 
        WHEN (seconds_within_shift + seconds_outside_shift) = 0 THEN 0
        ELSE total_time_idle * (seconds_outside_shift / (seconds_within_shift + seconds_outside_shift))
      END
    ) AS idle_outside_shift,
    
    -- Shift configuration
    MAX(s.pulse_shift_time) AS pulse_shift_time,
    MAX(s.adjusted_active_time) AS adjusted_active_time,
    MAX(s.adjusted_lock_time) AS adjusted_lock_time,
    MIN(s.start_time) AS shift_start_time,
    MAX(s.end_time) AS shift_end_time,
    
    -- Debug totals for validation
    SUM(seconds_within_shift) AS total_seconds_within_shift,
    SUM(seconds_outside_shift) AS total_seconds_outside_shift,
    SUM(total_activity_seconds) AS total_activity_seconds
    
  FROM ActivityTimeAllocation a
  JOIN EmployeeShiftsWithConfig s
    ON a.emp_id = s.emp_id AND a.shift_date = s.shift_date
  GROUP BY a.emp_id, a.shift_date
)

SELECT
  emp_id,
  shift_date,
  total_active_time,
  total_idle_time,
  total_lock_time,
  total_work_time,
  active_within_shift,
  idle_within_shift,
  lock_within_shift,
  active_outside_shift,
  idle_outside_shift,
  (active_within_shift + idle_within_shift + lock_within_shift) AS time_within_shift,
  (active_outside_shift + idle_outside_shift) AS time_outside_shift,
  pulse_shift_time,
  adjusted_active_time,
  adjusted_lock_time,
  shift_start_time,
  shift_end_time,
  -- Debug columns (can remove in production)
  total_seconds_within_shift,
  total_seconds_outside_shift,
  total_activity_seconds
FROM CurrentShiftData
ORDER BY emp_id, shift_date






Let me walk through your examples using Approach 1 logic:

Example 1: Shift 10:25:00 - 19:25:00, Login 10:45:12, Logout 20:42:45

For 10:00-11:00 interval:

· Activity: 10:45:12 - 11:00:00 (14m48s)
· seconds_within_shift = unix_timestamp('11:00:00') - unix_timestamp('10:45:12') = 888 seconds
· seconds_outside_shift = 0 seconds (all within shift)
· Ratio = 888/(888+0) = 1.0
· Result: 100% of active/idle/lock time allocated to WITHIN shift

For 19:00-20:00 interval:

· Activity: 19:00:00 - 20:00:00 (60m)
· Within shift portion: 19:00:00 - 19:25:00 (25m)
· Outside shift portion: 19:25:00 - 20:00:00 (35m)
· seconds_within_shift = unix_timestamp('19:25:00') - unix_timestamp('19:00:00') = 1500 seconds
· seconds_outside_shift = unix_timestamp('20:00:00') - unix_timestamp('19:25:00') = 2100 seconds
· Ratio = 1500/(1500+2100) = 0.4167 (within), 0.5833 (outside)
· Result: 41.67% of active/idle/lock to WITHIN shift, 58.33% of active/idle to OUTSIDE shift

For 20:00-21:00 interval:

· Activity: 20:00:00 - 20:42:45 (42m45s)
· seconds_within_shift = 0 seconds (completely after shift end)
· seconds_outside_shift = unix_timestamp('20:42:45') - unix_timestamp('20:00:00') = 2565 seconds
· Ratio = 0/(0+2565) = 0.0 (within), 1.0 (outside)
· Result: 100% of active/idle to OUTSIDE shift, 0% lock time

---

Example 2: Shift 10:25:00 - 19:25:00, Login 09:12:14, Logout 16:20:00

For 09:00-10:00 interval:

· Activity: 09:12:14 - 10:00:00 (47m46s)
· seconds_within_shift = 0 seconds (completely before shift start)
· seconds_outside_shift = unix_timestamp('10:00:00') - unix_timestamp('09:12:14') = 2866 seconds
· Ratio = 0/(0+2866) = 0.0 (within), 1.0 (outside)
· Result: 100% of active/idle to OUTSIDE shift

For 10:00-11:00 interval:

· Activity: 10:00:00 - 11:00:00 (60m)
· Within shift portion: 10:25:00 - 11:00:00 (35m)
· Outside shift portion: 10:00:00 - 10:25:00 (25m)
· seconds_within_shift = unix_timestamp('11:00:00') - unix_timestamp('10:25:00') = 2100 seconds
· seconds_outside_shift = unix_timestamp('10:25:00') - unix_timestamp('10:00:00') = 1500 seconds
· Ratio = 2100/(2100+1500) = 0.5833 (within), 0.4167 (outside)
· Result: 58.33% of active/idle/lock to WITHIN shift, 41.67% of active/idle to OUTSIDE shift

---

Example 3: Shift 09:30:00 - 18:30:00, Login 10:15:12, Logout 20:42:45

For 10:00-11:00 interval:

· Activity: 10:15:12 - 11:00:00 (44m48s)
· seconds_within_shift = unix_timestamp('11:00:00') - unix_timestamp('10:15:12') = 2688 seconds
· seconds_outside_shift = 0 seconds (all within shift)
· Ratio = 2688/(2688+0) = 1.0
· Result: 100% of active/idle/lock to WITHIN shift

For 18:00-19:00 interval:

· Activity: 18:00:00 - 19:00:00 (60m)
· Within shift portion: 18:00:00 - 18:30:00 (30m)
· Outside shift portion: 18:30:00 - 19:00:00 (30m)
· seconds_within_shift = unix_timestamp('18:30:00') - unix_timestamp('18:00:00') = 1800 seconds
· seconds_outside_shift = unix_timestamp('19:00:00') - unix_timestamp('18:30:00') = 1800 seconds
· Ratio = 1800/(1800+1800) = 0.5 (within), 0.5 (outside)
· Result: 50% of active/idle/lock to WITHIN shift, 50% of active/idle to OUTSIDE shift

Key Points:

1. Automatic handling: The ratio seconds_within_shift/(total_seconds) automatically gives correct allocation
2. Complete within: Ratio = 1.0 → 100% within shift
3. Complete outside: Ratio = 0.0 → 100% outside shift
4. Partial overlap: Exact proportional split based on time
5. Lock time: Only gets allocated to within-shift portion (as per business rule)

The logic is mathematically precise and handles all your scenarios correctly!






from datetime import datetime, date, timedelta
from typing import Optional
from fastapi import Query, Header, Depends
from fastapi.responses import JSONResponse
from fastapi import status
from auth.jwt_handler import AuthJWT

def get_last_day_of_month(dt: date) -> date:
    """Get the last day of the month for a given date"""
    if dt.month == 12:
        return date(dt.year + 1, 1, 1) - timedelta(days=1)
    return date(dt.year, dt.month + 1, 1) - timedelta(days=1)

@router.get('/get_team_active_idle_app_category')
async def get_team_app_activity(
    Authorize: AuthJWT = Depends(),
    authorization: str = Header(None),
    start_date: str = Query(...),
    end_date: str = Query(...),
    date_range: Optional[str] = Query("day")
):
    if not authorization or not authorization.startswith("Bearer"):
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"msg": "Authorization Token Missing"}
        )
    
    try:
        Authorize.jwt_required()
        user_identity = Authorize.get_jwt_subject()
        claims = Authorize.get_raw_jwt()
        manager_id = claims.get("user_id")
        
        if not manager_id:
            return JSONResponse(
                status_code=status.HTTP_401_UNAUTHORIZED,
                content={'message': "Invalid Token Claims"}
            )
        
        today = datetime.today().date()
        
        if start_date and end_date:
            try:
                start_date_dt = datetime.strptime(start_date, "%Y-%m-%d").date()
                end_date_dt = datetime.strptime(end_date, "%Y-%m-%d").date()
                
                period_days = (end_date_dt - start_date_dt).days + 1
                
                # Handle different date range types
                if date_range.lower() == "day":
                    # Previous week's same day
                    prev_start_date = start_date_dt - timedelta(days=7)
                    prev_end_date = prev_start_date
                    ext_end_date = end_date_dt + timedelta(days=1)
                    
                elif date_range.lower() == "week":
                    # Previous week's same days
                    prev_start_date = start_date_dt - timedelta(days=7)
                    prev_end_date = end_date_dt - timedelta(days=7)
                    ext_end_date = end_date_dt + timedelta(days=1)
                    
                elif date_range.lower() == "month":
                    # Check if selected dates are in current month
                    current_month = today.month
                    current_year = today.year
                    selected_month = start_date_dt.month
                    selected_year = start_date_dt.year
                    
                    if selected_month == current_month and selected_year == current_year:
                        # Current month: go back exactly 4 weeks (28 days)
                        prev_start_date = start_date_dt - timedelta(days=28)
                        prev_end_date = end_date_dt - timedelta(days=28)
                    else:
                        # Not current month: previous month's start and end
                        if start_date_dt.month == 1:
                            prev_start_date = date(start_date_dt.year - 1, 12, 1)
                        else:
                            prev_start_date = date(start_date_dt.year, start_date_dt.month - 1, 1)
                        
                        if end_date_dt.month == 1:
                            prev_end_date = date(end_date_dt.year - 1, 12, 31)
                        else:
                            prev_month = end_date_dt.month - 1
                            prev_year = end_date_dt.year
                            if prev_month == 0:
                                prev_month = 12
                                prev_year = end_date_dt.year - 1
                            prev_end_date = get_last_day_of_month(date(prev_year, prev_month, 1))
                    
                    ext_end_date = end_date_dt + timedelta(days=1)
                
                else:
                    # DEFAULT: Exact period days logic for unknown date_range values
                    prev_end_date = start_date_dt - timedelta(days=1)
                    prev_start_date = prev_end_date - timedelta(days=period_days - 1)
                    ext_end_date = end_date_dt + timedelta(days=1)
                
                return {
                    "start_date": start_date_dt.isoformat(),
                    "end_date": end_date_dt.isoformat(),
                    "previous_start_date": prev_start_date.isoformat(),
                    "previous_end_date": prev_end_date.isoformat(),
                    "extended_end_date": ext_end_date.isoformat(),
                    "date_range": date_range,
                    "period_days": period_days
                }
                
            except ValueError:
                return JSONResponse(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    content={"msg": "Invalid date format. Use YYYY-MM-DD."}
                )
        else:
            # Default to daily
            start_date_dt = today
            end_date_dt = today
            period_days = 1
            prev_end_date = start_date_dt - timedelta(days=1)
            prev_start_date = prev_end_date - timedelta(days=period_days - 1)
            ext_end_date = end_date_dt + timedelta(days=1)
            
            return {
                "start_date": start_date_dt.isoformat(),
                "end_date": end_date_dt.isoformat(),
                "previous_start_date": prev_start_date.isoformat(),
                "previous_end_date": prev_end_date.isoformat(),
                "extended_end_date": ext_end_date.isoformat(),
                "date_range": "day",
                "period_days": period_days
            }
            
    except Exception as e:
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={"msg": f"Internal server error: {str(e)}"}
        )




pip installs python-dateutil

from datetime import datetime, date, timedelta
from dateutil.relativedelta import relativedelta
from typing import Optional
from fastapi import Query, Header, Depends
from fastapi.responses import JSONResponse
from fastapi import status
from auth.jwt_handler import AuthJWT

@router.get('/get_team_active_idle_app_category')
async def get_team_app_activity(
    Authorize: AuthJWT = Depends(),
    authorization: str = Header(None),
    start_date: str = Query(...),
    end_date: str = Query(...),
    date_range: Optional[str] = Query("day")
):
    if not authorization or not authorization.startswith("Bearer"):
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"msg": "Authorization Token Missing"}
        )
    
    try:
        Authorize.jwt_required()
        user_identity = Authorize.get_jwt_subject()
        claims = Authorize.get_raw_jwt()
        manager_id = claims.get("user_id")
        
        if not manager_id:
            return JSONResponse(
                status_code=status.HTTP_401_UNAUTHORIZED,
                content={'message': "Invalid Token Claims"}
            )
        
        today = datetime.today().date()
        
        if start_date and end_date:
            try:
                start_date_dt = datetime.strptime(start_date, "%Y-%m-%d").date()
                end_date_dt = datetime.strptime(end_date, "%Y-%m-%d").date()
                
                period_days = (end_date_dt - start_date_dt).days + 1
                
                # Handle different date range types
                if date_range.lower() == "day":
                    # Previous week's same day
                    prev_start_date = start_date_dt - timedelta(days=7)
                    prev_end_date = prev_start_date  # Same day for day range
                    ext_end_date = end_date_dt + timedelta(days=1)
                    
                elif date_range.lower() == "week":
                    # Previous week's same days (exact number of days)
                    prev_start_date = start_date_dt - timedelta(days=7)
                    prev_end_date = end_date_dt - timedelta(days=7)
                    ext_end_date = end_date_dt + timedelta(days=1)
                    
                elif date_range.lower() == "month":
                    # Check if selected dates are in current month
                    current_month = today.month
                    current_year = today.year
                    selected_month = start_date_dt.month
                    selected_year = start_date_dt.year
                    
                    if selected_month == current_month and selected_year == current_year:
                        # Current month: go back exactly 4 weeks (28 days)
                        prev_start_date = start_date_dt - timedelta(days=28)
                        prev_end_date = end_date_dt - timedelta(days=28)
                    else:
                        # Not current month: previous month's start and end
                        # Get first day of previous month for start_date
                        prev_start_date = (start_date_dt.replace(day=1) - relativedelta(months=1)).replace(day=1)
                        
                        # Get last day of previous month for end_date
                        prev_month_end = (end_date_dt.replace(day=1) - relativedelta(months=1))
                        prev_end_date = (prev_month_end + relativedelta(months=1, days=-1))
                    
                    ext_end_date = end_date_dt + timedelta(days=1)
                
                else:
                    # Default to day logic for unknown ranges
                    prev_start_date = start_date_dt - timedelta(days=7)
                    prev_end_date = prev_start_date
                    ext_end_date = end_date_dt + timedelta(days=1)
                
                # Return the calculated dates
                return {
                    "start_date": start_date_dt.isoformat(),
                    "end_date": end_date_dt.isoformat(),
                    "previous_start_date": prev_start_date.isoformat(),
                    "previous_end_date": prev_end_date.isoformat(),
                    "extended_end_date": ext_end_date.isoformat(),
                    "date_range": date_range,
                    "period_days": period_days
                }
                
            except ValueError:
                return JSONResponse(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    content={"msg": "Invalid date format. Use YYYY-MM-DD."}
                )
        else:
            # Default to daily when no dates provided
            start_date_dt = today
            end_date_dt = today
            prev_start_date = today - timedelta(days=7)
            prev_end_date = prev_start_date
            ext_end_date = today + timedelta(days=1)
            
            return {
                "start_date": start_date_dt.isoformat(),
                "end_date": end_date_dt.isoformat(),
                "previous_start_date": prev_start_date.isoformat(),
                "previous_end_date": prev_end_date.isoformat(),
                "extended_end_date": ext_end_date.isoformat(),
                "date_range": "day",
                "period_days": 1
            }
            
    except Exception as e:
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={"msg": f"Internal server error: {str(e)}"}
        )







Here’s a cleaner, structured draft you can use for your proposal email or document:


---

Subject: Proposal to Exclude Week-Off Data from Average Calculations in RPEM APIs

Background:
Currently, our system captures and stores agent activity data for all employees, regardless of whether the date falls on a working day or a week-off (e.g., Saturday, Sunday, or designated weekly off-days). This includes cases where an employee might log in briefly (e.g., 10 minutes or 1 hour) during their week-off.

When average calculations are performed — whether for a week, month, or custom date range — this week-off data is included in the calculation. As a result:

A small amount of activity on a week-off skews both individual and team averages.

The reported averages may not accurately reflect true working-day performance.


Proposed Change:
For all APIs that calculate averages in RPEM, we propose excluding week-off employee activity from the average computation. This means:

If a date is a week-off for a given employee, their data for that date will be ignored in average calculations.

Averages will be calculated only on valid working days for each employee.


Expected Benefits:

More accurate and meaningful average metrics for individuals and teams.

Avoids distortion of averages caused by minimal week-off activity.

Consistency across all APIs, regardless of the selected time period filter (week, month, or custom).


Scope:

Changes will be implemented in all RPEM APIs that involve average calculations.

The logic to exclude week-off data will be applied at the calculation stage.


Request:
We seek sign-off on this proposal so that development can proceed with updating all relevant APIs to exclude week-off data from average computations.


---

Do you want me to also add a visual example showing how averages change before vs. after this change? That would make the proposal even easier to approve.

