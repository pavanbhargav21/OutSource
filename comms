WITH RECURSIVE EmployeeHierarchy AS (
    -- Anchor: Start with top-level managers (those with no manager above them)
    SELECT 
        emp_id,
        emp_name,
        funk_manager_id,
        emp_name AS top_manager,
        1 AS level,
        CAST(emp_id AS VARCHAR(100)) AS hierarchy_path
    FROM inbound.hr_employ_central
    WHERE funk_manager_id IS NULL  -- Top-level employees
    
    UNION ALL
    
    -- Recursive: Get subordinates
    SELECT 
        e.emp_id,
        e.emp_name,
        e.funk_manager_id,
        h.top_manager,
        h.level + 1 AS level,
        CAST(h.hierarchy_path || '->' || CAST(e.emp_id AS VARCHAR) AS VARCHAR(100))
    FROM inbound.hr_employ_central e
    INNER JOIN EmployeeHierarchy h 
        ON e.funk_manager_id = h.emp_id
)
SELECT 
    emp_id,
    emp_name,
    funk_manager_id,
    top_manager,
    level,
    hierarchy_path
FROM EmployeeHierarchy
ORDER BY top_manager, level, emp_name;


WITH RECURSIVE ManagerChain AS (
    -- Anchor: Start with a specific employee or all employees
    SELECT 
        emp_id,
        emp_name,
        funk_manager_id,
        1 AS level,
        CAST(emp_name AS VARCHAR(500)) AS management_chain
    FROM inbound.hr_employ_central
    -- Remove WHERE clause to get for all employees
    -- WHERE emp_id = 'specific_employee_id'  -- For specific employee
    
    UNION ALL
    
    -- Recursive: Go up the hierarchy
    SELECT 
        m.emp_id,
        m.emp_name,
        e.funk_manager_id,
        m.level + 1 AS level,
        CAST(m.management_chain || ' → ' || e.emp_name AS VARCHAR(500))
    FROM ManagerChain m
    INNER JOIN inbound.hr_employ_central e 
        ON m.funk_manager_id = e.emp_id
)
SELECT 
    emp_id,
    emp_name,
    level,
    management_chain AS full_management_chain
FROM ManagerChain
ORDER BY emp_id, level DESC;



WITH RECURSIVE OrgHierarchy AS (
    -- Top-level employees
    SELECT 
        emp_id,
        emp_name,
        funk_manager_id,
        emp_name AS hierarchy_tree,
        1 AS level,
        1 AS sort_order
    FROM inbound.hr_employ_central
    WHERE funk_manager_id IS NULL
    
    UNION ALL
    
    -- Subordinates
    SELECT 
        e.emp_id,
        e.emp_name,
        e.funk_manager_id,
        h.hierarchy_tree || ' > ' || e.emp_name,
        h.level + 1,
        h.sort_order * 100 + ROW_NUMBER() OVER (PARTITION BY e.funk_manager_id ORDER BY e.emp_name)
    FROM inbound.hr_employ_central e
    INNER JOIN OrgHierarchy h 
        ON e.funk_manager_id = h.emp_id
)
SELECT 
    level,
    LPAD('', (level - 1) * 4, ' ') || emp_name AS indented_name,
    emp_id,
    funk_manager_id,
    hierarchy_tree,
    sort_order
FROM OrgHierarchy
ORDER BY sort_order;




-- First, get upward chain (managers above)
WITH RECURSIVE UpwardChain AS (
    SELECT 
        emp_id,
        emp_name,
        funk_manager_id,
        0 AS level
    FROM inbound.hr_employ_central
    WHERE emp_id = 'your_employee_id_here'  -- Specify employee
    
    UNION ALL
    
    SELECT 
        m.emp_id,
        m.emp_name,
        m.funk_manager_id,
        uc.level - 1
    FROM inbound.hr_employ_central m
    INNER JOIN UpwardChain uc 
        ON m.emp_id = uc.funk_manager_id
),
-- Then get downward chain (employees below)
DownwardChain AS (
    SELECT 
        emp_id,
        emp_name,
        funk_manager_id,
        0 AS level
    FROM inbound.hr_employ_central
    WHERE emp_id = 'your_employee_id_here'  -- Same employee
    
    UNION ALL
    
    SELECT 
        e.emp_id,
        e.emp_name,
        e.funk_manager_id,
        dc.level + 1
    FROM inbound.hr_employ_central e
    INNER JOIN DownwardChain dc 
        ON e.funk_manager_id = dc.emp_id
)
-- Combine results
SELECT 'Manager Above' AS relation_type, level, emp_id, emp_name, funk_manager_id
FROM UpwardChain
WHERE level < 0

UNION ALL

SELECT 'Employee' AS relation_type, 0, emp_id, emp_name, funk_manager_id
FROM UpwardChain
WHERE level = 0

UNION ALL

SELECT 'Subordinate' AS relation_type, level, emp_id, emp_name, funk_manager_id
FROM DownwardChain
WHERE level > 0

ORDER BY relation_type DESC, level;

from pyspark.sql import functions as F
from datetime import datetime, timedelta
from pyspark.sql.window import Window

# -------------------------
# Configurations
# -------------------------
dbutils.widgets.text("dynamic_hours", "24000")
try:
    dynamic_hours = int(dbutils.widgets.get("dynamic_hours"))
except Exception as e:
    print(f"Error parsing dynamic_hours: {e}, using default 1")
    dynamic_hours = 24000

DEFAULT_START = "09:00:00"
DEFAULT_END = "18:00:00"
ZERO_TIME = "00:00:00"
PROCESSING_WINDOW_MINUTES = 60 * dynamic_hours
print(f"Processing window: {PROCESSING_WINDOW_MINUTES} minutes")

# -------------------------
# Time Setup
# -------------------------
current_time = datetime.now()
ingestion_threshold = current_time - timedelta(minutes=PROCESSING_WINDOW_MINUTES)

# -------------------------
# Step 1: Get Pulse Lite Users Data
# -------------------------

# Get login data for Pulse Lite users (first and last entry per day)
login_data_df = spark.table("sys_trace.emp_logindata") \
    .filter(F.col("ingestion_time") >= F.lit(ingestion_threshold)) \
    .filter(F.col("pulse_lite_version").isNotNull()) \
    .select("emp_id", "cal_date", "event_time") \
    .withColumn("is_login_data", F.lit(True))

# Get Window Lock/Unlock activities for Pulse Lite users
window_events_df = spark.table("app_trace.emp_activity") \
    .filter(F.col("ingestion_time") >= F.lit(ingestion_threshold)) \
    .filter(F.col("app_name").isin("Window Lock", "Window Unlock")) \
    .select("emp_id", "cal_date", "start_time", "app_name") \
    .withColumn("is_window_lock", F.when(F.col("app_name") == "Window Lock", F.lit(True)).otherwise(F.lit(False))) \
    .withColumn("is_window_unlock", F.when(F.col("app_name") == "Window Unlock", F.lit(True)).otherwise(F.lit(False)))

# Combine both sources to get all Pulse Lite user-date combinations
pulse_lite_dates = login_data_df.select("emp_id", "cal_date").distinct().union(
    window_events_df.select("emp_id", "cal_date").distinct()
).distinct().withColumn("is_pulse_lite", F.lit(True))

# -------------------------
# Step 2: Get Manager ID from HR Central
# -------------------------
hr_central_df = spark.table("hr_central.employee_master") \
    .select("emp_id", "func_manager_id") \
    .withColumnRenamed("emp_id", "hr_emp_id") \
    .withColumnRenamed("func_manager_id", "manager_id")

# -------------------------
# Add previous date
# -------------------------
emp_dates_df = pulse_lite_dates.withColumn("prev_date", F.date_sub("cal_date", 1))

# Join with manager data
emp_dates_with_manager = emp_dates_df.join(
    hr_central_df,
    emp_dates_df.emp_id == hr_central_df.hr_emp_id,
    "left"
).drop("hr_emp_id")

# -------------------------
# Step 3: Load shift data
# -------------------------
shift_df = spark.table("inbound.pulse_emp_shift_info") \
    .select("emp_id", "shift_date", "start_time", "end_time", "is_week_off")

# -------------------------
# Step 4: Join current shift
# -------------------------
cur_shift = emp_dates_with_manager.join(
    shift_df,
    (emp_dates_with_manager.emp_id == shift_df.emp_id) & (emp_dates_with_manager.cal_date == shift_df.shift_date),
    how='left'
).select(
    emp_dates_with_manager.emp_id,
    emp_dates_with_manager.cal_date,
    emp_dates_with_manager.prev_date,
    emp_dates_with_manager.is_pulse_lite,
    emp_dates_with_manager.manager_id,
    shift_df.start_time.alias("cur_start_time_raw"),
    shift_df.end_time.alias("cur_end_time_raw"),
    shift_df.is_week_off
)

# Step 5: Join previous shift
prev_shift = shift_df \
    .withColumnRenamed("shift_date", "prev_cal_date") \
    .withColumnRenamed("start_time", "prev_start_time_raw") \
    .withColumnRenamed("end_time", "prev_end_time_raw") \
    .withColumnRenamed("emp_id", "emp_id_prev") \
    .withColumnRenamed("is_week_off", "prev_is_week_off")

cur_with_prev = cur_shift.join(
    prev_shift,
    (cur_shift.emp_id == prev_shift.emp_id_prev) & (cur_shift.prev_date == prev_shift.prev_cal_date),
    how="left"
).drop("emp_id_prev", "prev_cal_date")

# Step 6: Apply defaults based on missing data and day of week
final_df = cur_with_prev \
    .withColumn("dow", F.date_format("cal_date", "E")) \
    .withColumn("prev_dow", F.date_format("prev_date", "E")) \
    .withColumn("cur_start_time",
        F.when(F.col("cur_start_time_raw").isNotNull(), F.col("cur_start_time_raw"))
         .when(F.col("is_week_off") == True, F.lit(ZERO_TIME))
         .when(F.col("dow").isin("Sat", "Sun"), F.lit(ZERO_TIME))
         .otherwise(F.lit(DEFAULT_START))
    ) \
    .withColumn("cur_end_time",
        F.when(F.col("cur_end_time_raw").isNotNull(), F.col("cur_end_time_raw"))
         .when(F.col("is_week_off") == True, F.lit(ZERO_TIME))
         .when(F.col("dow").isin("Sat", "Sun"), F.lit(ZERO_TIME))
         .otherwise(F.lit(DEFAULT_END))
    ) \
    .withColumn("prev_start_time",
        F.when(F.col("prev_start_time_raw").isNotNull(), F.col("prev_start_time_raw"))
         .when(F.col("prev_is_week_off") == True, F.lit(ZERO_TIME))
         .when(F.col("prev_dow").isin("Sat", "Sun"), F.lit(ZERO_TIME))
         .otherwise(F.lit(DEFAULT_START))
    ) \
    .withColumn("prev_end_time",
        F.when(F.col("prev_end_time_raw").isNotNull(), F.col("prev_end_time_raw"))
         .when(F.col("prev_is_week_off") == True, F.lit(ZERO_TIME))
         .when(F.col("prev_dow").isin("Sat", "Sun"), F.lit(ZERO_TIME))
         .otherwise(F.lit(DEFAULT_END))
    ) \
    .withColumn("cur_end_time",
        F.when(F.col("cur_start_time") > F.col("cur_end_time"),
               F.date_format(F.expr("cast(cur_end_time as timestamp) + interval 1 day"), "HH:mm:ss")
        ).otherwise(F.col("cur_end_time"))
    ) \
    .withColumn("prev_end_time",
        F.when(F.col("prev_start_time") > F.col("prev_end_time"),
               F.date_format(F.expr("cast(prev_end_time as timestamp) + interval 1 day"), "HH:mm:ss")
        ).otherwise(F.col("prev_end_time"))
    ) \
    .withColumn("cur_start_time_ts", F.concat_ws(" ", F.col("cal_date"), F.col("cur_start_time"))) \
    .withColumn("cur_end_time_ts", F.concat_ws(" ", F.col("cal_date"), F.col("cur_end_time"))) \
    .withColumn("prev_start_time_ts", F.concat_ws(" ", F.col("prev_date"), F.col("prev_start_time"))) \
    .withColumn("prev_end_time_ts", F.concat_ws(" ", F.col("prev_date"), F.col("prev_end_time"))) \
    .withColumn("cur_end_time_ts",
        F.when(F.col("cur_start_time") > F.col("cur_end_time"),
               F.date_format(F.expr("cast(cur_end_time_ts as timestamp) + interval 1 day"), "yyyy-MM-dd HH:mm:ss")
        ).otherwise(F.col("cur_end_time_ts"))
    ) \
    .withColumn("prev_end_time_ts",
        F.when(F.col("prev_start_time") > F.col("prev_end_time"),
               F.date_format(F.expr("cast(prev_end_time_ts as timestamp) + interval 1 day"), "yyyy-MM-dd HH:mm:ss")
        ).otherwise(F.col("prev_end_time_ts"))
    ) \
    .withColumn("is_week_off",
        F.when(
            (F.col("cur_start_time") == ZERO_TIME) & (F.col("cur_end_time") == ZERO_TIME),
            F.lit(True)
        ).otherwise(F.lit(False))
    ) \
    .withColumn("prev_is_week_off",
        F.when(
            (F.col("prev_start_time") == ZERO_TIME) & (F.col("prev_end_time") == ZERO_TIME),
            F.lit(True)
        ).otherwise(F.lit(False))
    ) \
    .select(
        "emp_id",
        "cal_date",
        "prev_date",
        "is_pulse_lite",
        "manager_id",
        "cur_start_time_ts",
        "cur_end_time_ts",
        "prev_start_time_ts",
        "prev_end_time_ts",
        "is_week_off",
        "prev_is_week_off"
    )

# -------------------------
# Step 7: Join with login data and window events
# -------------------------

# Get first and last login entry per day from login data
login_stats_df = login_data_df.groupBy("emp_id", "cal_date").agg(
    F.min("event_time").alias("first_login_time"),
    F.max("event_time").alias("last_login_time")
)

# Get window lock/unlock events
window_lock_df = window_events_df.filter(F.col("is_window_lock") == True) \
    .groupBy("emp_id", "cal_date").agg(
        F.min("start_time").alias("first_window_lock_time"),
        F.max("start_time").alias("last_window_lock_time")
    )

window_unlock_df = window_events_df.filter(F.col("is_window_unlock") == True) \
    .groupBy("emp_id", "cal_date").agg(
        F.min("start_time").alias("first_window_unlock_time"),
        F.max("start_time").alias("last_window_unlock_time")
    )

# Join all data sources
joined_df = final_df.join(
    login_stats_df,
    ["emp_id", "cal_date"],
    "left"
).join(
    window_lock_df,
    ["emp_id", "cal_date"],
    "left"
).join(
    window_unlock_df,
    ["emp_id", "cal_date"],
    "left"
)

# -------------------------
# Step 8: Calculate time windows for Pulse Lite
# -------------------------

jdf_with_windows = joined_df.withColumn(
    "current_window_start",
    F.when(
        (F.col("is_week_off") == True) & (F.col("prev_is_week_off") == False),
        F.greatest(
            F.expr("cast(prev_end_time_ts as timestamp) + interval 8 hours"),
            F.col("cur_start_time_ts")
        )
    ).when(
        F.col("is_week_off") == True,
        F.col("cur_start_time_ts")
    ).otherwise(
        F.expr("cast(cur_start_time_ts as timestamp) - interval 4 hours")
    )
).withColumn(
    "current_window_end",
    F.when(~F.col("is_week_off"), F.expr("cast(cur_end_time_ts as timestamp) + interval 8 hours"))
    .when(
        (F.col("is_week_off") == True) & (F.col("prev_is_week_off") == False),
        F.concat_ws(" ", F.col("cal_date"), F.lit("23:59:59"))
    )
    .otherwise(F.col("cur_end_time_ts"))
).withColumn(
    "prev_window_start",
    F.when(
        (F.col("prev_date") == F.date_sub(F.col("cal_date"), 1)) &
        ~F.col("prev_is_week_off"),
        F.col("prev_end_time_ts")
    ).otherwise(F.col("prev_start_time_ts"))
).withColumn(
    "prev_window_end",
    F.when(
        (F.col("prev_date") == F.date_sub(F.col("cal_date"), 1)) &
        ~F.col("prev_is_week_off"),
        F.expr("cast(prev_end_time_ts as timestamp) + interval 8 hours")
    ).otherwise(F.col("prev_end_time_ts"))
)

# -------------------------
# Step 9: Calculate login and logout times for Pulse Lite with refined logic
# -------------------------

# Define function to check if timestamp is within current window
def within_current_window(time_col):
    return (F.col(time_col) >= F.col("current_window_start")) & \
           (F.col(time_col) <= F.col("current_window_end"))

login_logout_df = jdf_with_windows.withColumn(
    "emp_login_time",
    # Priority 1: First login from login data within window
    F.when(
        within_current_window("first_login_time"),
        F.col("first_login_time")
    ).when(
        # Priority 2: First window unlock within window
        within_current_window("first_window_unlock_time"),
        F.col("first_window_unlock_time")
    ).when(
        # Priority 3: First window lock within window (as last resort)
        within_current_window("first_window_lock_time"),
        F.col("first_window_lock_time")
    )
).withColumn(
    "emp_logout_time",
    # Priority 1: Last window lock within window (latest logout)
    F.when(
        within_current_window("last_window_lock_time"),
        F.col("last_window_lock_time")
    ).when(
        # Priority 2: Last login data entry within window
        within_current_window("last_login_time"),
        F.col("last_login_time")
    ).when(
        # Priority 3: Last window unlock within window
        within_current_window("last_window_unlock_time"),
        F.col("last_window_unlock_time")
    ).when(
        # Priority 4: First window lock within window
        within_current_window("first_window_lock_time"),
        F.col("first_window_lock_time")
    ).when(
        # Priority 5: First login data entry within window
        within_current_window("first_login_time"),
        F.col("first_login_time")
    ).when(
        # Priority 6: First window unlock within window
        within_current_window("first_window_unlock_time"),
        F.col("first_window_unlock_time")
    )
)

# For cases where no event falls within current window, look for any event on that day
login_logout_df = login_logout_df.withColumn(
    "emp_login_time",
    F.coalesce(
        F.col("emp_login_time"),
        F.col("first_login_time"),
        F.col("first_window_unlock_time"),
        F.col("first_window_lock_time")
    )
).withColumn(
    "emp_logout_time",
    F.coalesce(
        F.col("emp_logout_time"),
        F.col("last_window_lock_time"),
        F.col("last_login_time"),
        F.col("last_window_unlock_time"),
        F.col("emp_login_time")  # Fallback to login time if nothing else
    )
)

# -------------------------
# Step 10: Handle previous day logout updates
# -------------------------

# Check if window unlock in current day should be considered as previous day logout
prev_day_updates = login_logout_df.filter(
    (F.col("first_window_unlock_time").isNotNull()) &
    (F.col("prev_date").isNotNull()) &
    (F.col("first_window_unlock_time") >= F.col("prev_window_start")) &
    (F.col("first_window_unlock_time") <= F.col("prev_window_end"))
).select(
    F.col("emp_id").alias("update_emp_id"),
    F.col("prev_date").alias("update_date"),
    F.col("first_window_unlock_time").alias("new_logout_time"),
    F.lit("window_unlock").alias("update_source")
)

# -------------------------
# Step 11: Prepare final output
# -------------------------

final_output = login_logout_df.select(
    "emp_id",
    "cal_date",
    "emp_login_time",
    "emp_logout_time",
    "cur_start_time_ts",
    "cur_end_time_ts",
    "is_week_off",
    "is_pulse_lite",
    "manager_id"
).filter(
    (F.col("emp_id").isNotNull()) &
    (F.col("cal_date").isNotNull()) &
    (F.col("emp_login_time").isNotNull()) &
    (F.col("emp_logout_time").isNotNull())
).withColumn(
    "emp_logout_time",
    F.when(
        F.col("emp_logout_time") < F.col("emp_login_time"),
        # If logout is before login, use login time as logout (edge case handling)
        F.col("emp_login_time")
    ).otherwise(F.col("emp_logout_time"))
).orderBy("emp_id", "cal_date")

# -------------------------
# Step 12: Create or update target table with manager_id
# -------------------------

# Create or update target table
spark.sql("""
CREATE TABLE IF NOT EXISTS gold_dashboard.analytics_emp_login_logout_pulse_lite (
    EMP_ID int,
    EMP_CONTRACTED_HOURS string,
    START_TIME string,
    END_TIME string,
    START_TIME_THRESHOLD string,
    END_TIME_THRESHOLD string,
    EMP_LOGIN_TIME string,
    EMP_LOGOUT_TIME string,
    SHIFT_DATE date,
    SHIFT_COMPLETED string,
    ATTENDENCE_STATUS string,
    LOGIN_STATUS string,
    LOGOUT_STATUS string,
    WORKING_HOURS float,
    MANAGER_ID int,
    UPDATED_ON TIMESTAMP,
    IS_WEEK_OFF BOOLEAN,
    IS_PULSE_LITE BOOLEAN,
    IS_HOLIDAY BOOLEAN
) USING DELTA
""")

spark.sql("""ALTER TABLE gold_dashboard.analytics_emp_login_logout_pulse_lite ALTER COLUMN UPDATED_ON SET DEFAULT current_timestamp()""")
spark.sql("""ALTER TABLE gold_dashboard.analytics_emp_login_logout_pulse_lite ALTER COLUMN IS_HOLIDAY SET DEFAULT FALSE""")
spark.sql("""ALTER TABLE gold_dashboard.analytics_emp_login_logout_pulse_lite ALTER COLUMN IS_PULSE_LITE SET DEFAULT TRUE""")

# Merge data into target table - Manager ID inserted only once, never updated
final_output.createOrReplaceTempView("temp_pulse_lite_login_logout")
spark.sql("""
MERGE INTO gold_dashboard.analytics_emp_login_logout_pulse_lite AS target
USING temp_pulse_lite_login_logout AS source
ON target.EMP_ID = source.emp_id AND target.SHIFT_DATE = source.cal_date
WHEN MATCHED THEN
    UPDATE SET 
        target.EMP_LOGIN_TIME = CASE 
            WHEN source.emp_login_time < target.EMP_LOGIN_TIME OR target.EMP_LOGIN_TIME IS NULL 
            THEN source.emp_login_time 
            ELSE target.EMP_LOGIN_TIME 
        END,
        target.EMP_LOGOUT_TIME = CASE
            WHEN source.emp_logout_time > target.EMP_LOGOUT_TIME OR target.EMP_LOGOUT_TIME IS NULL
            THEN source.emp_logout_time
            ELSE target.EMP_LOGOUT_TIME
        END,
        target.START_TIME = CASE
            WHEN target.START_TIME IS NULL THEN source.cur_start_time_ts
            ELSE target.START_TIME
        END,
        target.END_TIME = CASE
            WHEN target.END_TIME IS NULL THEN source.cur_end_time_ts
            ELSE target.END_TIME
        END,
        target.IS_WEEK_OFF = CASE
            WHEN target.IS_WEEK_OFF IS NULL THEN source.is_week_off
            ELSE target.IS_WEEK_OFF
        END
        -- Note: MANAGER_ID is NOT updated here - it's set only on insert
WHEN NOT MATCHED THEN
    INSERT (EMP_ID, START_TIME, END_TIME, EMP_LOGIN_TIME, EMP_LOGOUT_TIME, 
            SHIFT_DATE, IS_WEEK_OFF, IS_PULSE_LITE, MANAGER_ID)
    VALUES (source.emp_id, source.cur_start_time_ts, source.cur_end_time_ts,
            source.emp_login_time, source.emp_logout_time, source.cal_date,
            source.is_week_off, source.is_pulse_lite, source.manager_id)
""")

# Update previous day's logout times if needed (only update logout time, not manager_id)
prev_day_updates.createOrReplaceTempView("temp_prev_day_updates_lite")
spark.sql("""
MERGE INTO gold_dashboard.analytics_emp_login_logout_pulse_lite AS target
USING temp_prev_day_updates_lite AS source
ON target.EMP_ID = source.update_emp_id AND target.SHIFT_DATE = source.update_date
WHEN MATCHED AND 
    (target.EMP_LOGOUT_TIME IS NULL OR source.new_logout_time > target.EMP_LOGOUT_TIME) THEN 
    UPDATE SET target.EMP_LOGOUT_TIME = source.new_logout_time
""")

print(f"Processed {final_output.count()} Pulse Lite login/logout records")




# day_emp_pipeline_udf.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType
import json
from datetime import datetime, timedelta

def run_day_emp_pipeline_udf():
    """Day Employee Alerts Pipeline using UDF"""
    
    spark = SparkSession.builder.appName("DayEmpPipelineUDF").getOrCreate()
    
    # Check if day_emp needs processing
    four_hours_ago = (datetime.now() - timedelta(hours=4)).strftime('%Y-%m-%d %H:%M:%S')
    
    dates_df = spark.sql(f"""
        SELECT DISTINCT shift_date
        FROM day_emp 
        WHERE updated_on > '{four_hours_ago}'
        AND shift_date IS NOT NULL
        ORDER BY shift_date
    """)
    
    dates_to_process = dates_df.collect()
    
    if not dates_to_process:
        print("No day_emp records updated in last 4 hours. Skipping.")
        spark.stop()
        return {"status": "skipped"}
    
    results = {}
    
    # Define UDF to create alerts JSON
    def create_alerts_json(active_time, idle_time, lock_time, expected_shift_time, 
                          expected_break_time, active_percent):
        """Create JSON with triggered alerts"""
        triggered_alerts = []
        
        # Calculate each condition
        if (idle_time + lock_time) < expected_break_time:
            triggered_alerts.append(1)
        if active_time < expected_shift_time:
            triggered_alerts.append(2)
        if active_time > expected_shift_time:
            triggered_alerts.append(3)
        if active_time < (0.9 * expected_shift_time) and active_percent < 90:
            triggered_alerts.append(4)
        if active_time < (0.9 * expected_shift_time) and active_percent > 90:
            triggered_alerts.append(5)
        if active_time > (1.1 * expected_shift_time) and idle_time > 1.0:
            triggered_alerts.append(6)
        if ((idle_time + lock_time) < expected_break_time and 
            active_time < (0.9 * expected_shift_time) and 
            active_percent < 90):
            triggered_alerts.append(7)
        if ((idle_time + lock_time) < expected_break_time and 
            active_time > (1.1 * expected_shift_time) and 
            idle_time > 1.0):
            triggered_alerts.append(8)
        if ((idle_time + lock_time) < expected_break_time and 
            active_time < (0.9 * expected_shift_time) and 
            active_percent > 90):
            triggered_alerts.append(9)
        if (active_time < expected_shift_time and 
            active_time < (0.9 * expected_shift_time) and 
            active_percent < 90):
            triggered_alerts.append(10)
        if (active_time < expected_shift_time and 
            active_time < (0.9 * expected_shift_time) and 
            active_percent > 90):
            triggered_alerts.append(11)
        if (active_time > (1.1 * expected_shift_time) and 
            idle_time > 1.0 and 
            active_time > expected_shift_time):
            triggered_alerts.append(12)
        if ((idle_time + lock_time) < expected_break_time and 
            active_time < expected_shift_time):
            triggered_alerts.append(13)
        if ((idle_time + lock_time) < expected_break_time and 
            active_time > expected_shift_time):
            triggered_alerts.append(14)
        
        # Create JSON
        alerts_json = {
            "triggered_alerts": triggered_alerts,
            "summary": {
                "total_alerts": len(triggered_alerts),
                "calculated_at": datetime.now().isoformat()
            }
        }
        
        return json.dumps(alerts_json)
    
    create_alerts_json_udf = udf(create_alerts_json, StringType())
    
    for date_row in dates_to_process:
        shift_date = date_row.shift_date.strftime('%Y-%m-%d')
        
        print(f"Processing day: {shift_date}")
        
        # Get data for this date
        day_data_df = spark.sql(f"""
            SELECT 
                de.emp_id,
                de.shift_date,
                de.manager_id,
                de.active_time,
                de.idle_time,
                de.lock_time,
                de.expected_shift_time,
                de.expected_break_time,
                -- Calculate active percentage
                CASE 
                    WHEN (de.active_time + de.idle_time + de.lock_time) > 0 
                    THEN (de.active_time * 100.0 / (de.active_time + de.idle_time + de.lock_time))
                    ELSE 0 
                END as active_percent
            FROM day_emp de
            WHERE de.shift_date = '{shift_date}'
            AND de.updated_on > '{four_hours_ago}'
        """)
        
        # Apply UDF to create alerts JSON
        from pyspark.sql.functions import lit, current_timestamp
        
        alerts_df = day_data_df.withColumn(
            "alerts_json",
            create_alerts_json_udf(
                col("active_time"),
                col("idle_time"),
                col("lock_time"),
                col("expected_shift_time"),
                col("expected_break_time"),
                col("active_percent")
            )
        ).withColumn(
            "calculated_at",
            current_timestamp()
        ).filter(
            "alerts_json LIKE '%\"triggered_alerts\":[%' AND alerts_json NOT LIKE '%\"triggered_alerts\":[]%'"
        )
        
        # Create temp view
        alerts_df.createOrReplaceTempView(f"day_emp_results_{shift_date.replace('-', '_')}")
        
        # Insert/Update day_alerts table
        spark.sql(f"""
            MERGE INTO day_alerts target
            USING day_emp_results_{shift_date.replace('-', '_')} source
            ON target.emp_id = source.emp_id AND target.shift_date = source.shift_date
            WHEN MATCHED THEN
                UPDATE SET 
                    target.alerts_json = source.alerts_json,
                    target.updated_on = source.calculated_at
            WHEN NOT MATCHED THEN
                INSERT (emp_id, shift_date, alerts_json, updated_on)
                VALUES (source.emp_id, source.shift_date, source.alerts_json, source.calculated_at)
        """)
        
        count = alerts_df.count()
        results[shift_date] = count
        
        print(f"  → Processed {count} records")
    
    spark.stop()
    
    total = sum(results.values())
    print(f"\n✓ Day Employee Pipeline completed: {total} records across {len(results)} dates")
    return {"status": "success", "processed": total, "by_date": results}





ACTUAL RESPONSES IN EACH TABLE'S ALERTS_JSON COLUMN

1. day_alerts Table

```json
{
  "triggered_alerts": [1, 4, 7, 13],
  "summary": {
    "total_alerts": 4,
    "calculated_at": "2024-01-15T14:30:54Z"
  }
}
```

Actual database row:

```sql
-- emp_id: "E001"
-- shift_date: "2024-01-15"
-- alerts_json: '{"triggered_alerts":[1,4,7,13],"summary":{"total_alerts":4,"calculated_at":"2024-01-15T14:30:54Z"}}'
-- updated_on: "2024-01-15 14:30:54"
```

2. week_alerts Table

```json
{
  "alert_patterns": [
    {
      "alert": 1,
      "severity": "HIGH",
      "affected_days": 3,
      "consecutive_days": 3
    },
    {
      "alert": 4,
      "severity": "LOW",
      "affected_days": 1,
      "consecutive_days": 1
    }
  ],
  "summary": {
    "total_alerts": 2,
    "calculated_at": "2024-01-15T14:40:15Z"
  }
}
```

Actual database row:

```sql
-- emp_id: "E001"
-- week_start_date: "2024-01-08"
-- alerts_json: '{"alert_patterns":[{"alert":1,"severity":"HIGH","affected_days":3,"consecutive_days":3},{"alert":4,"severity":"LOW","affected_days":1,"consecutive_days":1}],"summary":{"total_alerts":2,"calculated_at":"2024-01-15T14:40:15Z"}}'
-- updated_on: "2024-01-15 14:40:15"
```

3. month_alerts Table

```json
{
  "month_patterns": [
    {
      "alert": 1,
      "severity": "MEDIUM",
      "total_days": 12,
      "weekly_patterns": [
        {
          "week": "2024-01-01",
          "affected_days": 3,
          "consecutive_days": 3
        },
        {
          "week": "2024-01-08",
          "affected_days": 4,
          "consecutive_days": 2
        }
      ]
    }
  ],
  "summary": {
    "total_alerts": 1,
    "calculated_at": "2024-01-31T23:59:59Z"
  }
}
```

Actual database row:

```sql
-- emp_id: "E001"
-- month_start_date: "2024-01-01"
-- alerts_json: '{"month_patterns":[{"alert":1,"severity":"MEDIUM","total_days":12,"weekly_patterns":[{"week":"2024-01-01","affected_days":3,"consecutive_days":3},{"week":"2024-01-08","affected_days":4,"consecutive_days":2}]}],"summary":{"total_alerts":1,"calculated_at":"2024-01-31T23:59:59Z"}}'
-- updated_on: "2024-01-31 23:59:59"
```

4. day_mgr_alerts Table

```json
{
  "team_alerts": [
    {
      "alert": 1,
      "severity": "HIGH",
      "affected_employees": 18,
      "team_size": 25,
      "percentage": 72.0
    },
    {
      "alert": 4,
      "severity": "MEDIUM",
      "affected_employees": 12,
      "team_size": 25,
      "percentage": 48.0
    }
  ],
  "summary": {
    "total_alerts": 2,
    "calculated_at": "2024-01-15T14:35:22Z"
  }
}
```

Actual database row:

```sql
-- manager_id: "M001"
-- shift_date: "2024-01-15"
-- team_size: 25
-- alerts_json: '{"team_alerts":[{"alert":1,"severity":"HIGH","affected_employees":18,"team_size":25,"percentage":72.0},{"alert":4,"severity":"MEDIUM","affected_employees":12,"team_size":25,"percentage":48.0}],"summary":{"total_alerts":2,"calculated_at":"2024-01-15T14:35:22Z"}}'
-- updated_on: "2024-01-15 14:35:22"
```

5. week_mgr_alerts Table

```json
{
  "team_patterns": [
    {
      "alert": 1,
      "severity": "HIGH",
      "max_daily_percent": 72.0,
      "days_above_20": 3,
      "employees_3_consecutive": 2
    }
  ],
  "summary": {
    "total_alerts": 1,
    "calculated_at": "2024-01-15T14:45:30Z"
  }
}
```

Actual database row:

```sql
-- manager_id: "M001"
-- week_start_date: "2024-01-08"
-- team_size: 25
-- alerts_json: '{"team_patterns":[{"alert":1,"severity":"HIGH","max_daily_percent":72.0,"days_above_20":3,"employees_3_consecutive":2}],"summary":{"total_alerts":1,"calculated_at":"2024-01-15T14:45:30Z"}}'
-- updated_on: "2024-01-15 14:45:30"
```

6. month_mgr_alerts Table

```json
{
  "month_team_patterns": [
    {
      "alert": 1,
      "severity": "HIGH",
      "metrics": {
        "team_affected_days": 18,
        "team_size": 25,
        "percentage_affected": 72.0,
        "individuals_with_3_consecutive": 3
      }
    }
  ],
  "summary": {
    "total_alerts": 1,
    "calculated_at": "2024-01-31T23:59:59Z"
  }
}
```

Actual database row:

```sql
-- manager_id: "M001"
-- month_start_date: "2024-01-01"
-- team_size: 25
-- alerts_json: '{"month_team_patterns":[{"alert":1,"severity":"HIGH","metrics":{"team_affected_days":18,"team_size":25,"percentage_affected":72.0,"individuals_with_3_consecutive":3}}],"summary":{"total_alerts":1,"calculated_at":"2024-01-31T23:59:59Z"}}'
-- updated_on: "2024-01-31 23:59:59"
```

---

COMPLETE TABLE SCHEMA WITH SAMPLE DATA

```sql
-- 1. day_alerts table
CREATE TABLE day_alerts (
    emp_id VARCHAR(50),
    shift_date DATE,
    alerts_json STRING,
    updated_on TIMESTAMP
);

INSERT INTO day_alerts VALUES 
('E001', '2024-01-15', 
 '{"triggered_alerts":[1,4,7,13],"summary":{"total_alerts":4,"calculated_at":"2024-01-15T14:30:54Z"}}',
 '2024-01-15 14:30:54'),

('E002', '2024-01-15', 
 '{"triggered_alerts":[2,5],"summary":{"total_alerts":2,"calculated_at":"2024-01-15T14:30:54Z"}}',
 '2024-01-15 14:30:54'),

('E003', '2024-01-15', 
 '{"triggered_alerts":[],"summary":{"total_alerts":0,"calculated_at":"2024-01-15T14:30:54Z"}}',
 '2024-01-15 14:30:54');

-- 2. week_alerts table  
CREATE TABLE week_alerts (
    emp_id VARCHAR(50),
    week_start_date DATE,
    alerts_json STRING,
    updated_on TIMESTAMP
);

INSERT INTO week_alerts VALUES
('E001', '2024-01-08',
 '{"alert_patterns":[{"alert":1,"severity":"HIGH","affected_days":3,"consecutive_days":3},{"alert":4,"severity":"LOW","affected_days":1,"consecutive_days":1}],"summary":{"total_alerts":2,"calculated_at":"2024-01-15T14:40:15Z"}}',
 '2024-01-15 14:40:15'),

('E002', '2024-01-08',
 '{"alert_patterns":[{"alert":2,"severity":"MEDIUM","affected_days":2,"consecutive_days":2}],"summary":{"total_alerts":1,"calculated_at":"2024-01-15T14:40:15Z"}}',
 '2024-01-15 14:40:15');

-- 3. month_alerts table
CREATE TABLE month_alerts (
    emp_id VARCHAR(50),
    month_start_date DATE,
    alerts_json STRING,
    updated_on TIMESTAMP
);

INSERT INTO month_alerts VALUES
('E001', '2024-01-01',
 '{"month_patterns":[{"alert":1,"severity":"MEDIUM","total_days":12,"weekly_patterns":[{"week":"2024-01-01","affected_days":3,"consecutive_days":3},{"week":"2024-01-08","affected_days":4,"consecutive_days":2}]}],"summary":{"total_alerts":1,"calculated_at":"2024-01-31T23:59:59Z"}}',
 '2024-01-31 23:59:59');

-- 4. day_mgr_alerts table
CREATE TABLE day_mgr_alerts (
    manager_id VARCHAR(50),
    shift_date DATE,
    team_size INT,
    alerts_json STRING,
    updated_on TIMESTAMP
);

INSERT INTO day_mgr_alerts VALUES
('M001', '2024-01-15', 25,
 '{"team_alerts":[{"alert":1,"severity":"HIGH","affected_employees":18,"team_size":25,"percentage":72.0},{"alert":4,"severity":"MEDIUM","affected_employees":12,"team_size":25,"percentage":48.0}],"summary":{"total_alerts":2,"calculated_at":"2024-01-15T14:35:22Z"}}',
 '2024-01-15 14:35:22'),

('M002', '2024-01-15', 30,
 '{"team_alerts":[{"alert":2,"severity":"LOW","affected_employees":8,"team_size":30,"percentage":26.7}],"summary":{"total_alerts":1,"calculated_at":"2024-01-15T14:35:22Z"}}',
 '2024-01-15 14:35:22');

-- 5. week_mgr_alerts table
CREATE TABLE week_mgr_alerts (
    manager_id VARCHAR(50),
    week_start_date DATE,
    team_size INT,
    alerts_json STRING,
    updated_on TIMESTAMP
);

INSERT INTO week_mgr_alerts VALUES
('M001', '2024-01-08', 25,
 '{"team_patterns":[{"alert":1,"severity":"HIGH","max_daily_percent":72.0,"days_above_20":3,"employees_3_consecutive":2}],"summary":{"total_alerts":1,"calculated_at":"2024-01-15T14:45:30Z"}}',
 '2024-01-15 14:45:30');

-- 6. month_mgr_alerts table
CREATE TABLE month_mgr_alerts (
    manager_id VARCHAR(50),
    month_start_date DATE,
    team_size INT,
    alerts_json STRING,
    updated_on TIMESTAMP
);

INSERT INTO month_mgr_alerts VALUES
('M001', '2024-01-01', 25,
 '{"month_team_patterns":[{"alert":1,"severity":"HIGH","metrics":{"team_affected_days":18,"team_size":25,"percentage_affected":72.0,"individuals_with_3_consecutive":3}}],"summary":{"total_alerts":1,"calculated_at":"2024-01-31T23:59:59Z"}}',
 '2024-01-31 23:59:59');
```

---

HOW TO QUERY THESE JSON FIELDS

1. Check if employee has specific alert on a day

```sql
-- Check if employee E001 had alert 1 on Jan 15
SELECT emp_id
FROM day_alerts 
WHERE emp_id = 'E001'
AND shift_date = '2024-01-15'
AND JSON_EXTRACT(alerts_json, '$.triggered_alerts') LIKE '%1%';

-- Or using JSON functions
SELECT emp_id
FROM day_alerts 
WHERE emp_id = 'E001'
AND shift_date = '2024-01-15'
AND EXISTS (
  SELECT 1
  FROM JSON_TABLE(
    JSON_EXTRACT(alerts_json, '$.triggered_alerts'),
    '$[*]' COLUMNS (alert INT PATH '$')
  ) AS alerts
  WHERE alert = 1
);
```

2. Get all managers with HIGH severity alerts today

```sql
SELECT manager_id, shift_date, alerts_json
FROM day_mgr_alerts
WHERE shift_date = '2024-01-15'
AND JSON_EXTRACT(alerts_json, '$.team_alerts[*].severity') LIKE '%HIGH%';
```

3. Count alerts by type for a week

```sql
SELECT 
  alert_id,
  COUNT(*) as employee_count
FROM (
  SELECT 
    emp_id,
    JSON_EXTRACT(pattern, '$.alert') as alert_id
  FROM week_alerts
  LATERAL VIEW EXPLODE(
    FROM_JSON(
      JSON_EXTRACT(alerts_json, '$.alert_patterns'),
      'array<struct<alert:int,severity:string,affected_days:int,consecutive_days:int>>'
    )
  ) AS pattern
  WHERE week_start_date = '2024-01-08'
) t
GROUP BY alert_id
ORDER BY employee_count DESC;
```

4. Get manager summary for UI

```sql
SELECT 
  manager_id,
  week_start_date,
  JSON_EXTRACT(alerts_json, '$.summary.total_alerts') as total_alerts,
  JSON_EXTRACT(alerts_json, '$.team_patterns') as patterns
FROM week_mgr_alerts
WHERE week_start_date = '2024-01-08'
ORDER BY total_alerts DESC;
```

---

REAL-WORLD EXAMPLE OUTPUTS

For Employee Dashboard:

```json
{
  "employee_id": "E001",
  "today_alerts": [1, 4, 7, 13],
  "week_summary": {
    "patterns": [
      {"alert": 1, "severity": "HIGH", "affected_days": 3},
      {"alert": 4, "severity": "LOW", "affected_days": 1}
    ]
  },
  "month_summary": {
    "patterns": [
      {"alert": 1, "severity": "MEDIUM", "total_days": 12}
    ]
  }
}
```

For Manager Dashboard:

```json
{
  "manager_id": "M001",
  "today_summary": {
    "team_alerts": [
      {"alert": 1, "severity": "HIGH", "affected_percentage": 72.0},
      {"alert": 4, "severity": "MEDIUM", "affected_percentage": 48.0}
    ]
  },
  "week_summary": {
    "team_patterns": [
      {"alert": 1, "severity": "HIGH", "max_daily_percent": 72.0}
    ]
  },
  "month_summary": {
    "team_patterns": [
      {"alert": 1, "severity": "HIGH", "percentage_affected": 72.0}
    ]
  }
}
```

These are the actual, real JSON responses stored in each table's alerts_json column! Clean, consistent, and easy to query.



COMPLETE CODE FOR ALL 6 PIPELINES

1. Day Employee Alerts Pipeline

```python
# day_emp_pipeline.py
from pyspark.sql import SparkSession
from datetime import datetime, timedelta

def run_day_emp_pipeline():
    """Day Employee Alerts Pipeline - Simple array format"""
    
    spark = SparkSession.builder.appName("DayEmpPipeline").getOrCreate()
    
    # Check if day_emp needs processing (updated in last 4 hours)
    four_hours_ago = (datetime.now() - timedelta(hours=4)).strftime('%Y-%m-%d %H:%M:%S')
    
    # Get dates with recent updates
    dates_to_process = spark.sql(f"""
        SELECT DISTINCT shift_date
        FROM day_emp 
        WHERE updated_on > '{four_hours_ago}'
        AND shift_date IS NOT NULL
        ORDER BY shift_date
    """).collect()
    
    if not dates_to_process:
        print("No day_emp records updated in last 4 hours. Skipping.")
        spark.stop()
        return {"status": "skipped"}
    
    results = {}
    
    for date_row in dates_to_process:
        shift_date = date_row.shift_date.strftime('%Y-%m-%d')
        
        print(f"Processing day: {shift_date}")
        
        # Calculate alerts for this date
        spark.sql(f"""
            WITH date_data AS (
                SELECT 
                    de.emp_id,
                    de.shift_date,
                    de.manager_id,
                    de.active_time,
                    de.idle_time,
                    de.lock_time,
                    de.expected_shift_time,
                    de.expected_break_time,
                    -- Calculate active percentage
                    CASE 
                        WHEN (de.active_time + de.idle_time + de.lock_time) > 0 
                        THEN (de.active_time * 100.0 / (de.active_time + de.idle_time + de.lock_time))
                        ELSE 0 
                    END as active_percent
                FROM day_emp de
                WHERE de.shift_date = '{shift_date}'
                AND de.updated_on > '{four_hours_ago}'
            ),
            alerts_calc AS (
                SELECT 
                    emp_id,
                    shift_date,
                    manager_id,
                    -- All 14 alerts as simple array
                    ARRAY_REMOVE(ARRAY(
                        -- Alert 1: Few Breaks
                        CASE WHEN (idle_time + lock_time) < expected_break_time THEN 1 ELSE NULL END,
                        
                        -- Alert 2: Low Active Time
                        CASE WHEN active_time < expected_shift_time THEN 2 ELSE NULL END,
                        
                        -- Alert 3: High Active Time
                        CASE WHEN active_time > expected_shift_time THEN 3 ELSE NULL END,
                        
                        -- Alert 4: Low Active & High Idle Time
                        CASE WHEN active_time < (0.9 * expected_shift_time) 
                             AND active_percent < 90 THEN 4 ELSE NULL END,
                        
                        -- Alert 5: Low Active & Low Idle Time
                        CASE WHEN active_time < (0.9 * expected_shift_time) 
                             AND active_percent > 90 THEN 5 ELSE NULL END,
                        
                        -- Alert 6: High Active & High Idle Time
                        CASE WHEN active_time > (1.1 * expected_shift_time) 
                             AND idle_time > 1.0 THEN 6 ELSE NULL END,
                        
                        -- Alert 7: Few Breaks + Low Active & High Idle Time
                        CASE WHEN (idle_time + lock_time) < expected_break_time
                             AND active_time < (0.9 * expected_shift_time)
                             AND active_percent < 90 THEN 7 ELSE NULL END,
                        
                        -- Alert 8: Few Breaks + High Active & High Idle Time
                        CASE WHEN (idle_time + lock_time) < expected_break_time
                             AND active_time > (1.1 * expected_shift_time)
                             AND idle_time > 1.0 THEN 8 ELSE NULL END,
                        
                        -- Alert 9: Few Breaks + Low Active & Low Idle Time
                        CASE WHEN (idle_time + lock_time) < expected_break_time
                             AND active_time < (0.9 * expected_shift_time)
                             AND active_percent > 90 THEN 9 ELSE NULL END,
                        
                        -- Alert 10: Low Active + High Idle Time
                        CASE WHEN active_time < expected_shift_time
                             AND active_time < (0.9 * expected_shift_time)
                             AND active_percent < 90 THEN 10 ELSE NULL END,
                        
                        -- Alert 11: Low Active + Low Idle Time
                        CASE WHEN active_time < expected_shift_time
                             AND active_time < (0.9 * expected_shift_time)
                             AND active_percent > 90 THEN 11 ELSE NULL END,
                        
                        -- Alert 12: High Active + High Idle Time
                        CASE WHEN active_time > (1.1 * expected_shift_time)
                             AND idle_time > 1.0
                             AND active_time > expected_shift_time THEN 12 ELSE NULL END,
                        
                        -- Alert 13: Low Active + Few Breaks
                        CASE WHEN (idle_time + lock_time) < expected_break_time
                             AND active_time < expected_shift_time THEN 13 ELSE NULL END,
                        
                        -- Alert 14: High Active + Few Breaks
                        CASE WHEN (idle_time + lock_time) < expected_break_time
                             AND active_time > expected_shift_time THEN 14 ELSE NULL END
                    ), NULL) as alert_ids
                FROM date_data
            )
            SELECT 
                emp_id,
                shift_date,
                manager_id,
                -- Build simple JSON with array
                CONCAT(
                    '{',
                    '"triggered_alerts":', 
                    CASE 
                        WHEN SIZE(alert_ids) > 0 THEN 
                            CONCAT('[', ARRAY_JOIN(alert_ids, ','), ']')
                        ELSE '[]'
                    END, ',',
                    '"summary":{',
                    '"total_alerts":', SIZE(alert_ids), ',',
                    '"calculated_at":"', CURRENT_TIMESTAMP(), '"',
                    '}',
                    '}'
                ) as alerts_json,
                CURRENT_TIMESTAMP() as calculated_at
            FROM alerts_calc
            WHERE SIZE(alert_ids) > 0
        """).createOrReplaceTempView(f"day_emp_results_{shift_date.replace('-', '_')}")
        
        # Insert/Update day_alerts table
        spark.sql(f"""
            MERGE INTO day_alerts target
            USING day_emp_results_{shift_date.replace('-', '_')} source
            ON target.emp_id = source.emp_id AND target.shift_date = source.shift_date
            WHEN MATCHED THEN
                UPDATE SET 
                    target.alerts_json = source.alerts_json,
                    target.updated_on = source.calculated_at
            WHEN NOT MATCHED THEN
                INSERT (emp_id, shift_date, alerts_json, updated_on)
                VALUES (source.emp_id, source.shift_date, source.alerts_json, source.calculated_at)
        """)
        
        count = spark.sql(f"SELECT COUNT(*) as cnt FROM day_emp_results_{shift_date.replace('-', '_')}").first().cnt
        results[shift_date] = count
        
        print(f"  → Processed {count} records")
    
    spark.stop()
    
    total = sum(results.values())
    print(f"\n✓ Day Employee Pipeline completed: {total} records across {len(results)} dates")
    return {"status": "success", "processed": total, "by_date": results}
```

2. Day Manager Alerts Pipeline

```python
# day_mgr_pipeline.py
def run_day_mgr_pipeline():
    """Day Manager Alerts Pipeline - From day_alerts"""
    
    spark = SparkSession.builder.appName("DayMgrPipeline").getOrCreate()
    
    # Check if day_alerts needs processing
    four_hours_ago = (datetime.now() - timedelta(hours=4)).strftime('%Y-%m-%d %H:%M:%S')
    
    # Get dates with recent day_alerts updates
    dates_to_process = spark.sql(f"""
        SELECT DISTINCT shift_date
        FROM day_alerts 
        WHERE updated_on > '{four_hours_ago}'
        AND shift_date IS NOT NULL
        ORDER BY shift_date
    """).collect()
    
    if not dates_to_process:
        print("No day_alerts updated in last 4 hours. Skipping.")
        spark.stop()
        return {"status": "skipped"}
    
    results = {}
    
    for date_row in dates_to_process:
        shift_date = date_row.shift_date.strftime('%Y-%m-%d')
        
        print(f"Processing day manager alerts for: {shift_date}")
        
        # Calculate team alerts for this date
        spark.sql(f"""
            WITH day_alert_data AS (
                SELECT 
                    de.emp_id,
                    de.manager_id,
                    FROM_JSON(
                        GET_JSON_OBJECT(da.alerts_json, '$.triggered_alerts'),
                        'array<int>'
                    ) as alert_ids
                FROM day_alerts da
                JOIN day_emp de ON da.emp_id = de.emp_id AND da.shift_date = de.shift_date
                WHERE da.shift_date = '{shift_date}'
                AND da.updated_on > '{four_hours_ago}'
                AND de.manager_id IS NOT NULL
            ),
            exploded_alerts AS (
                SELECT 
                    manager_id,
                    alert_id
                FROM day_alert_data
                LATERAL VIEW EXPLODE(alert_ids) t AS alert_id
            ),
            team_sizes AS (
                SELECT 
                    manager_id,
                    COUNT(DISTINCT emp_id) as team_size
                FROM day_emp 
                WHERE shift_date = '{shift_date}'
                AND manager_id IS NOT NULL
                GROUP BY manager_id
            ),
            alert_counts AS (
                SELECT 
                    ea.manager_id,
                    ea.alert_id,
                    COUNT(DISTINCT dad.emp_id) as affected_count,
                    ts.team_size
                FROM exploded_alerts ea
                JOIN day_alert_data dad ON ea.manager_id = dad.manager_id 
                    AND ea.alert_id IN dad.alert_ids
                JOIN team_sizes ts ON ea.manager_id = ts.manager_id
                GROUP BY ea.manager_id, ea.alert_id, ts.team_size
            )
            SELECT 
                manager_id,
                '{shift_date}' as shift_date,
                team_size,
                -- Build team alerts JSON
                CONCAT(
                    '{',
                    '"team_alerts":[',
                    ARRAY_JOIN(
                        COLLECT_LIST(CONCAT(
                            '{',
                            '"alert":', alert_id, ',',
                            '"severity":"',
                            CASE 
                                WHEN (affected_count * 100.0 / team_size) >= 60 THEN 'HIGH'
                                WHEN (affected_count * 100.0 / team_size) >= 40 THEN 'MEDIUM'
                                WHEN (affected_count * 100.0 / team_size) >= 20 THEN 'LOW'
                                ELSE 'NONE'
                            END, '",',
                            '"affected_employees":', affected_count, ',',
                            '"team_size":', team_size, ',',
                            '"percentage":', ROUND((affected_count * 100.0 / team_size), 1),
                            '}'
                        )),
                        ','
                    ),
                    '],',
                    '"summary":{',
                    '"total_alerts":', COUNT(*), ',',
                    '"calculated_at":"', CURRENT_TIMESTAMP(), '"',
                    '}',
                    '}'
                ) as alerts_json,
                CURRENT_TIMESTAMP() as calculated_at
            FROM alert_counts
            WHERE (affected_count * 100.0 / team_size) >= 20
            GROUP BY manager_id, team_size
        """).createOrReplaceTempView(f"day_mgr_results_{shift_date.replace('-', '_')}")
        
        # Insert/Update day_mgr_alerts table
        spark.sql(f"""
            MERGE INTO day_mgr_alerts target
            USING day_mgr_results_{shift_date.replace('-', '_')} source
            ON target.manager_id = source.manager_id AND target.shift_date = source.shift_date
            WHEN MATCHED THEN
                UPDATE SET 
                    target.alerts_json = source.alerts_json,
                    target.team_size = source.team_size,
                    target.updated_on = source.calculated_at
            WHEN NOT MATCHED THEN
                INSERT (manager_id, shift_date, team_size, alerts_json, updated_on)
                VALUES (source.manager_id, source.shift_date, source.team_size, 
                       source.alerts_json, source.calculated_at)
        """)
        
        count = spark.sql(f"SELECT COUNT(*) as cnt FROM day_mgr_results_{shift_date.replace('-', '_')}").first().cnt
        results[shift_date] = count
        
        print(f"  → Processed {count} manager records")
    
    spark.stop()
    
    total = sum(results.values())
    print(f"\n✓ Day Manager Pipeline completed: {total} records across {len(results)} dates")
    return {"status": "success", "processed": total, "by_date": results}
```

3. Week Employee Alerts Pipeline

```python
# week_emp_pipeline.py
def run_week_emp_pipeline():
    """Week Employee Alerts Pipeline - Patterns from day_alerts"""
    
    spark = SparkSession.builder.appName("WeekEmpPipeline").getOrCreate()
    
    # Check if week_alerts needs processing
    four_hours_ago = (datetime.now() - timedelta(hours=4)).strftime('%Y-%m-%d %H:%M:%S')
    
    # Get weeks with recent updates
    weeks_to_process = spark.sql(f"""
        SELECT DISTINCT week_start_date
        FROM week_alerts 
        WHERE updated_on > '{four_hours_ago}'
        ORDER BY week_start_date
    """).collect()
    
    if not weeks_to_process:
        print("No week_alerts updated in last 4 hours. Skipping.")
        spark.stop()
        return {"status": "skipped"}
    
    results = {}
    yesterday = (datetime.now() - timedelta(days=1)).date()
    
    for week_row in weeks_to_process:
        week_start = week_row.week_start_date
        week_start_str = week_start.strftime('%Y-%m-%d')
        
        # Week end: min(week_start + 6 days, yesterday)
        week_end_date = min(week_start + timedelta(days=6), yesterday)
        week_end_str = week_end_date.strftime('%Y-%m-%d')
        
        print(f"Processing week: {week_start_str} to {week_end_str}")
        
        # Get employees with recent week_alerts updates
        spark.sql(f"""
            SELECT DISTINCT emp_id
            FROM week_alerts 
            WHERE week_start_date = '{week_start_str}'
            AND updated_on > '{four_hours_ago}'
        """).createOrReplaceTempView(f"week_emps_{week_start_str.replace('-', '_')}")
        
        # Calculate week patterns
        spark.sql(f"""
            WITH week_data AS (
                SELECT 
                    da.emp_id,
                    da.shift_date,
                    FROM_JSON(
                        GET_JSON_OBJECT(da.alerts_json, '$.triggered_alerts'),
                        'array<int>'
                    ) as alert_ids
                FROM day_alerts da
                JOIN week_emps_{week_start_str.replace('-', '_')} we ON da.emp_id = we.emp_id
                WHERE da.shift_date BETWEEN '{week_start_str}' AND '{week_end_str}'
                AND da.shift_date <= '{yesterday.strftime('%Y-%m-%d')}'
            ),
            exploded_data AS (
                SELECT 
                    emp_id,
                    shift_date,
                    alert_id
                FROM week_data
                LATERAL VIEW EXPLODE(alert_ids) t AS alert_id
            ),
            patterns AS (
                SELECT 
                    emp_id,
                    alert_id,
                    COUNT(DISTINCT shift_date) as affected_days,
                    MAX(consecutive_count) as max_consecutive
                FROM (
                    SELECT 
                        emp_id,
                        alert_id,
                        shift_date,
                        COUNT(*) OVER (
                            PARTITION BY emp_id, alert_id, grp
                            ORDER BY shift_date
                        ) as consecutive_count
                    FROM (
                        SELECT 
                            emp_id,
                            alert_id,
                            shift_date,
                            DATE_SUB(shift_date, 
                                ROW_NUMBER() OVER (
                                    PARTITION BY emp_id, alert_id
                                    ORDER BY shift_date
                                )
                            ) as grp
                        FROM exploded_data
                    ) t
                ) t
                GROUP BY emp_id, alert_id
            )
            SELECT 
                emp_id,
                '{week_start_str}' as week_start_date,
                -- Build week patterns JSON
                CONCAT(
                    '{',
                    '"alert_patterns":[',
                    ARRAY_JOIN(
                        COLLECT_LIST(CONCAT(
                            '{',
                            '"alert":', alert_id, ',',
                            '"severity":"',
                            CASE 
                                WHEN max_consecutive >= 3 THEN 'HIGH'
                                WHEN max_consecutive >= 2 THEN 'MEDIUM'
                                WHEN affected_days >= 1 THEN 'LOW'
                                ELSE 'NONE'
                            END, '",',
                            '"affected_days":', affected_days, ',',
                            '"consecutive_days":', max_consecutive,
                            '}'
                        )),
                        ','
                    ),
                    '],',
                    '"summary":{',
                    '"total_alerts":', COUNT(*), ',',
                    '"calculated_at":"', CURRENT_TIMESTAMP(), '"',
                    '}',
                    '}'
                ) as alerts_json,
                CURRENT_TIMESTAMP() as calculated_at
            FROM patterns
            WHERE affected_days > 0
            GROUP BY emp_id
        """).createOrReplaceTempView(f"week_emp_results_{week_start_str.replace('-', '_')}")
        
        # Update week_alerts table
        spark.sql(f"""
            MERGE INTO week_alerts target
            USING week_emp_results_{week_start_str.replace('-', '_')} source
            ON target.emp_id = source.emp_id AND target.week_start_date = source.week_start_date
            WHEN MATCHED THEN
                UPDATE SET 
                    target.alerts_json = source.alerts_json,
                    target.updated_on = source.calculated_at
            WHEN NOT MATCHED THEN
                INSERT (emp_id, week_start_date, alerts_json, updated_on)
                VALUES (source.emp_id, source.week_start_date, source.alerts_json, source.calculated_at)
        """)
        
        count = spark.sql(f"SELECT COUNT(*) as cnt FROM week_emp_results_{week_start_str.replace('-', '_')}").first().cnt
        results[week_start_str] = count
        
        print(f"  → Processed {count} employee records")
    
    spark.stop()
    
    total = sum(results.values())
    print(f"\n✓ Week Employee Pipeline completed: {total} records across {len(results)} weeks")
    return {"status": "success", "processed": total, "by_week": results}
```

4. Week Manager Alerts Pipeline

```python
# week_mgr_pipeline.py
def run_week_mgr_pipeline():
    """Week Manager Alerts Pipeline - Team patterns from day_alerts"""
    
    spark = SparkSession.builder.appName("WeekMgrPipeline").getOrCreate()
    
    # Check if week_mgr_alerts needs processing
    four_hours_ago = (datetime.now() - timedelta(hours=4)).strftime('%Y-%m-%d %H:%M:%S')
    
    # Get weeks with recent updates
    weeks_to_process = spark.sql(f"""
        SELECT DISTINCT week_start_date
        FROM week_mgr_alerts 
        WHERE updated_on > '{four_hours_ago}'
        ORDER BY week_start_date
    """).collect()
    
    if not weeks_to_process:
        print("No week_mgr_alerts updated in last 4 hours. Skipping.")
        spark.stop()
        return {"status": "skipped"}
    
    results = {}
    yesterday = (datetime.now() - timedelta(days=1)).date()
    
    for week_row in weeks_to_process:
        week_start = week_row.week_start_date
        week_start_str = week_start.strftime('%Y-%m-%d')
        
        # Week end: min(week_start + 6 days, yesterday)
        week_end_date = min(week_start + timedelta(days=6), yesterday)
        week_end_str = week_end_date.strftime('%Y-%m-%d')
        
        print(f"Processing week manager alerts: {week_start_str} to {week_end_str}")
        
        # Calculate week manager patterns
        spark.sql(f"""
            WITH week_data AS (
                SELECT 
                    de.manager_id,
                    da.shift_date,
                    FROM_JSON(
                        GET_JSON_OBJECT(da.alerts_json, '$.triggered_alerts'),
                        'array<int>'
                    ) as alert_ids
                FROM day_alerts da
                JOIN day_emp de ON da.emp_id = de.emp_id AND da.shift_date = de.shift_date
                WHERE da.shift_date BETWEEN '{week_start_str}' AND '{week_end_str}'
                AND da.shift_date <= '{yesterday.strftime('%Y-%m-%d')}'
                AND de.manager_id IS NOT NULL
            ),
            daily_stats AS (
                SELECT 
                    manager_id,
                    shift_date,
                    alert_id,
                    COUNT(DISTINCT de.emp_id) as daily_affected,
                    (SELECT COUNT(DISTINCT emp_id) 
                     FROM day_emp 
                     WHERE manager_id = wd.manager_id 
                     AND shift_date = wd.shift_date) as daily_team_size
                FROM week_data wd
                LATERAL VIEW EXPLODE(alert_ids) t AS alert_id
                JOIN day_emp de ON wd.manager_id = de.manager_id 
                    AND wd.shift_date = de.shift_date
                    AND wd.alert_id IN (
                        SELECT alert FROM (
                            SELECT EXPLODE(alert_ids) as alert 
                            FROM week_data wd2 
                            WHERE wd2.manager_id = wd.manager_id 
                            AND wd2.shift_date = wd.shift_date
                        ) t
                    )
                GROUP BY manager_id, shift_date, alert_id
            ),
            week_patterns AS (
                SELECT 
                    manager_id,
                    alert_id,
                    MAX((daily_affected * 100.0 / NULLIF(daily_team_size, 0))) as max_daily_percent,
                    SUM(CASE WHEN (daily_affected * 100.0 / NULLIF(daily_team_size, 0)) >= 20 THEN 1 ELSE 0 END) as days_above_20
                FROM daily_stats
                GROUP BY manager_id, alert_id
            )
            SELECT 
                manager_id,
                '{week_start_str}' as week_start_date,
                -- Build week manager JSON
                CONCAT(
                    '{',
                    '"team_patterns":[',
                    ARRAY_JOIN(
                        COLLECT_LIST(CONCAT(
                            '{',
                            '"alert":', alert_id, ',',
                            '"severity":"',
                            CASE 
                                WHEN (max_daily_percent >= 60 AND days_above_20 >= 3) THEN 'HIGH'
                                WHEN (max_daily_percent >= 40 AND max_daily_percent < 60 AND days_above_20 >= 3) THEN 'MEDIUM'
                                WHEN (max_daily_percent >= 20 AND max_daily_percent < 40 AND days_above_20 >= 1) THEN 'LOW'
                                ELSE 'NONE'
                            END, '",',
                            '"max_daily_percent":', ROUND(max_daily_percent, 1), ',',
                            '"days_above_20":', days_above_20,
                            '}'
                        )),
                        ','
                    ),
                    '],',
                    '"summary":{',
                    '"total_alerts":', COUNT(*), ',',
                    '"calculated_at":"', CURRENT_TIMESTAMP(), '"',
                    '}',
                    '}'
                ) as alerts_json,
                CURRENT_TIMESTAMP() as calculated_at
            FROM week_patterns
            WHERE days_above_20 > 0
            GROUP BY manager_id
        """).createOrReplaceTempView(f"week_mgr_results_{week_start_str.replace('-', '_')}")
        
        # Update week_mgr_alerts table
        spark.sql(f"""
            MERGE INTO week_mgr_alerts target
            USING week_mgr_results_{week_start_str.replace('-', '_')} source
            ON target.manager_id = source.manager_id AND target.week_start_date = source.week_start_date
            WHEN MATCHED THEN
                UPDATE SET 
                    target.alerts_json = source.alerts_json,
                    target.updated_on = source.calculated_at
            WHEN NOT MATCHED THEN
                INSERT (manager_id, week_start_date, alerts_json, updated_on)
                VALUES (source.manager_id, source.week_start_date, source.alerts_json, source.calculated_at)
        """)
        
        count = spark.sql(f"SELECT COUNT(*) as cnt FROM week_mgr_results_{week_start_str.replace('-', '_')}").first().cnt
        results[week_start_str] = count
        
        print(f"  → Processed {count} manager records")
    
    spark.stop()
    
    total = sum(results.values())
    print(f"\n✓ Week Manager Pipeline completed: {total} records across {len(results)} weeks")
    return {"status": "success", "processed": total, "by_week": results}
```

5. Month Employee Alerts Pipeline

```python
# month_emp_pipeline.py
def run_month_emp_pipeline():
    """Month Employee Alerts Pipeline - Monthly patterns"""
    
    spark = SparkSession.builder.appName("MonthEmpPipeline").getOrCreate()
    
    # Check if month_alerts needs processing
    four_hours_ago = (datetime.now() - timedelta(hours=4)).strftime('%Y-%m-%d %H:%M:%S')
    
    # Get months with recent updates
    months_to_process = spark.sql(f"""
        SELECT DISTINCT month_start_date
        FROM month_alerts 
        WHERE updated_on > '{four_hours_ago}'
        ORDER BY month_start_date
    """).collect()
    
    if not months_to_process:
        print("No month_alerts updated in last 4 hours. Skipping.")
        spark.stop()
        return {"status": "skipped"}
    
    results = {}
    yesterday = (datetime.now() - timedelta(days=1)).date()
    
    for month_row in months_to_process:
        month_start = month_row.month_start_date
        month_start_str = month_start.strftime('%Y-%m-%d')
        
        # Determine month end
        if month_start.month == datetime.now().month and month_start.year == datetime.now().year:
            # Current month: until yesterday
            month_end_date = yesterday
        else:
            # Past month: last day of month
            month_end_date = month_start + timedelta(days=31)
            month_end_date = month_end_date.replace(day=1) - timedelta(days=1)
        
        month_end_str = month_end_date.strftime('%Y-%m-%d')
        
        print(f"Processing month: {month_start_str} to {month_end_str}")
        
        # Get employees with recent month_alerts updates
        spark.sql(f"""
            SELECT DISTINCT emp_id
            FROM month_alerts 
            WHERE month_start_date = '{month_start_str}'
            AND updated_on > '{four_hours_ago}'
        """).createOrReplaceTempView(f"month_emps_{month_start_str.replace('-', '_')}")
        
        # Calculate month patterns
        spark.sql(f"""
            WITH month_data AS (
                SELECT 
                    da.emp_id,
                    da.shift_date,
                    DATE_TRUNC('week', da.shift_date) as week_start,
                    FROM_JSON(
                        GET_JSON_OBJECT(da.alerts_json, '$.triggered_alerts'),
                        'array<int>'
                    ) as alert_ids
                FROM day_alerts da
                JOIN month_emps_{month_start_str.replace('-', '_')} me ON da.emp_id = me.emp_id
                WHERE da.shift_date BETWEEN '{month_start_str}' AND '{month_end_str}'
                AND da.shift_date <= '{yesterday.strftime('%Y-%m-%d')}'
            ),
            exploded_data AS (
                SELECT 
                    emp_id,
                    alert_id,
                    shift_date,
                    week_start
                FROM month_data
                LATERAL VIEW EXPLODE(alert_ids) t AS alert_id
            ),
            weekly_stats AS (
                SELECT 
                    emp_id,
                    alert_id,
                    week_start,
                    COUNT(DISTINCT shift_date) as affected_days_in_week,
                    MAX(consecutive_days) as max_consecutive_in_week
                FROM (
                    SELECT 
                        emp_id,
                        alert_id,
                        week_start,
                        shift_date,
                        COUNT(*) OVER (
                            PARTITION BY emp_id, alert_id, week_start, grp
                            ORDER BY shift_date
                        ) as consecutive_days
                    FROM (
                        SELECT 
                            emp_id,
                            alert_id,
                            week_start,
                            shift_date,
                            DATE_SUB(shift_date, 
                                ROW_NUMBER() OVER (
                                    PARTITION BY emp_id, alert_id, week_start
                                    ORDER BY shift_date
                                )
                            ) as grp
                        FROM exploded_data
                    ) t
                ) t
                GROUP BY emp_id, alert_id, week_start
            ),
            monthly_stats AS (
                SELECT 
                    ed.emp_id,
                    ed.alert_id,
                    COUNT(DISTINCT ed.shift_date) as total_days_in_month,
                    COLLECT_LIST(STRUCT(
                        ws.week_start,
                        ws.affected_days_in_week,
                        ws.max_consecutive_in_week
                    )) as weekly_patterns
                FROM exploded_data ed
                JOIN weekly_stats ws ON ed.emp_id = ws.emp_id 
                    AND ed.alert_id = ws.alert_id 
                    AND ed.week_start = ws.week_start
                GROUP BY ed.emp_id, ed.alert_id
            ),
            patterns AS (
                SELECT 
                    emp_id,
                    alert_id,
                    total_days_in_month,
                    weekly_patterns,
                    -- Check consecutive week patterns
                    EXISTS (
                        SELECT 1
                        FROM UNNEST(weekly_patterns) w1
                        CROSS JOIN UNNEST(weekly_patterns) w2
                        WHERE w1.week_start = DATE_SUB(w2.week_start, 7)
                        AND w1.max_consecutive_in_week >= 3
                        AND w2.max_consecutive_in_week >= 3
                    ) as has_high_pattern,
                    EXISTS (
                        SELECT 1
                        FROM UNNEST(weekly_patterns) w1
                        CROSS JOIN UNNEST(weekly_patterns) w2
                        WHERE w1.week_start = DATE_SUB(w2.week_start, 7)
                        AND w1.max_consecutive_in_week >= 2
                        AND w2.max_consecutive_in_week >= 2
                    ) as has_medium_pattern,
                    EXISTS (
                        SELECT 1
                        FROM UNNEST(weekly_patterns) w1
                        CROSS JOIN UNNEST(weekly_patterns) w2
                        WHERE w1.week_start = DATE_SUB(w2.week_start, 7)
                        AND w1.affected_days_in_week >= 1
                        AND w2.affected_days_in_week >= 1
                    ) as has_low_pattern
                FROM monthly_stats
            ),
            severity_assigned AS (
                SELECT 
                    emp_id,
                    alert_id,
                    total_days_in_month,
                    weekly_patterns,
                    CASE 
                        WHEN total_days_in_month >= 18 OR has_high_pattern THEN 'HIGH'
                        WHEN total_days_in_month >= 12 OR has_medium_pattern THEN 'MEDIUM'
                        WHEN total_days_in_month >= 6 OR has_low_pattern THEN 'LOW'
                        ELSE NULL
                    END as severity_month
                FROM patterns
                WHERE total_days_in_month >= 6 OR has_low_pattern
            )
            SELECT 
                emp_id,
                '{month_start_str}' as month_start_date,
                -- Build month patterns JSON
                CONCAT(
                    '{',
                    '"month_patterns":[',
                    ARRAY_JOIN(
                        COLLECT_LIST(CONCAT(
                            '{',
                            '"alert":', alert_id, ',',
                            '"severity":"', severity_month, '",',
                            '"total_days":', total_days_in_month, ',',
                            '"weekly_patterns":[',
                            ARRAY_JOIN(
                                TRANSFORM(weekly_patterns, wp -> CONCAT(
                                    '{',
                                    '"week":"', wp.week_start, '",',
                                    '"affected_days":', wp.affected_days_in_week, ',',
                                    '"consecutive_days":', wp.max_consecutive_in_week,
                                    '}'
                                )),
                                ','
                            ),
                            ']',
                            '}'
                        )),
                        ','
                    ),
                    '],',
                    '"summary":{',
                    '"total_alerts":', COUNT(*), ',',
                    '"calculated_at":"', CURRENT_TIMESTAMP(), '"',
                    '}',
                    '}'
                ) as alerts_json,
                CURRENT_TIMESTAMP() as calculated_at
            FROM severity_assigned
            WHERE severity_month IS NOT NULL
            GROUP BY emp_id
        """).createOrReplaceTempView(f"month_emp_results_{month_start_str.replace('-', '_')}")
        
        # Update month_alerts table
        spark.sql(f"""
            MERGE INTO month_alerts target
            USING month_emp_results_{month_start_str.replace('-', '_')} source
            ON target.emp_id = source.emp_id AND target.month_start_date = source.month_start_date
            WHEN MATCHED THEN
                UPDATE SET 
                    target.alerts_json = source.alerts_json,
                    target.updated_on = source.calculated_at
            WHEN NOT MATCHED THEN
                INSERT (emp_id, month_start_date, alerts_json, updated_on)
                VALUES (source.emp_id, source.month_start_date, source.alerts_json, source.calculated_at)
        """)
        
        count = spark.sql(f"SELECT COUNT(*) as cnt FROM month_emp_results_{month_start_str.replace('-', '_')}").first().cnt
        results[month_start_str] = count
        
        print(f"  → Processed {count} employee records")
    
    spark.stop()
    
    total = sum(results.values())
    print(f"\n✓ Month Employee Pipeline completed: {total} records across {len(results)} months")
    return {"status": "success", "processed": total, "by_month": results}
```

6. Month Manager Alerts Pipeline

```python
# month_mgr_pipeline.py
def run_month_mgr_pipeline():
    """Month Manager Alerts Pipeline - Team monthly patterns"""
    
    spark = SparkSession.builder.appName("MonthMgrPipeline").getOrCreate()
    
    # Check if month_mgr_alerts needs processing
    four_hours_ago = (datetime.now() - timedelta(hours=4)).strftime('%Y-%m-%d %H:%M:%S')
    
    # Get months with recent updates
    months_to_process = spark.sql(f"""
        SELECT DISTINCT month_start_date
        FROM month_mgr_alerts 
        WHERE updated_on > '{four_hours_ago}'
        ORDER BY month_start_date
    """).collect()
    
    if not months_to_process:
        print("No month_mgr_alerts updated in last 4 hours. Skipping.")
        spark.stop()
        return {"status": "skipped"}
    
    results = {}
    yesterday = (datetime.now() - timedelta(days=1)).date()
    
    for month_row in months_to_process:
        month_start = month_row.month_start_date
        month_start_str = month_start.strftime('%Y-%m-%d')
        
        # Determine month end
        if month_start.month == datetime.now().month and month_start.year == datetime.now().year:
            # Current month: until yesterday
            month_end_date = yesterday
        else:
            # Past month: last day of month
            month_end_date = month_start + timedelta(days=31)
            month_end_date = month_end_date.replace(day=1) - timedelta(days=1)
        
        month_end_str = month_end_date.strftime('%Y-%m-%d')
        
        print(f"Processing month manager alerts: {month_start_str} to {month_end_str}")
        
        # Calculate month manager patterns
        spark.sql(f"""
            WITH month_data AS (
                SELECT 
                    de.manager_id,
                    de.emp_id,
                    da.shift_date,
                    DATE_TRUNC('week', da.shift_date) as week_start,
                    FROM_JSON(
                        GET_JSON_OBJECT(da.alerts_json, '$.triggered_alerts'),
                        'array<int>'
                    ) as alert_ids
                FROM day_alerts da
                JOIN day_emp de ON da.emp_id = de.emp_id AND da.shift_date = de.shift_date
                WHERE da.shift_date BETWEEN '{month_start_str}' AND '{month_end_str}'
                AND da.shift_date <= '{yesterday.strftime('%Y-%m-%d')}'
                AND de.manager_id IS NOT NULL
            ),
            team_sizes AS (
                SELECT 
                    manager_id,
                    COUNT(DISTINCT emp_id) as team_size
                FROM day_emp 
                WHERE shift_date BETWEEN '{month_start_str}' AND '{month_end_str}'
                AND manager_id IN (SELECT DISTINCT manager_id FROM month_data)
                GROUP BY manager_id
            ),
            -- Condition 1: % of team affected for ≥6 days
            team_days_affected AS (
                SELECT 
                    manager_id,
                    alert_id,
                    COUNT(DISTINCT shift_date) as team_affected_days
                FROM (
                    SELECT 
                        manager_id,
                        shift_date,
                        alert_id
                    FROM month_data
                    LATERAL VIEW EXPLODE(alert_ids) t AS alert_id
                ) t
                GROUP BY manager_id, alert_id
            ),
            team_percentage AS (
                SELECT 
                    tda.manager_id,
                    tda.alert_id,
                    tda.team_affected_days,
                    ts.team_size,
                    (tda.team_affected_days * 100.0 / ts.team_size) as percentage_affected
                FROM team_days_affected tda
                JOIN team_sizes ts ON tda.manager_id = ts.manager_id
                WHERE tda.team_affected_days >= 6
            ),
            -- Condition 2: Individuals with 3 consecutive days across 2 weeks
            individual_patterns AS (
                SELECT 
                    manager_id,
                    alert_id,
                    COUNT(DISTINCT emp_id) as individuals_with_3_consecutive
                FROM (
                    SELECT 
                        manager_id,
                        emp_id,
                        alert_id,
                        COUNT(DISTINCT week_start) as weeks_count
                    FROM (
                        SELECT 
                            manager_id,
                            emp_id,
                            alert_id,
                            week_start,
                            MAX(consecutive_days) as max_consecutive
                        FROM (
                            SELECT 
                                manager_id,
                                emp_id,
                                alert_id,
                                week_start,
                                shift_date,
                                COUNT(*) OVER (
                                    PARTITION BY manager_id, emp_id, alert_id, week_start, grp
                                    ORDER BY shift_date
                                ) as consecutive_days
                            FROM (
                                SELECT 
                                    manager_id,
                                    emp_id,
                                    alert_id,
                                    week_start,
                                    shift_date,
                                    DATE_SUB(shift_date, 
                                        ROW_NUMBER() OVER (
                                            PARTITION BY manager_id, emp_id, alert_id, week_start
                                            ORDER BY shift_date
                                        )
                                    ) as grp
                                FROM month_data
                                LATERAL VIEW EXPLODE(alert_ids) t AS alert_id
                            ) t
                        ) t
                        GROUP BY manager_id, emp_id, alert_id, week_start
                        HAVING MAX(consecutive_days) >= 3
                    ) t
                    GROUP BY manager_id, emp_id, alert_id
                    HAVING COUNT(DISTINCT week_start) >= 2
                ) t
                GROUP BY manager_id, alert_id
            ),
            combined_metrics AS (
                SELECT 
                    COALESCE(tp.manager_id, ip.manager_id) as manager_id,
                    COALESCE(tp.alert_id, ip.alert_id) as alert_id,
                    COALESCE(tp.team_affected_days, 0) as team_affected_days,
                    COALESCE(tp.team_size, 0) as team_size,
                    COALESCE(tp.percentage_affected, 0) as percentage_affected,
                    COALESCE(ip.individuals_with_3_consecutive, 0) as individuals_count
                FROM team_percentage tp
                FULL OUTER JOIN individual_patterns ip 
                    ON tp.manager_id = ip.manager_id AND tp.alert_id = ip.alert_id
            ),
            severity_assigned AS (
                SELECT 
                    manager_id,
                    alert_id,
                    team_affected_days,
                    team_size,
                    percentage_affected,
                    individuals_count,
                    CASE 
                        -- HIGH: ≥60% team affected OR ≥3 individuals with pattern
                        WHEN percentage_affected >= 60 OR individuals_count >= 3 THEN 'HIGH'
                        -- MEDIUM: 40-60% team affected OR 2 individuals with pattern
                        WHEN (percentage_affected >= 40 AND percentage_affected < 60) 
                             OR individuals_count >= 2 THEN 'MEDIUM'
                        -- LOW: 20-40% team affected OR 1 individual with pattern
                        WHEN (percentage_affected >= 20 AND percentage_affected < 40) 
                             OR individuals_count >= 1 THEN 'LOW'
                        ELSE NULL
                    END as severity_month
                FROM combined_metrics
                WHERE (percentage_affected >= 20 AND team_affected_days >= 6) 
                   OR individuals_count >= 1
            )
            SELECT 
                manager_id,
                '{month_start_str}' as month_start_date,
                team_size,
                -- Build month manager JSON
                CONCAT(
                    '{',
                    '"month_team_patterns":[',
                    ARRAY_JOIN(
                        COLLECT_LIST(CONCAT(
                            '{',
                            '"alert":', alert_id, ',',
                            '"severity":"', severity_month, '",',
                            '"metrics":{',
                            '"team_affected_days":', team_affected_days, ',',
                            '"team_size":', team_size, ',',
                            '"percentage_affected":', ROUND(percentage_affected, 1), ',',
                            '"individuals_with_3_consecutive":', individuals_count,
                            '}',
                            '}'
                        )),
                        ','
                    ),
                    '],',
                    '"summary":{',
                    '"total_alerts":', COUNT(*), ',',
                    '"calculated_at":"', CURRENT_TIMESTAMP(), '"',
                    '}',
                    '}'
                ) as alerts_json,
                CURRENT_TIMESTAMP() as calculated_at
            FROM severity_assigned
            WHERE severity_month IS NOT NULL
            GROUP BY manager_id, team_size
        """).createOrReplaceTempView(f"month_mgr_results_{month_start_str.replace('-', '_')}")
        
        # Update month_mgr_alerts table
        spark.sql(f"""
            MERGE INTO month_mgr_alerts target
            USING month_mgr_results_{month_start_str.replace('-', '_')} source
            ON target.manager_id = source.manager_id AND target.month_start_date = source.month_start_date
            WHEN MATCHED THEN
                UPDATE SET 
                    target.alerts_json = source.alerts_json,
                    target.team_size = source.team_size,
                    target.updated_on = source.calculated_at
            WHEN NOT MATCHED THEN
                INSERT (manager_id, month_start_date, team_size, alerts_json, updated_on)
                VALUES (source.manager_id, source.month_start_date, source.team_size, 
                       source.alerts_json, source.calculated_at)
        """)
        
        count = spark.sql(f"SELECT COUNT(*) as cnt FROM month_mgr_results_{month_start_str.replace('-', '_')}").first().cnt
        results[month_start_str] = count
        
        print(f"  → Processed {count} manager records")
    
    spark.stop()
    
    total = sum(results.values())
    print(f"\n✓ Month Manager Pipeline completed: {total} records across {len(results)} months")
    return {"status": "success", "processed": total, "by_month": results}
```

Complete Orchestration

```python
# main_orchestrator.py
from datetime import datetime

def run_all_pipelines():
    """Run all 6 pipelines"""
    
    print("=" * 60)
    print("ALERTS PROCESSING PIPELINE")
    print("=" * 60)
    print(f"Started at: {datetime.now()}")
    print()
    
    all_results = {}
    
    try:
        # 1. Day Employee Alerts
        print("1. DAY EMPLOYEE ALERTS")
        print("-" * 40)
        all_results['day_emp'] = run_day_emp_pipeline()
        
        # 2. Day Manager Alerts
        print("\n2. DAY MANAGER ALERTS")
        print("-" * 40)
        all_results['day_mgr'] = run_day_mgr_pipeline()
        
        # 3. Week Employee Alerts
        print("\n3. WEEK EMPLOYEE ALERTS")
        print("-" * 40)
        all_results['week_emp'] = run_week_emp_pipeline()
        
        # 4. Week Manager Alerts
        print("\n4. WEEK MANAGER ALERTS")
        print("-" * 40)
        all_results['week_mgr'] = run_week_mgr_pipeline()
        
        # 5. Month Employee Alerts
        print("\n5. MONTH EMPLOYEE ALERTS")
        print("-" * 40)
        all_results['month_emp'] = run_month_emp_pipeline()
        
        # 6. Month Manager Alerts
        print("\n6. MONTH MANAGER ALERTS")
        print("-" * 40)
        all_results['month_mgr'] = run_month_mgr_pipeline()
        
        all_results['overall_status'] = 'success'
        
    except Exception as e:
        print(f"\n✗ Pipeline failed: {str(e)}")
        all_results['overall_status'] = 'failed'
        all_results['error'] = str(e)
    
    print("\n" + "=" * 60)
    print("SUMMARY")
    print("=" * 60)
    
    for pipeline, result in all_results.items():
        if isinstance(result, dict) and 'processed' in result:
            print(f"{pipeline}: {result['processed']} records")
    
    print(f"\nCompleted at: {datetime.now()}")
    return all_results

if __name__ == "__main__":
    results = run_all_pipelines()
```

This is the complete implementation for all 6 pipelines with the clean JSON structure you specified!





SELECT
    emp_id,
    shift_date,
    alert_ids,

    to_json(
        struct(
            transform(
                alert_ids,
                id -> struct(
                    id AS alert,
                    'MEDIUM' AS severity
                )
            ) AS triggered_alerts,
            struct(
                size(alert_ids) AS total_alerts,
                CAST(current_timestamp() AS STRING) AS calculated_at
            ) AS summary
        )
    ) AS alerts_json,

    current_timestamp() AS calculated_at
FROM triggered_alerts
WHERE size(alert_ids) > 0


Perfect! You're absolutely right - keep it simple and clean. Here's the minimal, clean JSON structure:

Day Employee Alerts (day_emp)

```json
{
  "triggered_alerts": [
    {
      "alert": 1,
      "severity": "MEDIUM"
    },
    {
      "alert": 4, 
      "severity": "MEDIUM"
    },
    {
      "alert": 7,
      "severity": "MEDIUM"
    }
  ],
  "summary": {
    "total_alerts": 3,
    "calculated_at": "2024-01-15T14:30:54Z"
  }
}
```

Complete Implementation

```python
# day_emp_alerts_clean.py
from pyspark.sql import SparkSession
from datetime import datetime, timedelta

def calculate_day_emp_alerts():
    """Calculate and store clean alerts JSON in day_emp"""
    
    spark = SparkSession.builder.appName("DayEmpAlerts").getOrCreate()
    
    # 1. Get records updated in last 4 hours
    four_hours_ago = (datetime.now() - timedelta(hours=4)).strftime('%Y-%m-%d %H:%M:%S')
    
    spark.sql(f"""
        WITH updated_records AS (
            SELECT DISTINCT emp_id, shift_date 
            FROM loginlogout 
            WHERE updated_on > '{four_hours_ago}'
        ),
        day_data AS (
            SELECT 
                de.*,
                -- Calculate active percentage
                CASE 
                    WHEN (de.active_time + de.idle_time + de.lock_time) > 0 
                    THEN (de.active_time * 100.0 / (de.active_time + de.idle_time + de.lock_time))
                    ELSE 0 
                END as active_percent
            FROM day_emp de
            JOIN updated_records ur ON de.emp_id = ur.emp_id AND de.shift_date = ur.shift_date
        ),
        triggered_alerts AS (
            SELECT 
                emp_id,
                shift_date,
                -- Collect ALL triggered alerts
                ARRAY_REMOVE(ARRAY(
                    -- Alert 1: Few Breaks
                    CASE WHEN (idle_time + lock_time) < expected_break_time THEN 1 ELSE NULL END,
                    
                    -- Alert 2: Low Active Time
                    CASE WHEN active_time < expected_shift_time THEN 2 ELSE NULL END,
                    
                    -- Alert 3: High Active Time
                    CASE WHEN active_time > expected_shift_time THEN 3 ELSE NULL END,
                    
                    -- Alert 4: Low Active & High Idle Time
                    CASE WHEN active_time < (0.9 * expected_shift_time) 
                         AND active_percent < 90 THEN 4 ELSE NULL END,
                    
                    -- Alert 5: Low Active & Low Idle Time
                    CASE WHEN active_time < (0.9 * expected_shift_time) 
                         AND active_percent > 90 THEN 5 ELSE NULL END,
                    
                    -- Alert 6: High Active & High Idle Time
                    CASE WHEN active_time > (1.1 * expected_shift_time) 
                         AND idle_time > 1.0 THEN 6 ELSE NULL END,
                    
                    -- Alert 7: Few Breaks + Low Active & High Idle Time
                    CASE WHEN (idle_time + lock_time) < expected_break_time
                         AND active_time < (0.9 * expected_shift_time)
                         AND active_percent < 90 THEN 7 ELSE NULL END,
                    
                    -- Alert 8: Few Breaks + High Active & High Idle Time
                    CASE WHEN (idle_time + lock_time) < expected_break_time
                         AND active_time > (1.1 * expected_shift_time)
                         AND idle_time > 1.0 THEN 8 ELSE NULL END,
                    
                    -- Alert 9: Few Breaks + Low Active & Low Idle Time
                    CASE WHEN (idle_time + lock_time) < expected_break_time
                         AND active_time < (0.9 * expected_shift_time)
                         AND active_percent > 90 THEN 9 ELSE NULL END,
                    
                    -- Alert 10: Low Active + High Idle Time
                    CASE WHEN active_time < expected_shift_time
                         AND active_time < (0.9 * expected_shift_time)
                         AND active_percent < 90 THEN 10 ELSE NULL END,
                    
                    -- Alert 11: Low Active + Low Idle Time
                    CASE WHEN active_time < expected_shift_time
                         AND active_time < (0.9 * expected_shift_time)
                         AND active_percent > 90 THEN 11 ELSE NULL END,
                    
                    -- Alert 12: High Active + High Idle Time
                    CASE WHEN active_time > (1.1 * expected_shift_time)
                         AND idle_time > 1.0
                         AND active_time > expected_shift_time THEN 12 ELSE NULL END,
                    
                    -- Alert 13: Low Active + Few Breaks
                    CASE WHEN (idle_time + lock_time) < expected_break_time
                         AND active_time < expected_shift_time THEN 13 ELSE NULL END,
                    
                    -- Alert 14: High Active + Few Breaks
                    CASE WHEN (idle_time + lock_time) < expected_break_time
                         AND active_time > expected_shift_time THEN 14 ELSE NULL END
                ), NULL) as alert_ids
            FROM day_data
        )
        SELECT 
            emp_id,
            shift_date,
            alert_ids,
            -- Build clean JSON
            CONCAT(
                '{',
                '"triggered_alerts":[',
                ARRAY_JOIN(
                    TRANSFORM(alert_ids, id -> CONCAT(
                        '{"alert":', id, ',',
                        '"severity":"MEDIUM"}'
                    )),
                    ','
                ),
                '],',
                '"summary":{',
                '"total_alerts":', SIZE(alert_ids), ',',
                '"calculated_at":"', CURRENT_TIMESTAMP(), '"',
                '}',
                '}'
            ) as alerts_json,
            CURRENT_TIMESTAMP() as calculated_at
        FROM triggered_alerts
        WHERE SIZE(alert_ids) > 0
    """).createOrReplaceTempView("day_alerts_clean")
    
    # Update day_emp table
    spark.sql("""
        MERGE INTO day_emp target
        USING day_alerts_clean source
        ON target.emp_id = source.emp_id AND target.shift_date = source.shift_date
        WHEN MATCHED THEN
            UPDATE SET 
                target.alerts_json = source.alerts_json,
                target.alert_updated_at = source.calculated_at
        WHEN NOT MATCHED THEN
            INSERT (emp_id, shift_date, alerts_json, alert_updated_at)
            VALUES (source.emp_id, source.shift_date, source.alerts_json, source.calculated_at)
    """)
    
    count = spark.sql("SELECT COUNT(*) as cnt FROM day_alerts_clean").first().cnt
    spark.stop()
    return count

if __name__ == "__main__":
    processed = calculate_day_emp_alerts()
    print(f"Processed {processed} records")
```

Day Manager Alerts (day_mgr)

```json
{
  "team_alerts": [
    {
      "alert": 1,
      "severity": "HIGH",
      "affected_employees": 18,
      "team_size": 25,
      "percentage": 72.0
    },
    {
      "alert": 4,
      "severity": "MEDIUM",
      "affected_employees": 12,
      "team_size": 25,
      "percentage": 48.0
    }
  ],
  "summary": {
    "total_alerts": 2,
    "calculated_at": "2024-01-15T14:35:22Z"
  }
}
```

```python
# day_mgr_alerts_clean.py
def calculate_day_mgr_alerts():
    """Calculate team alerts from day_emp data"""
    
    spark = SparkSession.builder.appName("DayMgrAlerts").getOrCreate()
    
    spark.sql("""
        WITH today_alerts AS (
            SELECT 
                hc.fun_mgr_id as manager_id,
                de.shift_date,
                -- Extract alert IDs from JSON
                GET_JSON_OBJECT(de.alerts_json, '$.triggered_alerts[*].alert') as alert_ids
            FROM day_emp de
            JOIN hrcentral hc ON de.emp_id = hc.emplid
            WHERE de.alerts_json IS NOT NULL
            AND de.shift_date = CURRENT_DATE() - 1  -- Yesterday
        ),
        exploded_alerts AS (
            SELECT 
                manager_id,
                shift_date,
                EXPLODE(FROM_JSON(alert_ids, 'array<int>')) as alert_id
            FROM today_alerts
            WHERE alert_ids IS NOT NULL
        ),
        team_sizes AS (
            SELECT 
                fun_mgr_id as manager_id,
                COUNT(*) as team_size
            FROM hrcentral
            GROUP BY fun_mgr_id
        ),
        alert_counts AS (
            SELECT 
                ea.manager_id,
                ea.shift_date,
                ea.alert_id,
                COUNT(*) as affected_count,
                ts.team_size,
                ROUND((COUNT(*) * 100.0 / ts.team_size), 1) as percentage
            FROM exploded_alerts ea
            JOIN team_sizes ts ON ea.manager_id = ts.manager_id
            GROUP BY ea.manager_id, ea.shift_date, ea.alert_id, ts.team_size
        )
        SELECT 
            manager_id,
            shift_date,
            team_size,
            -- Build clean JSON
            CONCAT(
                '{',
                '"team_alerts":[',
                ARRAY_JOIN(
                    COLLECT_LIST(CONCAT(
                        '{',
                        '"alert":', alert_id, ',',
                        '"severity":"',
                        CASE 
                            WHEN percentage >= 60 THEN 'HIGH'
                            WHEN percentage >= 40 THEN 'MEDIUM'
                            WHEN percentage >= 20 THEN 'LOW'
                            ELSE 'NONE'
                        END, '",',
                        '"affected_employees":', affected_count, ',',
                        '"team_size":', team_size, ',',
                        '"percentage":', percentage,
                        '}'
                    )),
                    ','
                ),
                '],',
                '"summary":{',
                '"total_alerts":', COUNT(*), ',',
                '"calculated_at":"', CURRENT_TIMESTAMP(), '"',
                '}',
                '}'
            ) as alerts_json,
            CURRENT_TIMESTAMP() as calculated_at
        FROM alert_counts
        WHERE percentage >= 20  -- Only include if meets threshold
        GROUP BY manager_id, shift_date, team_size
    """).createOrReplaceTempView("mgr_alerts_clean")
    
    # Update day_mgr table
    spark.sql("""
        MERGE INTO day_mgr target
        USING mgr_alerts_clean source
        ON target.manager_id = source.manager_id AND target.shift_date = source.shift_date
        WHEN MATCHED THEN
            UPDATE SET 
                target.alerts_json = source.alerts_json,
                target.team_size = source.team_size,
                target.alert_updated_at = source.calculated_at
        WHEN NOT MATCHED THEN
            INSERT (manager_id, shift_date, team_size, alerts_json, alert_updated_at)
            VALUES (source.manager_id, source.shift_date, source.team_size, 
                   source.alerts_json, source.calculated_at)
    """)
    
    count = spark.sql("SELECT COUNT(*) as cnt FROM mgr_alerts_clean").first().cnt
    spark.stop()
    return count
```

Week Employee Alerts (week_emp)

```json
{
  "alert_patterns": [
    {
      "alert": 1,
      "severity": "HIGH",
      "affected_days": 3,
      "consecutive_days": 3
    },
    {
      "alert": 4,
      "severity": "LOW",
      "affected_days": 1,
      "consecutive_days": 1
    }
  ],
  "summary": {
    "total_alerts": 2,
    "calculated_at": "2024-01-15T14:40:15Z"
  }
}
```

```python
# week_emp_alerts_clean.py
def calculate_week_emp_alerts(week_start):
    """Calculate week patterns from day_emp alerts"""
    
    spark = SparkSession.builder.appName("WeekEmpAlerts").getOrCreate()
    
    spark.sql(f"""
        WITH week_data AS (
            SELECT 
                emp_id,
                shift_date,
                GET_JSON_OBJECT(alerts_json, '$.triggered_alerts[*].alert') as alert_ids
            FROM day_emp
            WHERE shift_date BETWEEN '{week_start}' AND DATE_ADD('{week_start}', 6)
            AND alerts_json IS NOT NULL
        ),
        exploded_alerts AS (
            SELECT 
                emp_id,
                shift_date,
                EXPLODE(FROM_JSON(alert_ids, 'array<int>')) as alert_id
            FROM week_data
            WHERE alert_ids IS NOT NULL
        ),
        patterns AS (
            SELECT 
                emp_id,
                alert_id,
                COUNT(DISTINCT shift_date) as affected_days,
                MAX(consecutive_count) as consecutive_days
            FROM (
                SELECT 
                    emp_id,
                    alert_id,
                    shift_date,
                    COUNT(*) OVER (
                        PARTITION BY emp_id, alert_id, grp
                        ORDER BY shift_date
                    ) as consecutive_count
                FROM (
                    SELECT 
                        emp_id,
                        alert_id,
                        shift_date,
                        DATE_SUB(shift_date, 
                            ROW_NUMBER() OVER (
                                PARTITION BY emp_id, alert_id
                                ORDER BY shift_date
                            )
                        ) as grp
                    FROM exploded_alerts
                ) t
            ) t
            GROUP BY emp_id, alert_id
        )
        SELECT 
            emp_id,
            '{week_start}' as week_start,
            CONCAT(
                '{',
                '"alert_patterns":[',
                ARRAY_JOIN(
                    COLLECT_LIST(CONCAT(
                        '{',
                        '"alert":', alert_id, ',',
                        '"severity":"',
                        CASE 
                            WHEN consecutive_days >= 3 THEN 'HIGH'
                            WHEN consecutive_days >= 2 THEN 'MEDIUM'
                            WHEN affected_days >= 1 THEN 'LOW'
                            ELSE 'NONE'
                        END, '",',
                        '"affected_days":', affected_days, ',',
                        '"consecutive_days":', consecutive_days,
                        '}'
                    )),
                    ','
                ),
                '],',
                '"summary":{',
                '"total_alerts":', COUNT(*), ',',
                '"calculated_at":"', CURRENT_TIMESTAMP(), '"',
                '}',
                '}'
            ) as alerts_json,
            CURRENT_TIMESTAMP() as calculated_at
        FROM patterns
        GROUP BY emp_id
    """).createOrReplaceTempView("week_alerts_clean")
    
    # Update week_emp table
    spark.sql("""
        MERGE INTO week_emp target
        USING week_alerts_clean source
        ON target.emp_id = source.emp_id AND target.week_start_date = source.week_start
        WHEN MATCHED THEN
            UPDATE SET 
                target.alerts_json = source.alerts_json,
                target.alert_updated_at = source.calculated_at
        WHEN NOT MATCHED THEN
            INSERT (emp_id, week_start_date, alerts_json, alert_updated_at)
            VALUES (source.emp_id, source.week_start, source.alerts_json, source.calculated_at)
    """)
    
    count = spark.sql("SELECT COUNT(*) as cnt FROM week_alerts_clean").first().cnt
    spark.stop()
    return count
```

Week Manager Alerts (week_mgr)

```json
{
  "team_patterns": [
    {
      "alert": 1,
      "severity": "HIGH",
      "max_daily_percent": 72.0,
      "days_above_20": 3,
      "employees_3_consecutive": 2
    }
  ],
  "summary": {
    "total_alerts": 1,
    "calculated_at": "2024-01-15T14:45:30Z"
  }
}
```

Month Employee Alerts (month_emp)

```json
{
  "month_patterns": [
    {
      "alert": 1,
      "severity": "MEDIUM",
      "total_days": 12,
      "weekly_patterns": [
        {"week": "2024-01-01", "days": 3, "consecutive": 3},
        {"week": "2024-01-08", "days": 4, "consecutive": 2}
      ]
    }
  ],
  "summary": {
    "total_alerts": 1,
    "calculated_at": "2024-01-15T14:50:45Z"
  }
}
```

The Complete Pipeline

```python
# alerts_pipeline_clean.py
from datetime import datetime, timedelta

class CleanAlertsPipeline:
    def run(self):
        """Complete pipeline with clean JSON"""
        
        print("=" * 60)
        print("CLEAN ALERTS PIPELINE")
        print("=" * 60)
        
        # 1. Day Employee Alerts (BASE)
        print("1. Calculating Day Employee Alerts...")
        day_emp_count = calculate_day_emp_alerts()
        print(f"   → {day_emp_count} employee-days processed")
        
        if day_emp_count > 0:
            # 2. Day Manager Alerts
            print("2. Calculating Day Manager Alerts...")
            day_mgr_count = calculate_day_mgr_alerts()
            print(f"   → {day_mgr_count} manager-days processed")
            
            # 3. Week Employee Alerts (last week)
            week_start = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
            print(f"3. Calculating Week Employee Alerts (week: {week_start})...")
            week_emp_count = calculate_week_emp_alerts(week_start)
            print(f"   → {week_emp_count} employee-weeks processed")
            
            # 4. Month Employee Alerts (current month)
            month_start = datetime.now().replace(day=1).strftime('%Y-%m-%d')
            print(f"4. Calculating Month Employee Alerts (month: {month_start})...")
            
        else:
            print("No day alerts found, skipping aggregation")
        
        print("=" * 60)
        print("PIPELINE COMPLETE")
        print("=" * 60)
```

Key Benefits of This Clean Structure:

1. Minimal: Only essential fields
2. Consistent: Same structure across all tables
3. Scalable: Easy to add new alerts
4. Queryable: Easy to parse and analyze
5. Readable: Simple JSON for frontend

Exactly what you asked for - clean, simple, and effective!

Sure. Below is a clear, structured implementation plan that you can use for design review, CAB approval, or stakeholder communication. I’ve organized it into phases, with dependencies, downtime handling, and validation checkpoints explicitly called out.


---

Implementation Plan: Migration from Hourly to 30-Minute Pipelines

Objective

Migrate Gold dashboard data pipelines from hourly intervals to 30-minute intervals, update dependent CDM pipelines, backend APIs, and dashboard queries, ensuring data consistency and minimal disruption.


---

Phase 0: Pre-Implementation Preparation (Mandatory)

0.1 Impact Analysis & Alignment

Identify all Gold tables currently refreshed on an hourly basis.

Identify:

Dashboards consuming hourly data

APIs querying Gold / CDM tables

Backend calculations dependent on hourly aggregations


Confirm new 30-minute table structure, naming, and schema compatibility.

Align stakeholders on:

Required downtime window

Data backfill duration

Rollback plan



0.2 Change Freeze

Freeze:

Dashboard query changes

API schema changes

CDM logic changes


No dependent deployments until Phase 1 completes successfully.



---

Phase 1: Deploy 30-Minute Gold Pipelines (First & Critical Step)

1.1 Deploy 30-Minute Pipelines

Deploy new or modified pipelines that:

Refresh Gold tables every 30 minutes

Either:

Replace existing hourly tables, or

Create parallel 30-minute tables (preferred for safety)




1.2 Historical Data Backfill

Load historical data into 30-minute tables.

This step is mandatory before allowing any consumer access.

During this period:

Hourly dashboards and APIs must not query the Gold layer.



1.3 Downtime Window (Planned)

Put dashboard in maintenance / read-only mode or disable refresh.

APIs should:

Be temporarily disabled, or

Return maintenance response


Reason:

Dashboards still expect hourly intervals

30-minute history is still loading → data inconsistency risk



1.4 Validation

Validate:

Row counts vs hourly aggregates

Time-window correctness (no gaps or overlaps)

SLA adherence (30-minute refresh stability)



Exit Criteria

30-minute Gold tables fully populated (historical + incremental)

Data quality checks passed



---

Phase 2: Update Dashboard Queries & API Logic

2.1 Dashboard Query Updates

Modify dashboard queries to:

Consume 30-minute granularity

Adjust:

Time filters

Aggregations

Windowing logic



Ensure backward logic expecting hourly buckets is removed.


2.2 API Changes

Update APIs to:

Query 30-minute Gold tables

Handle new time intervals correctly


Validate:

API response correctness

Performance impact due to increased data points



2.3 Validation

Test dashboards against:

Short time ranges

Long historical ranges


Validate API responses against expected metrics.


Exit Criteria

Dashboard loads correctly using 30-minute data

APIs return correct and consistent results



---

Phase 3: CDM Pipelines Deployment

3.1 CDM Pipeline Alignment

Modify CDM pipelines to:

Consume only the new 30-minute Gold tables

Remove any dependency on hourly tables


Ensure CDM logic accounts for:

Increased data frequency

Correct rollups where required



3.2 Deploy CDM Pipelines

Deploy CDM pipelines after Phase 2 completion.

Run full CDM refresh.


3.3 Validation

Validate:

CDM table counts

Aggregated values vs Gold layer

Downstream dependency readiness



Exit Criteria

CDM pipelines stable and fully aligned with 30-minute data



---

Phase 4: Backend Deployment

4.1 Backend Logic Updates

Update backend services to:

Use CDM tables built on 30-minute data

Use 30-minute Gold tables where CDM is not applicable


Update:

Calculations

Time-based logic

Caching strategies (if any)



4.2 Deploy Backend

Deploy backend changes after CDM validation.

Enable APIs for production traffic.


4.3 End-to-End Validation

Validate:

Dashboard → API → Backend → CDM → Gold flow

Performance under expected load

Data correctness for multiple time windows




---

Phase 5: Go-Live & Monitoring

5.1 Enable Dashboards

Remove maintenance mode.

Enable scheduled refresh.


5.2 Monitoring & Support

Monitor:

Pipeline failures

API latency

Dashboard load performance


Keep rollback option available for a defined observation window.



---

Rollback Strategy (High Level)

Re-enable hourly pipelines (if kept in parallel).

Switch APIs and dashboards back to hourly tables.

Disable 30-minute pipelines if critical issues arise.



---

Summary Flow (High Level Order)

1. Deploy 30-minute Gold pipelines


2. Backfill historical data (downtime required)


3. Update dashboard queries & APIs


4. Deploy CDM pipelines using 30-minute data


5. Deploy backend changes


6. Go-live & monitor




---

If you want, I can:

Convert this into a one-page CAB document

Create a timeline with estimated durations

Rewrite this in executive-level language or technical runbook format



Absolutely — here is a strong, practical list of KPIs / analytics metrics for monitoring usage of your application (dashboards, APIs, features, filters, pages, departments, etc.). These are very commonly used in enterprise monitoring dashboards.


---

📊 Application Usage Monitoring KPIs

User Activity

Metric	Purpose

Daily Active Users (DAU)	How many unique users used the app today
Weekly Active Users (WAU)	Trend compared to daily usage
Monthly Active Users (MAU)	Long-term adoption
New Users vs Returning Users	Retention analysis
Peak Usage Hours	When to optimize infrastructure



---

Role-Based Usage

Metric	Purpose

Usage by Role (Manager vs Employee)	Adoption segmentation
Top Performing Departments / Teams	Who uses most
Inactive Users	Target for re-engagement or training


Example query output:

role | users
manager | 180
employee | 420


---

Feature / Page-Level Analytics

Track usage of individual pages, sections, or tabs:

Metric	Purpose

Most / Least accessed pages	Understand what users care about
Drop-off points	Find confusing UI spots
Button clicks / feature interactions	Prioritize improvements


Examples:

Dashboard → Page views

File Upload → usage count

Report Download → count by type

Time spent per page



---

Filter Usage Analytics

Metric	Purpose

Most used time filters (Day / Week / Month / Custom)	Prioritize UI / caching strategy
Usage based on date range size	Query optimization insights
Filter selections by role / department	Personalized UX decisions


Example question you can answer:

Do managers use weekly view more often?

Do employees use daily view more often?



---

API & Performance KPIs

Metric	Purpose

Requests per API per day	Load distribution
Average API response time	Performance tuning
Max response time	Detect bottlenecks
Error rate (4xx / 5xx)	Stability
Slowest pages / endpoints	Optimization focus



---

Department / Team Based KPIs

Metric	Purpose

Top departments based on dashboard hits	Adoption success areas
Least active departments	Training or rollout needed
Usage trend per team over time	Efficiency analysis



---

User Behavior Patterns

Metric	Purpose

Duration spent in application	Engagement
Session count per day per user	productivity measurement
Most common user journey / navigation path	UI optimization



---

🧠 Insight examples you’ll be able to answer

Business questions

Example Insight

“Daily active users increased 18% this week.”
“Managers use dashboards mostly between 10 AM – 12 PM.”
“Weekly filter is used 62% more than daily or monthly.”
“Team-A has 4× dashboard usage compared to Team-B.”
“Report page is least used — may need redesign.”



---

🧱 What to log in audit table

Field

user_id
role
timestamp
page / section / route
api_id / endpoint
filter_type
department or team
session_id
status / duration_ms



---

📈 How to visualize in dashboard (Power BI / Vue UI etc.)

Charts	Use

Line chart	DAU / WAU trend
Bar chart	Usage by team / department
Heatmap	Peak usage by hour & weekday
Pie chart	Filter usage
Table	Most/least accessed pages
Ranking table	Top 10 API calls



---

💡 Final Summary

Yes — tracking API & feature usage lets you compute high-value KPIs like:

Daily active users

Usage by manager vs employees

Most used pages & filters

Most active teams & departments

Performance metrics

Time-based usage trends

Behavioral patterns


This is an excellent monitoring model for backend analytics & real application improvement.


---

👇 Next?

Do you want: A) Database table design for usage logging
B) Python backend logging architecture
C) Example SQL queries for these KPIs
D) Architecture diagram with scheduler + DB + dashboard

Reply A / B / C / D / ALL 😄





Nice — clear. Below I give you a ready-to-drop-in sync implementation that matches what you described:

Request payload contains only: recommended, fd_comment (comment), and rating.

emp_id and email_id will be taken from the JWT (subject/claims) via AuthJWT.

The endpoint is sync (no async def) and uses your session_scope() context manager to insert into the EmailRecommendations table you showed earlier.

recommended accepts either boolean or string in the request and is normalized to "yes"/"no" for the DB.

On success it returns a JSON with the inserted record id and a success message.


Paste these into your project (adjust imports/paths if your project differs).


---

1) Pydantic payload — app/schema_designer.py

# app/schema_designer.py
from pydantic import BaseModel, Field
from typing import Optional, Union

class FeedbackPayload(BaseModel):
    # Accept either bool or string for convenience; we'll normalize to "yes"/"no".
    recommended: Optional[Union[bool, str]] = Field(
        ..., description="Recommendation (true/false or 'yes'/'no')"
    )
    fd_comment: Optional[str] = Field(None, description="Feedback comment")
    rating: Optional[int] = Field(None, ge=0, le=10, description="Rating (0-10)")

    class Config:
        schema_extra = {
            "example": {
                "recommended": True,
                "fd_comment": "Loved the flow",
                "rating": 5
            }
        }

> Adjust rating bounds (ge/le) to your required range; I used 0-10 as an example.




---

2) SQLAlchemy model (if you don't already have one) — app/models/email_recommendation.py

If you already have a model that matches your EmailRecommendations table from earlier messages, you can reuse it. For completeness:

# app/models/email_recommendation.py
from sqlalchemy import Column, Integer, String, DateTime, Boolean
from sqlalchemy.sql import func
from app.models.database_connectivity import BASE  # your existing BASE

class EmailRecommendations(BASE):
    __tablename__ = "EmailRecommendations"

    id = Column(Integer, primary_key=True, autoincrement=True)
    emp_id = Column(Integer, nullable=False)
    email_id = Column(String(255), nullable=False)
    recommended = Column(String(10), nullable=False)   # "yes"/"no"
    comments = Column(String(1000))
    rating = Column(Integer)
    email_sent = Column(Boolean, default=False)
    email_sent_at = Column(DateTime)
    created_at = Column(DateTime, server_default=func.getdate())
    updated_at = Column(DateTime)
    created_by = Column(String(255))
    updated_by = Column(String(255))
    is_deleted = Column(Boolean, default=False)


---

3) Route / endpoint — app/routes/feedback.py

# app/routes/feedback.py
from fastapi import APIRouter, Depends, HTTPException, status
from fastapi_jwt_auth import AuthJWT
from app.schema_designer import FeedbackPayload
from app.models.email_recommendation import EmailRecommendations
from app.models.database_connectivity import session_scope

router = APIRouter()

def _normalize_recommended(val):
    """Return 'yes' or 'no' from bool or str; raise ValueError if invalid."""
    if isinstance(val, bool):
        return "yes" if val else "no"
    if isinstance(val, str):
        s = val.strip().lower()
        if s in ("yes", "y", "true", "1"):
            return "yes"
        if s in ("no", "n", "false", "0"):
            return "no"
    raise ValueError("recommended must be a boolean or 'yes'/'no' string")

@router.post("/feedback", status_code=201)
def post_feedback(payload: FeedbackPayload, Authorize: AuthJWT = Depends()):
    # 1) Verify JWT is present and valid
    try:
        Authorize.jwt_required()
    except Exception as e:
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=str(e))

    # 2) Extract identity: try subject() then raw claims
    #    (Adapt depending on how your tokens are issued.)
    subj = Authorize.get_jwt_subject()  # typically a string subject
    raw_jwt = Authorize.get_raw_jwt()   # dict with claims

    # 3) Get emp_id and email_id from JWT claims or subject
    #    Try common claim keys; adapt to your token structure.
    emp_id = raw_jwt.get("user_id") or raw_jwt.get("emp_id")
    email_id = raw_jwt.get("email") or raw_jwt.get("email_id")

    # If subject contains a JSON-like string or id, you may need to parse it.
    # If subj is numeric id:
    if emp_id is None:
        # fallback: if subject is numeric string -> treat as emp_id
        if subj is not None and str(subj).isdigit():
            emp_id = int(subj)
    if emp_id is None or email_id is None:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Missing user identity (emp_id/email) in JWT claims"
        )

    # 4) Normalize recommended
    try:
        recommended_db = _normalize_recommended(payload.recommended)
    except ValueError as e:
        raise HTTPException(status_code=422, detail=str(e))

    # 5) Insert into DB synchronously using your session_scope
    with session_scope() as session:
        record = EmailRecommendations(
            emp_id=int(emp_id),
            email_id=str(email_id),
            recommended=recommended_db,
            comments=payload.fd_comment,
            rating=payload.rating,
            created_by=str(email_id)  # or use emp_id or subject
        )
        session.add(record)
        session.flush()  # ensure id is populated

        inserted_id = record.id

    # 6) Return success (you asked: "irrespective JSON response" — return success JSON)
    return {"status": "success", "id": inserted_id, "message": "Feedback saved"}

Notes on the endpoint:

Uses Authorize.jwt_required() to enforce authentication.

Tries to extract emp_id and email_id from JWT raw claims first (user_id, emp_id, email, email_id), then falls back to sub if numeric.

Converts recommended to "yes"/"no" for the DB.

Uses session.flush() to access record.id before commit/exit.

Returns HTTP 201 & JSON with id.



---

4) Include the router in app/main.py

# app/main.py (excerpt)
from fastapi import FastAPI
from app.routes.feedback import router as feedback_router

app = FastAPI(title="Pulse Landing Page", version="1.0.0")
app.include_router(feedback_router, prefix="/api")
# ... other middleware & routers


---

5) Example request (curl)

curl -X POST "http://localhost:8000/api/feedback" \
  -H "Authorization: Bearer <access_token>" \
  -H "Content-Type: application/json" \
  -d '{"recommended": true, "fd_comment":"Nice UI", "rating":5}'

Expected response:

{
  "status": "success",
  "id": 123,
  "message": "Feedback saved"
}


---

Extra tips / gotchas

1. Token shape: Confirm what your JWT contains. If emp_id and email are under custom claim names, update the claim extraction keys in the endpoint accordingly.


2. Type conversion: Ensure emp_id coming from JWT is convertible to int.


3. session_scope: I used your existing session_scope() context manager as previously discussed; make sure it commits on exit.


4. Validation: If recommended should be required, keep ... in Pydantic; if optional, set default; I used required (...) so the field must be present.


5. Audit fields: If you want created_by to be emp_id instead of email, change that line.


6. Race conditions: session.flush() is used to force id population; session_scope must commit afterward.




---

If you want, I can now:

adapt claim key names to exactly what your JWT contains (paste a sample decoded JWT payload), or

add an update (PUT) endpoint to update existing feedback by the same user, or

add soft-delete / list endpoints.


Which next?
__________________________________________







Here are 30 clear, manager-friendly, insight-driven questions you can use for your GNI Bot to query the Pulse Dashboard.
I’ve grouped them by theme so they are easy to plug into your bot’s logic later.


---

✅ Team Working Time & Productivity Questions

1. What is my team’s total working time for this week?


2. How much active time vs idle time has my team logged this week?


3. How much total screen lock time did my team have this week?


4. Which team member has the lowest working time this week?


5. Which team member has the highest idle time this week?


6. Show the day-wise breakdown of working, active, idle, and lock time for my team this week.


7. Compare my team’s working time for this week vs last week.


8. Which week in the past four months had the lowest working time for my team?


9. Which week in the past four months had the highest idle time?


10. Show the month-wise trend of working, active, idle, and lock time for the last four months.




---

✅ Team Comparison & Multi-Team Questions

11. Show the working and idle time comparison between all my teams.


12. Which team performed best based on active time percentage?


13. Which team had the highest screen lock duration this week?


14. Compare application usage across all my teams.


15. Which team spent the most time on non-productive applications?




---

✅ Application & Category Usage Questions

16. Which are the top 10 applications used by my team this week?


17. How much time did my team spend on productive vs non-productive categories this week?


18. Show the category-wise usage trend for the last 30 days.


19. Which applications caused the highest idle time?


20. Which team member spent the most time on unnecessary or flagged applications?


21. Show productive vs non-productive application usage for each day this week.




---

✅ Keystroke & Mouse Activity Questions

22. What is my team’s hourly keystroke activity for this week?


23. How many mouse clicks did my team perform per hour today?


24. How many times did my team use error-handling keys like backspace, delete, and escape today?


25. Show copy and paste usage trends for my team this week.


26. Show enter-key usage per hour for each team member.


27. Which team member has unusual keystroke patterns compared to the team average?




---

✅ System Performance & Resource Questions

28. Which team member’s system has the highest RAM usage this week?


29. Compare CPU, RAM, and disk usage across all team members.


30. Show the trend of active vs window-focus vs idle time for each team member.




---

Want more?

I can generate 40+, 50+, or even categorized templates depending on what your GNI bot expects (like short questions, long questions, or API-friendly versions).





Perfect — here’s the revised and polished version of your incident report with all your updates applied, in a professional and production-ready format:


---

Incident Summary

During the login process to the Pulse Dashboard, the application incorrectly referenced the gold_dashboard.analytics.LoginLogout_BKP table instead of the main gold_analytics.empLoginLogout table while fetching the PulseAgentCount.
This misconfiguration led to incorrect or missing data being fetched during user login in the production environment, resulting in login failures.


---

Impact Analysis

Duration: From initial deployment of the incorrect code until the hotfix was applied.

Affected Users: All users attempting to log in during the affected period.

System Behavior: The application attempted to fetch data from the backup table, which does not exist in the production environment, causing login failures.

Detection: The issue was identified through user reports and validation checks by the support team.



---

Business Impact

Users whose tokens had expired after 24 hours experienced login failures.

Temporary restriction in user workflow and operational tracking for reactivations of expired sessions.

Partial disruption in Pulse Dashboard accessibility and reporting during the incident window.



---

Technical Impact

The production codebase referenced the backup table (gold_dashboard.analytics.LoginLogout_BKP) instead of the main table (gold_analytics.empLoginLogout) due to a merge issue between DEV, UAT, and PROD branches.

The environment code versions were not aligned, leading to inconsistent database references.



---

Root Cause

Code inconsistency across environments (DEV, UAT, and PROD).

During a merge from DEV to PROD, an outdated configuration referencing the backup table was unintentionally deployed.

Lack of pre-deployment validation for database table references in configuration or code review.



---

Resolution Followed

1. Immediately restricted affected users to prevent further login failures.


2. Identified and confirmed the incorrect table reference in the production code.


3. Updated the configuration to point to the correct gold_analytics.empLoginLogout table.


4. Performed an emergency hotfix deployment to production.


5. Verified functionality with multiple production users, including managers and employees.


6. Monitored logs and user sessions post-fix to confirm successful resolution and stability.




---

Preventive Measures

Ensure environment code parity before deployments (DEV, UAT, PROD synchronization).

Implement a pre-deployment validation checklist for database table references and configuration values.

Introduce automated regression and configuration consistency checks.

Strengthen merge and code review processes to detect outdated references before release.

Schedule periodic environment alignment reviews between development, QA, and release teams.



---

Would you like me to make a shorter version (1–2 paragraphs) for including as a Jira comment, while keeping this as the full RCA document?






Root Cause Analysis / Incident Explanation:
During the Login-Logout process, the application was expected to fetch the PulseAgentCount from the empLoginLogout table. However, due to a branch version mismatch, the deployed code in PROD was referencing the PulseLoginLogout_BKP (backup) table instead of the main table.

This occurred because different environments (UAT, DEV, and PROD) had inconsistent versions of the codebase. While merging the DEV branch into PROD, the reference to the backup table was unintentionally included. As a result, the application retrieved incorrect data from the backup table.

Resolution:
Once the issue was identified, we immediately disabled affected users from accessing the Login module to prevent further data inconsistencies. The code was then hotfixed and redeployed to reference the correct empLoginLogout table.

Verification:
Post-deployment, validation was performed with production users, including managers and three employees, confirming that the Login-Logout functionality and PulseAgentCount values were working correctly.

Current Status:
✅ Issue resolved
✅ Functionality verified in PROD
✅ Code base aligned across environments




Here’s a clean and well-structured draft of your concept, written as a clear functional and technical explanation.


---

Feature: Manager-Based Dashboard Access Control

Objective

Restrict dashboard access for managers based on the number of employees actively running the agent in the past 30 days. This ensures that only managers with a minimum level of agent activity within their team can access the dashboard.


---

Data Source: LoginLogout Table

Column Name	Description

EmployeeID	Unique identifier of the employee running the agent
ManagerID	Unique identifier of the employee’s reporting manager
ShiftDate	Date on which the employee ran the agent
LoginTime	Timestamp of agent start/login
LogoutTime	Timestamp of agent stop/logout


> Each record represents an instance of an employee running the agent on a given day.




---

Logic Flow

1. Add Manager Reference

Include a ManagerID column in the LoginLogout table.

On each agent run (login event), determine the employee’s manager and insert that ManagerID along with the record.



2. Identify Active Employees

For each manager, identify all unique employees (EmployeeID) who have at least one LoginLogout entry in the past 30 days.



3. Calculate Agent Run Count

For each manager:

Count how many distinct employees under them have logged agent runs within the past 30 days.

Example:

SELECT 
    ManagerID, 
    COUNT(DISTINCT EmployeeID) AS ActiveEmployeeCount
FROM LoginLogout
WHERE ShiftDate >= CURDATE() - INTERVAL 30 DAY
GROUP BY ManagerID;




4. Apply Access Restriction

Define the minimum active employee threshold: 4 employees.

If a manager’s ActiveEmployeeCount < 4, block their access to the dashboard.

If the count is 4 or more, grant access as usual.



5. Notify Restricted Managers

When a manager with insufficient active employees attempts to access the dashboard:

Display a message such as:

> “Dashboard access is temporarily restricted. Fewer than 4 employees in your team have been active in the past 30 days.”








---

Example Scenario

ManagerID	Employee Count (Last 30 Days)	Access

MGR101	8	✅ Granted
MGR102	3	❌ Blocked
MGR103	5	✅ Granted



---

Implementation Notes

This logic can be implemented:

In backend (Python/Flask) as part of the dashboard API route — check access before returning data.

In frontend (Vue.js) — handle the access flag in API response and display a notification popup if blocked.


The 30-day check should be rolling, always calculated from the current system date.

To optimize performance, consider caching results daily since the data changes minimally.



---

Would you like me to include a sample Flask backend implementation (SQLAlchemy + Flask route) that performs this check and returns an access flag to the dashboard frontend?




Perfect — that adds an important layer of client-side state handling. Here’s a revised and properly structured draft that incorporates your cookie-based logic for maintaining and validating counts.


---

Feature: Dashboard Access Control Based on Active Employee Count (Cookie-Based Logic)

Objective

Restrict dashboard access for managers based on the number of employees actively running the agent within the past 30 days.
Access is dynamically checked and maintained using cookies to reduce database load and ensure smooth user experience.


---

Data Source: LoginLogout Table

Column Name	Description

EmployeeID	Unique identifier of the employee running the agent
ManagerID	Manager to whom the employee reports
ShiftDate	Date when the employee ran the agent
LoginTime	Timestamp when the agent was started
LogoutTime	Timestamp when the agent was stopped


Each entry indicates that the agent was active for that employee on the specified date.


---

System Flow

1. Daily Cookie Validation

On each day when a manager logs in or accesses the dashboard:

The system checks for two cookies:

HRCount → total number of employees mapped under the manager.

PulseAgentCount → number of employees who ran the agent in the last 30 days.


If the cookies do not exist or are older than 24 hours, they are refreshed.




---

2. Cookie Refresh Logic

When a cookie refresh is triggered:

1. Backend query runs to compute the agent count for the manager:

SELECT 
    ManagerID, 
    COUNT(DISTINCT EmployeeID) AS ActiveEmployeeCount
FROM LoginLogout
WHERE ShiftDate >= CURDATE() - INTERVAL 30 DAY
GROUP BY ManagerID;


2. The backend returns:

HRCount: total employees under the manager (from HR or mapping table)

PulseAgentCount: number of employees who ran the agent in the last 30 days



3. The frontend updates cookies:

document.cookie = `HRCount=${hrCount}; max-age=86400`; // 1 day
document.cookie = `PulseAgentCount=${activeCount}; max-age=86400`;





---

3. Access Decision

On dashboard load or login click:

The system compares the stored cookie values:

if (parseInt(getCookie('PulseAgentCount')) < 4) {
    showAccessBlockedMessage();
    redirectToLoginPage();
} else {
    grantDashboardAccess();
}


If the PulseAgentCount is less than 4, access is blocked, and the manager is redirected to the login page with a notification.



---

4. Notification Message

When access is restricted:

> “Access Denied: Dashboard is unavailable because fewer than 4 employees in your team have been active (agent run) in the past 30 days.”




---

5. Example Flow

1. Manager logs in → system checks cookies


2. Cookies missing or outdated → backend query triggered, cookies updated


3. Counts evaluated:

HRCount = 10

PulseAgentCount = 3



4. Access Result: Blocked


5. Next day, if new employees run the agent → count updates, access unlocked.




---

6. Benefits of Cookie-Based Approach

✅ Reduces frequent backend calls
✅ Provides daily snapshot for consistent behavior
✅ Keeps logic lightweight on the client side
✅ Enables smooth redirection and notification handling


---

Would you like me to extend this draft to show a sample implementation — for example,

a Flask route (/check-access) that returns HR and agent counts, and

a corresponding Vue.js or JavaScript snippet that updates cookies and checks access?





Hi [Team/Name],

Currently, our backend web application connects to the SQL Warehouse to execute all dashboard queries. These queries involve operations like joins and aggregations, and the processed data is stored in Delta Lake tables.

However, since all queries are executed through the SQL Warehouse, the response time is sometimes delayed — especially when the clusters are fully utilized or take time to scale up.

I wanted to check if there’s a way to directly access and query the Delta tables (bypassing SQL Warehouse) to achieve faster, more transactional-style query execution — similar to how traditional RDBMS systems operate.

In short:

Can we query the Delta tables directly without going through SQL Warehouse?

If yes, what would be the recommended approach or architecture to support such access efficiently and securely?


Appreciate your inputs on this.

Best regards,
[Your Name]



I wanted to share some context regarding the current issue and the upcoming change request scheduled for Monday or Tuesday.

At present, the login-logout recalculation is based on a delete and insert process. This approach leads to inaccurate data being returned if queries are made while the job is still running. The current process recalculates login-logout data for employees whose records fall within the past 72 hours, regardless of whether their actual login or logout times have changed. This is especially problematic with shift changes.

In the upcoming CR, this will be improved. The login-logout recalculation will occur only when there is an actual change in the employee’s login or logout time. Additionally, the process will be handled using an upsert approach instead of delete and insert. This means new data will be appended or updated without removing existing records, ensuring data consistency and accuracy.

With these changes, the issue of querying data during job execution will no longer occur.

Let me know if you have any questions.

import os
import logging
from dotenv import load_dotenv
import urllib.parse
import threading
from databricks import sql
from databricks.sdk.core import Config, oauth_service_principal
from queue import Queue
import time
from contextlib import contextmanager
from typing import Dict, Any
import json
from fastapi.responses import JSONResponse

# Load environment variables from .env file
load_dotenv()

# Configure Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Connection Pool Configuration
POOL_SIZE = 120  # Max number of connections in the pool
CONNECTION_TTL = 1800  # 30 minutes in seconds
connection_pool = Queue(maxsize=POOL_SIZE)
pool_lock = threading.Lock()

# Connection monitoring class
class ConnectionMonitor:
    def __init__(self):
        self.active_connections = 0
        self.total_connections_created = 0
        self.connections_reused = 0
        self.connections_closed = 0
        self.monitor_lock = threading.Lock()
    
    def connection_created(self):
        with self.monitor_lock:
            self.active_connections += 1
            self.total_connections_created += 1
            logger.info(f"New connection created. Active: {self.active_connections}, Total created: {self.total_connections_created}")
    
    def connection_reused(self):
        with self.monitor_lock:
            self.connections_reused += 1
            logger.info(f"Connection reused from pool. Reused count: {self.connections_reused}")
    
    def connection_returned(self):
        with self.monitor_lock:
            logger.info(f"Connection returned to pool. Pool size: {connection_pool.qsize()}")
    
    def connection_closed(self, reason: str):
        with self.monitor_lock:
            self.active_connections -= 1
            self.connections_closed += 1
            logger.info(f"Connection closed ({reason}). Active: {self.active_connections}, Closed: {self.connections_closed}")
    
    def get_stats(self) -> Dict[str, Any]:
        with self.monitor_lock:
            return {
                "active_connections": self.active_connections,
                "total_connections_created": self.total_connections_created,
                "connections_reused": self.connections_reused,
                "connections_closed": self.connections_closed,
                "pool_size": connection_pool.qsize(),
                "pool_capacity": POOL_SIZE
            }

# Initialize connection monitor
connection_monitor = ConnectionMonitor()

# Function to create a new Databricks connection with logging
def create_connection():
    try:
        server_hostname = os.getenv('DATABRICKS_SERVER_HOST')
        config = Config(
            host=f"https://{server_hostname}",
            client_id=os.getenv('DATABRICKS_CLIENT_ID'),
            client_secret=os.getenv('DATABRICKS_CLIENT_SECRET')
        )
        
        logger.info(f"Creating new connection to {server_hostname}")
        conn = sql.connect(
            server_hostname=server_hostname,
            http_path=os.getenv('DATABRICKS_HTTP_PATH'),
            credentials_provider=lambda: oauth_service_principal(config)
        )
        
        connection_monitor.connection_created()
        return conn
        
    except Exception as e:
        logger.error(f"Failed to create connection: {str(e)}")
        raise

# Context Manager for Connection Pooling with enhanced logging
class DatabricksSession:
    def __enter__(self):
        now = time.time()
        connection_obtained = False
        
        with pool_lock:
            # Try to get a connection from the pool
            while not connection_pool.empty() and not connection_obtained:
                conn, last_used = connection_pool.get()
                if now - last_used < CONNECTION_TTL:
                    self.conn = conn
                    connection_obtained = True
                    connection_monitor.connection_reused()
                    logger.info(f"Reused connection from pool. Time since last use: {now - last_used:.2f}s")
                else:
                    try:
                        conn.close()
                        connection_monitor.connection_closed("stale")
                        logger.info("Closed stale connection from pool")
                    except Exception as e:
                        logger.error(f"Error closing stale connection: {e}")
            
            # If no valid connection found in pool, create a new one
            if not connection_obtained:
                self.conn = create_connection()
                logger.info("Created new connection (pool empty or no valid connections)")
        
        logger.info(f"Connection obtained. Pool status: {connection_pool.qsize()}/{POOL_SIZE}")
        return self.conn

    def __exit__(self, exc_type, exc_val, exc_tb):
        try:
            # Check if connection is still valid before returning to pool
            if hasattr(self, 'conn') and self.conn:
                if exc_type is not None:
                    # If there was an exception, don't return to pool
                    logger.warning(f"Exception occurred in session, closing connection: {exc_val}")
                    self.conn.close()
                    connection_monitor.connection_closed("exception")
                else:
                    # Return connection to pool if it's not full
                    with pool_lock:
                        if connection_pool.full():
                            logger.warning("Connection pool full, closing connection")
                            self.conn.close()
                            connection_monitor.connection_closed("pool_full")
                        else:
                            connection_pool.put((self.conn, time.time()))
                            connection_monitor.connection_returned()
                            logger.debug(f"Connection returned to pool. New size: {connection_pool.qsize()}")
            else:
                logger.warning("No valid connection to return to pool")
                
        except Exception as e:
            logger.error(f"Error handling connection cleanup: {e}")
            try:
                if hasattr(self, 'conn') and self.conn:
                    self.conn.close()
                    connection_monitor.connection_closed("cleanup_error")
            except Exception as close_error:
                logger.error(f"Error closing connection during cleanup: {close_error}")

# Function to execute query with logging
def execute_query(conn, query: str):
    try:
        logger.info(f"Executing query: {query[:100]}...")  # Log first 100 chars
        start_time = time.time()
        
        with conn.cursor() as cursor:
            cursor.execute(query)
            result = cursor.fetchall()
        
        execution_time = time.time() - start_time
        logger.info(f"Query executed successfully in {execution_time:.2f}s. Rows returned: {len(result)}")
        return result
        
    except Exception as e:
        logger.error(f"Query execution failed: {str(e)}")
        raise

# Function to get connection pool statistics
def get_connection_stats() -> Dict[str, Any]:
    stats = connection_monitor.get_stats()
    logger.info(f"Connection pool statistics: {stats}")
    return stats

# Your existing usage pattern remains the same
def process_query(query: str):
    try:
        with DatabricksSession() as conn:
            data = execute_query(conn, query)
            
            if data and data[0]:
                json_string = data[0][0]
                json_data = json.loads(json_string)
            else:
                json_data = {"empSummary": {}, "teamSummary": {}, "empCount": None}
        
        # Log connection stats after operation
        stats = get_connection_stats()
        logger.info(f"Operation completed. Connection stats: {stats}")
        
        if not json_data:
            return JSONResponse(status_code=404, content={'message': "Execution Not Success"})
        return JSONResponse(status_code=200, content=json_data)
        
    except Exception as e:
        logger.error(f"Error in process_query: {str(e)}")
        return JSONResponse(status_code=500, content={'message': f"Internal Server Error: {str(e)}"})

# Optional: Add periodic logging of connection stats
def log_periodic_stats(interval: int = 300):  # 5 minutes
    """Periodically log connection pool statistics"""
    while True:
        time.sleep(interval)
        stats = get_connection_stats()
        logger.info(f"Periodic connection pool report: {stats}")

# Start periodic logging in a background thread (optional)
# stats_thread = threading.Thread(target=log_periodic_stats, daemon=True)
# stats_thread.start()







result_df = classified_df.groupBy("emp_id", "cal_date").agg(
    # Current login: Minimum start_time within current window and app_name != "Window Lock"
    F.min(F.when(
        (F.col("is_current_login")) & (F.col("app_name") != "WindowLock"),
        F.col("start_time")
    )).alias("current_login"),
    
    # Last activity: Maximum start_time within current window BUT exclude WindowLock
    F.max(F.when(
        (F.col("start_time") >= F.col("current_window_start")) &
        (F.col("start_time") <= F.col("current_window_end")) &
        (F.col("app_name") != "WindowLock"),  # EXCLUDE WindowLock
        F.col("start_time")
    )).alias("last_activity"),
    
    # ... keep other aggregations as they are
)

# For mouse/keyboard data, we already exclude WindowLock by nature
result_mkdf = classified_mkdf.groupBy("emp_id", "cal_date").agg(
    F.max(F.when(
        (F.col("event_time") >= F.col("current_window_start")) &
        (F.col("event_time") <= F.col("current_window_end")),
        F.col("event_time")
    )).alias("last_mk_activity"),
    # ... other aggregations
)




final_result = result_df.withColumn(
    "emp_login_time",
    F.when(
        F.col("is_week_off"),
        F.coalesce(F.col("week_off_login"), F.col("last_activity"))
    ).otherwise(
        F.coalesce(F.col("current_login"), F.col("last_activity"))
    )
).withColumn(
    "emp_logout_time",
    F.when(
        F.col("is_week_off"),
        F.coalesce(F.col("week_off_logout"), F.col("last_activity"))
    ).otherwise(
        # PREFER mouse/keyboard logout over app activities (which excludes WindowLock)
        F.coalesce(F.col("last_mk_activity"), F.col("last_activity"), F.col("current_login"))
    )
)




# Single combined merge with optimized logic
spark.sql("""
MERGE INTO gold_dashboard.analytics_emp_login_logout AS target
USING (
    -- Combined source data with operation type indicator
    SELECT ..., 1 as operation_type FROM temp_filtered_login_logout
    UNION ALL
    SELECT ..., 2 as operation_type FROM temp_prev_day_updates
) AS source
ON (
    -- Combined join conditions
    (operation_type = 1 AND target.EMP_ID = source.emp_id AND target.SHIFT_DATE = source.shift_date)
    OR
    (operation_type = 2 AND target.EMP_ID = source.update_emp_id AND target.SHIFT_DATE = source.update_date)
)
WHEN MATCHED AND source.operation_type = 1 AND (
    -- Only update if source values are better
    (unix_timestamp(source.emp_login_time) < unix_timestamp(target.EMP_LOGIN_TIME)) OR
    (unix_timestamp(source.emp_logout_time) > unix_timestamp(target.EMP_LOGOUT_TIME))
) THEN
    UPDATE SET 
        target.EMP_LOGIN_TIME = CASE 
            WHEN unix_timestamp(source.emp_login_time) < unix_timestamp(target.EMP_LOGIN_TIME)
            THEN source.emp_login_time 
            ELSE target.EMP_LOGIN_TIME 
        END,
        target.EMP_LOGOUT_TIME = CASE 
            WHEN unix_timestamp(source.emp_logout_time) > unix_timestamp(target.EMP_LOGOUT_TIME)
            THEN source.emp_logout_time 
            ELSE target.EMP_LOGOUT_TIME 
        END
WHEN MATCHED AND source.operation_type = 2 AND (
    -- Only update previous logout if source is later
    unix_timestamp(source.new_logout_time) > unix_timestamp(target.EMP_LOGOUT_TIME)
) THEN
    UPDATE SET target.EMP_LOGOUT_TIME = source.new_logout_time
WHEN NOT MATCHED AND source.operation_type = 1 THEN
    INSERT (EMP_ID, START_TIME, END_TIME, EMP_LOGIN_TIME, EMP_LOGOUT_TIME, SHIFT_DATE)
    VALUES (source.emp_id, source.start_time, source.end_time,
            source.emp_login_time, source.emp_logout_time, source.shift_date)
""")




WITH FilteredEmployees AS (
  SELECT
    emplid AS emp_id,
    COALESCE(country, 'Default') AS country
  FROM inbound.hr_employee_central
  WHERE func_mar_id = 43560069
    AND (TERMINATION_DT > CURRENT_TIMESTAMP() OR TERMINATION_DT IS NULL)
),

EmployeeShiftsWithConfig AS (
  SELECT
    l.emp_id,
    l.shift_date,
    TO_TIMESTAMP(l.start_time, 'yyyy-MM-dd HH:mm:ss') AS start_time,
    TO_TIMESTAMP(l.end_time, 'yyyy-MM-dd HH:mm:ss') AS end_time,
    TO_TIMESTAMP(l.emp_login_time, 'yyyy-MM-dd HH:mm:ss') AS emp_login_time,
    TO_TIMESTAMP(l.emp_logout_time, 'yyyy-MM-dd HH:mm:ss') AS emp_logout_time,
    (UNIX_TIMESTAMP(l.end_time) - UNIX_TIMESTAMP(l.start_time)) / 3600.0 AS pulse_shift_time,
    (COALESCE(o.shift_dur_hrs, 8) * (UNIX_TIMESTAMP(l.end_time) - UNIX_TIMESTAMP(l.start_time)) / 3600.0 / COALESCE(o.shift_length_hrs, 9)) AS adjusted_active_time,
    (COALESCE(o.break_dur_hrs, 1) * (UNIX_TIMESTAMP(l.end_time) - UNIX_TIMESTAMP(l.start_time)) / 3600.0 / COALESCE(o.shift_length_hrs, 9)) AS adjusted_lock_time
  FROM gold_dashboard.analytics_emp_login_logout l
  JOIN FilteredEmployees e ON l.emp_id = e.emp_id
  LEFT JOIN inbound.countrywise_shift o ON e.country = o.country
  WHERE (l.shift_date BETWEEN '{start_date}' AND '{end_date}' OR l.shift_date BETWEEN '{prev_start_date}' AND '{prev_end_date}')
    AND l.is_weekoff = false
    AND l.emp_login_time IS NOT NULL
    AND l.emp_logout_time IS NOT NULL
),

FilteredAppInfo AS (
  SELECT
    i.emp_id,
    i.cal_date AS shift_date,
    TO_TIMESTAMP(CONCAT(i.cal_date, ' ', SUBSTRING(i.interval, 1, 5), ':00'), 'yyyy-MM-dd HH:mm:ss') AS interval_start,
    TO_TIMESTAMP(CONCAT(i.cal_date, ' ', SUBSTRING(i.interval, 7, 5), ':00'), 'yyyy-MM-dd HH:mm:ss') AS interval_end,
    i.total_time_spent_active AS total_time_active,
    i.total_time_spent_idle AS total_time_idle,
    i.window_lock_time
  FROM gold_dashboard.analytics_emp_app_info i
  JOIN FilteredEmployees e ON i.emp_id = e.emp_id
  WHERE (i.cal_date BETWEEN '{start_date}' AND '{end_date}' OR i.cal_date BETWEEN '{prev_start_date}' AND '{prev_end_date}')
),

ActivityWithShiftBounds AS (
  SELECT
    i.emp_id,
    i.shift_date,
    s.start_time,
    s.end_time,
    i.interval_start,
    i.interval_end,
    s.emp_login_time,
    s.emp_logout_time,
    i.total_time_active,
    i.total_time_idle,
    i.window_lock_time,
    GREATEST(i.interval_start, s.start_time) AS overlap_start,
    LEAST(i.interval_end, s.end_time) AS overlap_end,
    GREATEST(i.interval_start, s.emp_login_time) AS activity_start,
    LEAST(i.interval_end, s.emp_logout_time) AS activity_end
  FROM FilteredAppInfo i
  JOIN EmployeeShiftsWithConfig s ON i.emp_id = s.emp_id AND i.shift_date = s.shift_date
  WHERE s.emp_login_time < i.interval_end
    AND s.emp_logout_time > i.interval_start
),

ActivityTimeAllocation AS (
  SELECT
    emp_id,
    shift_date,
    total_time_active,
    total_time_idle,
    window_lock_time,
    start_time,
    end_time,
    
    -- Calculate seconds within shift
    CASE 
      WHEN activity_start >= end_time OR activity_end <= start_time THEN 0
      WHEN activity_start >= start_time AND activity_end <= end_time THEN
        UNIX_TIMESTAMP(activity_end) - UNIX_TIMESTAMP(activity_start)
      ELSE
        UNIX_TIMESTAMP(LEAST(activity_end, end_time)) - UNIX_TIMESTAMP(GREATEST(activity_start, start_time))
    END AS seconds_within_shift,
    
    -- Calculate seconds outside shift
    CASE 
      WHEN activity_start >= end_time OR activity_end <= start_time THEN
        UNIX_TIMESTAMP(activity_end) - UNIX_TIMESTAMP(activity_start)
      WHEN activity_start >= start_time AND activity_end <= end_time THEN 0
      ELSE
        (UNIX_TIMESTAMP(activity_end) - UNIX_TIMESTAMP(activity_start)) - 
        (UNIX_TIMESTAMP(LEAST(activity_end, end_time)) - UNIX_TIMESTAMP(GREATEST(activity_start, start_time)))
    END AS seconds_outside_shift
  FROM ActivityWithShiftBounds
  WHERE activity_start < activity_end
),

CurrentShiftData AS (
  SELECT
    a.emp_id,
    a.shift_date,
    
    -- Total metrics
    SUM(total_time_active) AS total_active_time,
    SUM(total_time_idle) AS total_idle_time,
    SUM(window_lock_time) AS total_lock_time,
    SUM(total_time_active + total_time_idle + window_lock_time) AS total_work_time,
    
    -- Within shift allocation
    SUM(
      total_time_active * (seconds_within_shift / NULLIF((seconds_within_shift + seconds_outside_shift), 0))
    ) AS active_within_shift,
    
    SUM(
      total_time_idle * (seconds_within_shift / NULLIF((seconds_within_shift + seconds_outside_shift), 0))
    ) AS idle_within_shift,
    
    SUM(
      window_lock_time * (seconds_within_shift / NULLIF((seconds_within_shift + seconds_outside_shift), 0))
    ) AS lock_within_shift,
    
    -- Outside shift allocation
    SUM(
      total_time_active * (seconds_outside_shift / NULLIF((seconds_within_shift + seconds_outside_shift), 0))
    ) AS active_outside_shift,
    
    SUM(
      total_time_idle * (seconds_outside_shift / NULLIF((seconds_within_shift + seconds_outside_shift), 0))
    ) AS idle_outside_shift,
    
    -- Shift configuration
    MAX(s.pulse_shift_time) AS pulse_shift_time,
    MAX(s.adjusted_active_time) AS adjusted_active_time,
    MAX(s.adjusted_lock_time) AS adjusted_lock_time,
    
    -- Period indicator
    CASE 
      WHEN a.shift_date BETWEEN '{start_date}' AND '{end_date}' THEN 'current'
      WHEN a.shift_date BETWEEN '{prev_start_date}' AND '{prev_end_date}' THEN 'previous'
    END AS period
    
  FROM ActivityTimeAllocation a
  JOIN EmployeeShiftsWithConfig s ON a.emp_id = s.emp_id AND a.shift_date = s.shift_date
  GROUP BY a.emp_id, a.shift_date, 
    CASE 
      WHEN a.shift_date BETWEEN '{start_date}' AND '{end_date}' THEN 'current'
      WHEN a.shift_date BETWEEN '{prev_start_date}' AND '{prev_end_date}' THEN 'previous'
    END
),

PeriodAggregates AS (
  SELECT
    period,
    AVG(total_work_time) AS avg_total_work_time,
    AVG(total_active_time) AS avg_total_active_time,
    AVG(total_idle_time) AS avg_total_idle_time,
    AVG(total_lock_time) AS avg_total_lock_time,
    AVG(active_within_shift + idle_within_shift + lock_within_shift) AS avg_total_work_time_within_shift,
    AVG(active_outside_shift + idle_outside_shift) AS avg_total_work_time_outside_shift,
    AVG(pulse_shift_time * 3600) AS avg_pulse_shift_time_seconds,
    AVG(adjusted_active_time * 3600) AS avg_adjusted_active_time_seconds,
    AVG(adjusted_lock_time * 3600) AS avg_adjusted_lock_time_seconds
  FROM CurrentShiftData
  WHERE period IN ('current', 'previous')
  GROUP BY period
),

CurrentPeriod AS (
  SELECT * FROM PeriodAggregates WHERE period = 'current'
),

PreviousPeriod AS (
  SELECT * FROM PeriodAggregates WHERE period = 'previous'
)

SELECT
  -- Current period metrics (use 0 if no data exists)
  COALESCE(curr.avg_total_work_time, 0) AS current_total_work_time,
  COALESCE(curr.avg_total_active_time, 0) AS current_total_active_time,
  COALESCE(curr.avg_total_idle_time, 0) AS current_total_idle_time,
  COALESCE(curr.avg_total_lock_time, 0) AS current_total_lock_time,
  COALESCE(curr.avg_total_work_time_within_shift, 0) AS current_total_work_time_within_shift,
  COALESCE(curr.avg_total_work_time_outside_shift, 0) AS current_total_work_time_outside_shift,
  
  -- Schedule comparisons (only calculate if current data exists)
  CASE 
    WHEN curr.avg_total_work_time IS NOT NULL 
    THEN COALESCE(curr.avg_total_work_time, 0) - COALESCE(curr.avg_pulse_shift_time_seconds, 0)
    ELSE 0 
  END AS total_work_shift_schedule,
  
  CASE 
    WHEN curr.avg_total_work_time IS NULL THEN 'No Data'
    WHEN COALESCE(curr.avg_total_work_time, 0) - COALESCE(curr.avg_pulse_shift_time_seconds, 0) > 0 THEN 'Up'
    WHEN COALESCE(curr.avg_total_work_time, 0) - COALESCE(curr.avg_pulse_shift_time_seconds, 0) < 0 THEN 'Down'
    ELSE 'No Change'
  END AS total_work_shift_schedule_trend,
  
  CASE 
    WHEN curr.avg_total_active_time IS NOT NULL 
    THEN COALESCE(curr.avg_total_active_time, 0) - COALESCE(curr.avg_adjusted_active_time_seconds, 0)
    ELSE 0 
  END AS total_active_shift_schedule,
  
  CASE 
    WHEN curr.avg_total_active_time IS NULL THEN 'No Data'
    WHEN COALESCE(curr.avg_total_active_time, 0) - COALESCE(curr.avg_adjusted_active_time_seconds, 0) > 0 THEN 'Up'
    WHEN COALESCE(curr.avg_total_active_time, 0) - COALESCE(curr.avg_adjusted_active_time_seconds, 0) < 0 THEN 'Down'
    ELSE 'No Change'
  END AS total_active_shift_schedule_trend,
  
  CASE 
    WHEN curr.avg_total_lock_time IS NOT NULL 
    THEN COALESCE(curr.avg_total_lock_time, 0) - COALESCE(curr.avg_adjusted_lock_time_seconds, 0)
    ELSE 0 
  END AS total_lock_shift_schedule,
  
  CASE 
    WHEN curr.avg_total_lock_time IS NULL THEN 'No Data'
    WHEN COALESCE(curr.avg_total_lock_time, 0) - COALESCE(curr.avg_adjusted_lock_time_seconds, 0) > 0 THEN 'Up'
    WHEN COALESCE(curr.avg_total_lock_time, 0) - COALESCE(curr.avg_adjusted_lock_time_seconds, 0) < 0 THEN 'Down'
    ELSE 'No Change'
  END AS total_lock_shift_schedule_trend,
  
  -- Previous period metrics (use 0 if no data exists)
  COALESCE(prev.avg_total_work_time, 0) AS prev_total_work_time,
  COALESCE(prev.avg_total_active_time, 0) AS prev_total_active_time,
  COALESCE(prev.avg_total_idle_time, 0) AS prev_total_idle_time,
  COALESCE(prev.avg_total_lock_time, 0) AS prev_total_lock_time,
  
  -- Period comparison trends (handle missing data scenarios)
  CASE 
    WHEN curr.avg_total_work_time IS NULL OR prev.avg_total_work_time IS NULL THEN 'No Comparison Data'
    WHEN curr.avg_total_work_time > prev.avg_total_work_time THEN 'Up'
    WHEN curr.avg_total_work_time < prev.avg_total_work_time THEN 'Down'
    ELSE 'No Change'
  END AS total_work_trend,
  
  CASE 
    WHEN curr.avg_total_work_time IS NULL OR prev.avg_total_work_time IS NULL OR prev.avg_total_work_time = 0 THEN 0
    ELSE ROUND(((curr.avg_total_work_time - prev.avg_total_work_time) / prev.avg_total_work_time) * 100, 2)
  END AS total_work_percentage_change,
  
  CASE 
    WHEN curr.avg_total_active_time IS NULL OR prev.avg_total_active_time IS NULL THEN 'No Comparison Data'
    WHEN curr.avg_total_active_time > prev.avg_total_active_time THEN 'Up'
    WHEN curr.avg_total_active_time < prev.avg_total_active_time THEN 'Down'
    ELSE 'No Change'
  END AS total_active_trend,
  
  CASE 
    WHEN curr.avg_total_active_time IS NULL OR prev.avg_total_active_time IS NULL OR prev.avg_total_active_time = 0 THEN 0
    ELSE ROUND(((curr.avg_total_active_time - prev.avg_total_active_time) / prev.avg_total_active_time) * 100, 2)
  END AS total_active_percentage_change,
  
  CASE 
    WHEN curr.avg_total_idle_time IS NULL OR prev.avg_total_idle_time IS NULL THEN 'No Comparison Data'
    WHEN curr.avg_total_idle_time > prev.avg_total_idle_time THEN 'Up'
    WHEN curr.avg_total_idle_time < prev.avg_total_idle_time THEN 'Down'
    ELSE 'No Change'
  END AS total_idle_trend,
  
  CASE 
    WHEN curr.avg_total_idle_time IS NULL OR prev.avg_total_idle_time IS NULL OR prev.avg_total_idle_time = 0 THEN 0
    ELSE ROUND(((curr.avg_total_idle_time - prev.avg_total_idle_time) / prev.avg_total_idle_time) * 100, 2)
  END AS total_idle_percentage_change,
  
  CASE 
    WHEN curr.avg_total_lock_time IS NULL OR prev.avg_total_lock_time IS NULL THEN 'No Comparison Data'
    WHEN curr.avg_total_lock_time > prev.avg_total_lock_time THEN 'Up'
    WHEN curr.avg_total_lock_time < prev.avg_total_lock_time THEN 'Down'
    ELSE 'No Change'
  END AS total_lock_trend,
  
  CASE 
    WHEN curr.avg_total_lock_time IS NULL OR prev.avg_total_lock_time IS NULL OR prev.avg_total_lock_time = 0 THEN 0
    ELSE ROUND(((curr.avg_total_lock_time - prev.avg_total_lock_time) / prev.avg_total_lock_time) * 100, 2)
  END AS total_lock_percentage_change
  
FROM 
  (SELECT 1 AS join_key) AS dummy
LEFT JOIN CurrentPeriod curr ON 1=1
LEFT JOIN PreviousPeriod prev ON 1=1






WITH FilteredEmployees AS (
  SELECT
    emplid AS emp_id,
    COALESCE(country, 'Default') AS country
  FROM inbound.hr_employee_central
  WHERE func_mar_id = 43560069
    AND (TERMINATION_DT > CURRENT_TIMESTAMP() OR TERMINATION_DT IS NULL)
),

EmployeeShiftsWithConfig AS (
  SELECT
    l.emp_id,
    l.shift_date,
    TO_TIMESTAMP(l.start_time, 'yyyy-MM-dd HH:mm:ss') AS start_time,
    TO_TIMESTAMP(l.end_time, 'yyyy-MM-dd HH:mm:ss') AS end_time,
    TO_TIMESTAMP(l.emp_login_time, 'yyyy-MM-dd HH:mm:ss') AS emp_login_time,
    TO_TIMESTAMP(l.emp_logout_time, 'yyyy-MM-dd HH:mm:ss') AS emp_logout_time,
    (UNIX_TIMESTAMP(l.end_time) - UNIX_TIMESTAMP(l.start_time)) / 3600.0 AS pulse_shift_time,
    (COALESCE(o.shift_dur_hrs, 8) * (UNIX_TIMESTAMP(l.end_time) - UNIX_TIMESTAMP(l.start_time)) / 3600.0 / COALESCE(o.shift_length_hrs, 9)) AS adjusted_active_time,
    (COALESCE(o.break_dur_hrs, 1) * (UNIX_TIMESTAMP(l.end_time) - UNIX_TIMESTAMP(l.start_time)) / 3600.0 / COALESCE(o.shift_length_hrs, 9)) AS adjusted_lock_time
  FROM gold_dashboard.analytics_emp_login_logout l
  JOIN FilteredEmployees e ON l.emp_id = e.emp_id
  LEFT JOIN inbound.countrywise_shift o ON e.country = o.country
  WHERE (l.shift_date BETWEEN '{start_date}' AND '{end_date}' OR l.shift_date BETWEEN '{prev_start_date}' AND '{prev_end_date}')
    AND l.is_weekoff = false
    AND l.emp_login_time IS NOT NULL
    AND l.emp_logout_time IS NOT NULL
),

FilteredAppInfo AS (
  SELECT
    i.emp_id,
    i.cal_date AS shift_date,
    TO_TIMESTAMP(CONCAT(i.cal_date, ' ', SUBSTRING(i.interval, 1, 5), ':00'), 'yyyy-MM-dd HH:mm:ss') AS interval_start,
    TO_TIMESTAMP(CONCAT(i.cal_date, ' ', SUBSTRING(i.interval, 7, 5), ':00'), 'yyyy-MM-dd HH:mm:ss') AS interval_end,
    i.total_time_spent_active AS total_time_active,
    i.total_time_spent_idle AS total_time_idle,
    i.window_lock_time
  FROM gold_dashboard.analytics_emp_app_info i
  JOIN FilteredEmployees e ON i.emp_id = e.emp_id
  WHERE (i.cal_date BETWEEN '{start_date}' AND '{end_date}' OR i.cal_date BETWEEN '{prev_start_date}' AND '{prev_end_date}')
),

ActivityWithShiftBounds AS (
  SELECT
    i.emp_id,
    i.shift_date,
    s.start_time,
    s.end_time,
    i.interval_start,
    i.interval_end,
    s.emp_login_time,
    s.emp_logout_time,
    i.total_time_active,
    i.total_time_idle,
    i.window_lock_time,
    GREATEST(i.interval_start, s.start_time) AS overlap_start,
    LEAST(i.interval_end, s.end_time) AS overlap_end,
    GREATEST(i.interval_start, s.emp_login_time) AS activity_start,
    LEAST(i.interval_end, s.emp_logout_time) AS activity_end
  FROM FilteredAppInfo i
  JOIN EmployeeShiftsWithConfig s ON i.emp_id = s.emp_id AND i.shift_date = s.shift_date
  WHERE s.emp_login_time < i.interval_end
    AND s.emp_logout_time > i.interval_start
),

ActivityTimeAllocation AS (
  SELECT
    emp_id,
    shift_date,
    total_time_active,
    total_time_idle,
    window_lock_time,
    start_time,
    end_time,
    
    -- Calculate seconds within shift
    CASE 
      WHEN activity_start >= end_time OR activity_end <= start_time THEN 0
      WHEN activity_start >= start_time AND activity_end <= end_time THEN
        UNIX_TIMESTAMP(activity_end) - UNIX_TIMESTAMP(activity_start)
      ELSE
        UNIX_TIMESTAMP(LEAST(activity_end, end_time)) - UNIX_TIMESTAMP(GREATEST(activity_start, start_time))
    END AS seconds_within_shift,
    
    -- Calculate seconds outside shift
    CASE 
      WHEN activity_start >= end_time OR activity_end <= start_time THEN
        UNIX_TIMESTAMP(activity_end) - UNIX_TIMESTAMP(activity_start)
      WHEN activity_start >= start_time AND activity_end <= end_time THEN 0
      ELSE
        (UNIX_TIMESTAMP(activity_end) - UNIX_TIMESTAMP(activity_start)) - 
        (UNIX_TIMESTAMP(LEAST(activity_end, end_time)) - UNIX_TIMESTAMP(GREATEST(activity_start, start_time)))
    END AS seconds_outside_shift
  FROM ActivityWithShiftBounds
  WHERE activity_start < activity_end
),

CurrentShiftData AS (
  SELECT
    a.emp_id,
    a.shift_date,
    
    -- Total metrics
    SUM(total_time_active) AS total_active_time,
    SUM(total_time_idle) AS total_idle_time,
    SUM(window_lock_time) AS total_lock_time,
    SUM(total_time_active + total_time_idle + window_lock_time) AS total_work_time,
    
    -- Within shift allocation
    SUM(
      total_time_active * (seconds_within_shift / NULLIF((seconds_within_shift + seconds_outside_shift), 0))
    ) AS active_within_shift,
    
    SUM(
      total_time_idle * (seconds_within_shift / NULLIF((seconds_within_shift + seconds_outside_shift), 0))
    ) AS idle_within_shift,
    
    SUM(
      window_lock_time * (seconds_within_shift / NULLIF((seconds_within_shift + seconds_outside_shift), 0))
    ) AS lock_within_shift,
    
    -- Outside shift allocation
    SUM(
      total_time_active * (seconds_outside_shift / NULLIF((seconds_within_shift + seconds_outside_shift), 0))
    ) AS active_outside_shift,
    
    SUM(
      total_time_idle * (seconds_outside_shift / NULLIF((seconds_within_shift + seconds_outside_shift), 0))
    ) AS idle_outside_shift,
    
    -- Shift configuration
    MAX(s.pulse_shift_time) AS pulse_shift_time,
    MAX(s.adjusted_active_time) AS adjusted_active_time,
    MAX(s.adjusted_lock_time) AS adjusted_lock_time,
    
    -- Period indicator
    CASE 
      WHEN a.shift_date BETWEEN '{start_date}' AND '{end_date}' THEN 'current'
      WHEN a.shift_date BETWEEN '{prev_start_date}' AND '{prev_end_date}' THEN 'previous'
    END AS period
    
  FROM ActivityTimeAllocation a
  JOIN EmployeeShiftsWithConfig s ON a.emp_id = s.emp_id AND a.shift_date = s.shift_date
  GROUP BY a.emp_id, a.shift_date, 
    CASE 
      WHEN a.shift_date BETWEEN '{start_date}' AND '{end_date}' THEN 'current'
      WHEN a.shift_date BETWEEN '{prev_start_date}' AND '{prev_end_date}' THEN 'previous'
    END
),

PeriodAggregates AS (
  SELECT
    period,
    AVG(total_work_time) AS avg_total_work_time,
    AVG(total_active_time) AS avg_total_active_time,
    AVG(total_idle_time) AS avg_total_idle_time,
    AVG(total_lock_time) AS avg_total_lock_time,
    AVG(active_within_shift + idle_within_shift + lock_within_shift) AS avg_total_work_time_within_shift,
    AVG(active_outside_shift + idle_outside_shift) AS avg_total_work_time_outside_shift,
    AVG(pulse_shift_time * 3600) AS avg_pulse_shift_time_seconds,
    AVG(adjusted_active_time * 3600) AS avg_adjusted_active_time_seconds,
    AVG(adjusted_lock_time * 3600) AS avg_adjusted_lock_time_seconds
  FROM CurrentShiftData
  WHERE period IN ('current', 'previous')
  GROUP BY period
)

SELECT
  -- Current period metrics
  curr.avg_total_work_time AS current_total_work_time,
  curr.avg_total_active_time AS current_total_active_time,
  curr.avg_total_idle_time AS current_total_idle_time,
  curr.avg_total_lock_time AS current_total_lock_time,
  curr.avg_total_work_time_within_shift AS current_total_work_time_within_shift,
  curr.avg_total_work_time_outside_shift AS current_total_work_time_outside_shift,
  
  -- Schedule comparisons
  (curr.avg_total_work_time - curr.avg_pulse_shift_time_seconds) AS total_work_shift_schedule,
  CASE 
    WHEN (curr.avg_total_work_time - curr.avg_pulse_shift_time_seconds) > 0 THEN 'Up'
    WHEN (curr.avg_total_work_time - curr.avg_pulse_shift_time_seconds) < 0 THEN 'Down'
    ELSE 'No Change'
  END AS total_work_shift_schedule_trend,
  
  (curr.avg_total_active_time - curr.avg_adjusted_active_time_seconds) AS total_active_shift_schedule,
  CASE 
    WHEN (curr.avg_total_active_time - curr.avg_adjusted_active_time_seconds) > 0 THEN 'Up'
    WHEN (curr.avg_total_active_time - curr.avg_adjusted_active_time_seconds) < 0 THEN 'Down'
    ELSE 'No Change'
  END AS total_active_shift_schedule_trend,
  
  (curr.avg_total_lock_time - curr.avg_adjusted_lock_time_seconds) AS total_lock_shift_schedule,
  CASE 
    WHEN (curr.avg_total_lock_time - curr.avg_adjusted_lock_time_seconds) > 0 THEN 'Up'
    WHEN (curr.avg_total_lock_time - curr.avg_adjusted_lock_time_seconds) < 0 THEN 'Down'
    ELSE 'No Change'
  END AS total_lock_shift_schedule_trend,
  
  -- Previous period metrics
  prev.avg_total_work_time AS prev_total_work_time,
  prev.avg_total_active_time AS prev_total_active_time,
  prev.avg_total_idle_time AS prev_total_idle_time,
  prev.avg_total_lock_time AS prev_total_lock_time,
  
  -- Period comparison trends
  CASE 
    WHEN curr.avg_total_work_time > prev.avg_total_work_time THEN 'Up'
    WHEN curr.avg_total_work_time < prev.avg_total_work_time THEN 'Down'
    ELSE 'No Change'
  END AS total_work_trend,
  
  ROUND(((curr.avg_total_work_time - prev.avg_total_work_time) / NULLIF(prev.avg_total_work_time, 0)) * 100, 2) AS total_work_percentage_change,
  
  CASE 
    WHEN curr.avg_total_active_time > prev.avg_total_active_time THEN 'Up'
    WHEN curr.avg_total_active_time < prev.avg_total_active_time THEN 'Down'
    ELSE 'No Change'
  END AS total_active_trend,
  
  ROUND(((curr.avg_total_active_time - prev.avg_total_active_time) / NULLIF(prev.avg_total_active_time, 0)) * 100, 2) AS total_active_percentage_change,
  
  CASE 
    WHEN curr.avg_total_idle_time > prev.avg_total_idle_time THEN 'Up'
    WHEN curr.avg_total_idle_time < prev.avg_total_idle_time THEN 'Down'
    ELSE 'No Change'
  END AS total_idle_trend,
  
  ROUND(((curr.avg_total_idle_time - prev.avg_total_idle_time) / NULLIF(prev.avg_total_idle_time, 0)) * 100, 2) AS total_idle_percentage_change,
  
  CASE 
    WHEN curr.avg_total_lock_time > prev.avg_total_lock_time THEN 'Up'
    WHEN curr.avg_total_lock_time < prev.avg_total_lock_time THEN 'Down'
    ELSE 'No Change'
  END AS total_lock_trend,
  
  ROUND(((curr.avg_total_lock_time - prev.avg_total_lock_time) / NULLIF(prev.avg_total_lock_time, 0)) * 100, 2) AS total_lock_percentage_change
  
FROM 
  (SELECT * FROM PeriodAggregates WHERE period = 'current') curr
CROSS JOIN
  (SELECT * FROM PeriodAggregates WHERE period = 'previous') prev
WHERE curr.avg_total_work_time IS NOT NULL AND prev.avg_total_work_time IS NOT NULL






---------
-----------------
WITH ActivityWithShiftBounds AS (
  SELECT
    i.emp_id,
    i.shift_date,
    i.start_time,
    i.end_time,
    i.interval_start,
    i.interval_end,
    i.emp_login_time,
    i.emp_logout_time,
    i.total_time_active,
    i.total_time_idle,
    i.window_lock_time,
    greatest(i.interval_start, i.start_time) AS overlap_start,
    least(i.interval_end, i.end_time) AS overlap_end,
    greatest(i.interval_start, i.emp_login_time) AS activity_start,
    least(i.interval_end, i.emp_logout_time) AS activity_end
  FROM FilteredAppInfo i
  LEFT JOIN EmployeeShiftsWithConfig s
    ON i.emp_id = s.emp_id AND i.shift_date = s.shift_date
),

ActivityTimeAllocation AS (
  SELECT
    emp_id,
    shift_date,
    total_time_active,
    total_time_idle,
    window_lock_time,
    start_time,
    end_time,
    activity_start,
    activity_end,
    
    -- Calculate seconds within shift (considers BOTH login/logout AND shift boundaries)
    CASE 
      WHEN activity_start >= end_time OR activity_end <= start_time THEN
        0
      WHEN activity_start >= start_time AND activity_end <= end_time THEN
        unix_timestamp(activity_end) - unix_timestamp(activity_start)
      ELSE
        unix_timestamp(least(activity_end, end_time)) - unix_timestamp(greatest(activity_start, start_time))
    END AS seconds_within_shift,
    
    -- Calculate seconds outside shift
    CASE 
      WHEN activity_start >= end_time OR activity_end <= start_time THEN
        unix_timestamp(activity_end) - unix_timestamp(activity_start)
      WHEN activity_start >= start_time AND activity_end <= end_time THEN
        0
      ELSE
        (unix_timestamp(activity_end) - unix_timestamp(activity_start)) - 
        (unix_timestamp(least(activity_end, end_time)) - unix_timestamp(greatest(activity_start, start_time)))
    END AS seconds_outside_shift,
    
    -- Total activity seconds for validation
    unix_timestamp(activity_end) - unix_timestamp(activity_start) AS total_activity_seconds
    
  FROM ActivityWithShiftBounds
  WHERE activity_start < activity_end
),

CurrentShiftData AS (
  SELECT
    a.emp_id,
    a.shift_date,
    
    -- Total metrics
    SUM(total_time_active) AS total_active_time,
    SUM(total_time_idle) AS total_idle_time,
    SUM(window_lock_time) AS total_lock_time,
    SUM(total_time_active + total_time_idle + window_lock_time) AS total_work_time,
    
    -- Within shift allocation
    SUM(
      CASE 
        WHEN (seconds_within_shift + seconds_outside_shift) = 0 THEN 0
        ELSE total_time_active * (seconds_within_shift / (seconds_within_shift + seconds_outside_shift))
      END
    ) AS active_within_shift,
    
    SUM(
      CASE 
        WHEN (seconds_within_shift + seconds_outside_shift) = 0 THEN 0
        ELSE total_time_idle * (seconds_within_shift / (seconds_within_shift + seconds_outside_shift))
      END
    ) AS idle_within_shift,
    
    SUM(
      CASE 
        WHEN (seconds_within_shift + seconds_outside_shift) = 0 THEN 0
        ELSE window_lock_time * (seconds_within_shift / (seconds_within_shift + seconds_outside_shift))
      END
    ) AS lock_within_shift,
    
    -- Outside shift allocation (only active + idle, no lock time)
    SUM(
      CASE 
        WHEN (seconds_within_shift + seconds_outside_shift) = 0 THEN 0
        ELSE total_time_active * (seconds_outside_shift / (seconds_within_shift + seconds_outside_shift))
      END
    ) AS active_outside_shift,
    
    SUM(
      CASE 
        WHEN (seconds_within_shift + seconds_outside_shift) = 0 THEN 0
        ELSE total_time_idle * (seconds_outside_shift / (seconds_within_shift + seconds_outside_shift))
      END
    ) AS idle_outside_shift,
    
    -- Shift configuration
    MAX(s.pulse_shift_time) AS pulse_shift_time,
    MAX(s.adjusted_active_time) AS adjusted_active_time,
    MAX(s.adjusted_lock_time) AS adjusted_lock_time,
    MIN(s.start_time) AS shift_start_time,
    MAX(s.end_time) AS shift_end_time,
    
    -- Debug totals for validation
    SUM(seconds_within_shift) AS total_seconds_within_shift,
    SUM(seconds_outside_shift) AS total_seconds_outside_shift,
    SUM(total_activity_seconds) AS total_activity_seconds
    
  FROM ActivityTimeAllocation a
  JOIN EmployeeShiftsWithConfig s
    ON a.emp_id = s.emp_id AND a.shift_date = s.shift_date
  GROUP BY a.emp_id, a.shift_date
)

SELECT
  emp_id,
  shift_date,
  total_active_time,
  total_idle_time,
  total_lock_time,
  total_work_time,
  active_within_shift,
  idle_within_shift,
  lock_within_shift,
  active_outside_shift,
  idle_outside_shift,
  (active_within_shift + idle_within_shift + lock_within_shift) AS time_within_shift,
  (active_outside_shift + idle_outside_shift) AS time_outside_shift,
  pulse_shift_time,
  adjusted_active_time,
  adjusted_lock_time,
  shift_start_time,
  shift_end_time,
  -- Debug columns (can remove in production)
  total_seconds_within_shift,
  total_seconds_outside_shift,
  total_activity_seconds
FROM CurrentShiftData
ORDER BY emp_id, shift_date






Let me walk through your examples using Approach 1 logic:

Example 1: Shift 10:25:00 - 19:25:00, Login 10:45:12, Logout 20:42:45

For 10:00-11:00 interval:

· Activity: 10:45:12 - 11:00:00 (14m48s)
· seconds_within_shift = unix_timestamp('11:00:00') - unix_timestamp('10:45:12') = 888 seconds
· seconds_outside_shift = 0 seconds (all within shift)
· Ratio = 888/(888+0) = 1.0
· Result: 100% of active/idle/lock time allocated to WITHIN shift

For 19:00-20:00 interval:

· Activity: 19:00:00 - 20:00:00 (60m)
· Within shift portion: 19:00:00 - 19:25:00 (25m)
· Outside shift portion: 19:25:00 - 20:00:00 (35m)
· seconds_within_shift = unix_timestamp('19:25:00') - unix_timestamp('19:00:00') = 1500 seconds
· seconds_outside_shift = unix_timestamp('20:00:00') - unix_timestamp('19:25:00') = 2100 seconds
· Ratio = 1500/(1500+2100) = 0.4167 (within), 0.5833 (outside)
· Result: 41.67% of active/idle/lock to WITHIN shift, 58.33% of active/idle to OUTSIDE shift

For 20:00-21:00 interval:

· Activity: 20:00:00 - 20:42:45 (42m45s)
· seconds_within_shift = 0 seconds (completely after shift end)
· seconds_outside_shift = unix_timestamp('20:42:45') - unix_timestamp('20:00:00') = 2565 seconds
· Ratio = 0/(0+2565) = 0.0 (within), 1.0 (outside)
· Result: 100% of active/idle to OUTSIDE shift, 0% lock time

---

Example 2: Shift 10:25:00 - 19:25:00, Login 09:12:14, Logout 16:20:00

For 09:00-10:00 interval:

· Activity: 09:12:14 - 10:00:00 (47m46s)
· seconds_within_shift = 0 seconds (completely before shift start)
· seconds_outside_shift = unix_timestamp('10:00:00') - unix_timestamp('09:12:14') = 2866 seconds
· Ratio = 0/(0+2866) = 0.0 (within), 1.0 (outside)
· Result: 100% of active/idle to OUTSIDE shift

For 10:00-11:00 interval:

· Activity: 10:00:00 - 11:00:00 (60m)
· Within shift portion: 10:25:00 - 11:00:00 (35m)
· Outside shift portion: 10:00:00 - 10:25:00 (25m)
· seconds_within_shift = unix_timestamp('11:00:00') - unix_timestamp('10:25:00') = 2100 seconds
· seconds_outside_shift = unix_timestamp('10:25:00') - unix_timestamp('10:00:00') = 1500 seconds
· Ratio = 2100/(2100+1500) = 0.5833 (within), 0.4167 (outside)
· Result: 58.33% of active/idle/lock to WITHIN shift, 41.67% of active/idle to OUTSIDE shift

---

Example 3: Shift 09:30:00 - 18:30:00, Login 10:15:12, Logout 20:42:45

For 10:00-11:00 interval:

· Activity: 10:15:12 - 11:00:00 (44m48s)
· seconds_within_shift = unix_timestamp('11:00:00') - unix_timestamp('10:15:12') = 2688 seconds
· seconds_outside_shift = 0 seconds (all within shift)
· Ratio = 2688/(2688+0) = 1.0
· Result: 100% of active/idle/lock to WITHIN shift

For 18:00-19:00 interval:

· Activity: 18:00:00 - 19:00:00 (60m)
· Within shift portion: 18:00:00 - 18:30:00 (30m)
· Outside shift portion: 18:30:00 - 19:00:00 (30m)
· seconds_within_shift = unix_timestamp('18:30:00') - unix_timestamp('18:00:00') = 1800 seconds
· seconds_outside_shift = unix_timestamp('19:00:00') - unix_timestamp('18:30:00') = 1800 seconds
· Ratio = 1800/(1800+1800) = 0.5 (within), 0.5 (outside)
· Result: 50% of active/idle/lock to WITHIN shift, 50% of active/idle to OUTSIDE shift

Key Points:

1. Automatic handling: The ratio seconds_within_shift/(total_seconds) automatically gives correct allocation
2. Complete within: Ratio = 1.0 → 100% within shift
3. Complete outside: Ratio = 0.0 → 100% outside shift
4. Partial overlap: Exact proportional split based on time
5. Lock time: Only gets allocated to within-shift portion (as per business rule)

The logic is mathematically precise and handles all your scenarios correctly!






from datetime import datetime, date, timedelta
from typing import Optional
from fastapi import Query, Header, Depends
from fastapi.responses import JSONResponse
from fastapi import status
from auth.jwt_handler import AuthJWT

def get_last_day_of_month(dt: date) -> date:
    """Get the last day of the month for a given date"""
    if dt.month == 12:
        return date(dt.year + 1, 1, 1) - timedelta(days=1)
    return date(dt.year, dt.month + 1, 1) - timedelta(days=1)

@router.get('/get_team_active_idle_app_category')
async def get_team_app_activity(
    Authorize: AuthJWT = Depends(),
    authorization: str = Header(None),
    start_date: str = Query(...),
    end_date: str = Query(...),
    date_range: Optional[str] = Query("day")
):
    if not authorization or not authorization.startswith("Bearer"):
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"msg": "Authorization Token Missing"}
        )
    
    try:
        Authorize.jwt_required()
        user_identity = Authorize.get_jwt_subject()
        claims = Authorize.get_raw_jwt()
        manager_id = claims.get("user_id")
        
        if not manager_id:
            return JSONResponse(
                status_code=status.HTTP_401_UNAUTHORIZED,
                content={'message': "Invalid Token Claims"}
            )
        
        today = datetime.today().date()
        
        if start_date and end_date:
            try:
                start_date_dt = datetime.strptime(start_date, "%Y-%m-%d").date()
                end_date_dt = datetime.strptime(end_date, "%Y-%m-%d").date()
                
                period_days = (end_date_dt - start_date_dt).days + 1
                
                # Handle different date range types
                if date_range.lower() == "day":
                    # Previous week's same day
                    prev_start_date = start_date_dt - timedelta(days=7)
                    prev_end_date = prev_start_date
                    ext_end_date = end_date_dt + timedelta(days=1)
                    
                elif date_range.lower() == "week":
                    # Previous week's same days
                    prev_start_date = start_date_dt - timedelta(days=7)
                    prev_end_date = end_date_dt - timedelta(days=7)
                    ext_end_date = end_date_dt + timedelta(days=1)
                    
                elif date_range.lower() == "month":
                    # Check if selected dates are in current month
                    current_month = today.month
                    current_year = today.year
                    selected_month = start_date_dt.month
                    selected_year = start_date_dt.year
                    
                    if selected_month == current_month and selected_year == current_year:
                        # Current month: go back exactly 4 weeks (28 days)
                        prev_start_date = start_date_dt - timedelta(days=28)
                        prev_end_date = end_date_dt - timedelta(days=28)
                    else:
                        # Not current month: previous month's start and end
                        if start_date_dt.month == 1:
                            prev_start_date = date(start_date_dt.year - 1, 12, 1)
                        else:
                            prev_start_date = date(start_date_dt.year, start_date_dt.month - 1, 1)
                        
                        if end_date_dt.month == 1:
                            prev_end_date = date(end_date_dt.year - 1, 12, 31)
                        else:
                            prev_month = end_date_dt.month - 1
                            prev_year = end_date_dt.year
                            if prev_month == 0:
                                prev_month = 12
                                prev_year = end_date_dt.year - 1
                            prev_end_date = get_last_day_of_month(date(prev_year, prev_month, 1))
                    
                    ext_end_date = end_date_dt + timedelta(days=1)
                
                else:
                    # DEFAULT: Exact period days logic for unknown date_range values
                    prev_end_date = start_date_dt - timedelta(days=1)
                    prev_start_date = prev_end_date - timedelta(days=period_days - 1)
                    ext_end_date = end_date_dt + timedelta(days=1)
                
                return {
                    "start_date": start_date_dt.isoformat(),
                    "end_date": end_date_dt.isoformat(),
                    "previous_start_date": prev_start_date.isoformat(),
                    "previous_end_date": prev_end_date.isoformat(),
                    "extended_end_date": ext_end_date.isoformat(),
                    "date_range": date_range,
                    "period_days": period_days
                }
                
            except ValueError:
                return JSONResponse(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    content={"msg": "Invalid date format. Use YYYY-MM-DD."}
                )
        else:
            # Default to daily
            start_date_dt = today
            end_date_dt = today
            period_days = 1
            prev_end_date = start_date_dt - timedelta(days=1)
            prev_start_date = prev_end_date - timedelta(days=period_days - 1)
            ext_end_date = end_date_dt + timedelta(days=1)
            
            return {
                "start_date": start_date_dt.isoformat(),
                "end_date": end_date_dt.isoformat(),
                "previous_start_date": prev_start_date.isoformat(),
                "previous_end_date": prev_end_date.isoformat(),
                "extended_end_date": ext_end_date.isoformat(),
                "date_range": "day",
                "period_days": period_days
            }
            
    except Exception as e:
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={"msg": f"Internal server error: {str(e)}"}
        )




pip installs python-dateutil

from datetime import datetime, date, timedelta
from dateutil.relativedelta import relativedelta
from typing import Optional
from fastapi import Query, Header, Depends
from fastapi.responses import JSONResponse
from fastapi import status
from auth.jwt_handler import AuthJWT

@router.get('/get_team_active_idle_app_category')
async def get_team_app_activity(
    Authorize: AuthJWT = Depends(),
    authorization: str = Header(None),
    start_date: str = Query(...),
    end_date: str = Query(...),
    date_range: Optional[str] = Query("day")
):
    if not authorization or not authorization.startswith("Bearer"):
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"msg": "Authorization Token Missing"}
        )
    
    try:
        Authorize.jwt_required()
        user_identity = Authorize.get_jwt_subject()
        claims = Authorize.get_raw_jwt()
        manager_id = claims.get("user_id")
        
        if not manager_id:
            return JSONResponse(
                status_code=status.HTTP_401_UNAUTHORIZED,
                content={'message': "Invalid Token Claims"}
            )
        
        today = datetime.today().date()
        
        if start_date and end_date:
            try:
                start_date_dt = datetime.strptime(start_date, "%Y-%m-%d").date()
                end_date_dt = datetime.strptime(end_date, "%Y-%m-%d").date()
                
                period_days = (end_date_dt - start_date_dt).days + 1
                
                # Handle different date range types
                if date_range.lower() == "day":
                    # Previous week's same day
                    prev_start_date = start_date_dt - timedelta(days=7)
                    prev_end_date = prev_start_date  # Same day for day range
                    ext_end_date = end_date_dt + timedelta(days=1)
                    
                elif date_range.lower() == "week":
                    # Previous week's same days (exact number of days)
                    prev_start_date = start_date_dt - timedelta(days=7)
                    prev_end_date = end_date_dt - timedelta(days=7)
                    ext_end_date = end_date_dt + timedelta(days=1)
                    
                elif date_range.lower() == "month":
                    # Check if selected dates are in current month
                    current_month = today.month
                    current_year = today.year
                    selected_month = start_date_dt.month
                    selected_year = start_date_dt.year
                    
                    if selected_month == current_month and selected_year == current_year:
                        # Current month: go back exactly 4 weeks (28 days)
                        prev_start_date = start_date_dt - timedelta(days=28)
                        prev_end_date = end_date_dt - timedelta(days=28)
                    else:
                        # Not current month: previous month's start and end
                        # Get first day of previous month for start_date
                        prev_start_date = (start_date_dt.replace(day=1) - relativedelta(months=1)).replace(day=1)
                        
                        # Get last day of previous month for end_date
                        prev_month_end = (end_date_dt.replace(day=1) - relativedelta(months=1))
                        prev_end_date = (prev_month_end + relativedelta(months=1, days=-1))
                    
                    ext_end_date = end_date_dt + timedelta(days=1)
                
                else:
                    # Default to day logic for unknown ranges
                    prev_start_date = start_date_dt - timedelta(days=7)
                    prev_end_date = prev_start_date
                    ext_end_date = end_date_dt + timedelta(days=1)
                
                # Return the calculated dates
                return {
                    "start_date": start_date_dt.isoformat(),
                    "end_date": end_date_dt.isoformat(),
                    "previous_start_date": prev_start_date.isoformat(),
                    "previous_end_date": prev_end_date.isoformat(),
                    "extended_end_date": ext_end_date.isoformat(),
                    "date_range": date_range,
                    "period_days": period_days
                }
                
            except ValueError:
                return JSONResponse(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    content={"msg": "Invalid date format. Use YYYY-MM-DD."}
                )
        else:
            # Default to daily when no dates provided
            start_date_dt = today
            end_date_dt = today
            prev_start_date = today - timedelta(days=7)
            prev_end_date = prev_start_date
            ext_end_date = today + timedelta(days=1)
            
            return {
                "start_date": start_date_dt.isoformat(),
                "end_date": end_date_dt.isoformat(),
                "previous_start_date": prev_start_date.isoformat(),
                "previous_end_date": prev_end_date.isoformat(),
                "extended_end_date": ext_end_date.isoformat(),
                "date_range": "day",
                "period_days": 1
            }
            
    except Exception as e:
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={"msg": f"Internal server error: {str(e)}"}
        )







Here’s a cleaner, structured draft you can use for your proposal email or document:


---

Subject: Proposal to Exclude Week-Off Data from Average Calculations in RPEM APIs

Background:
Currently, our system captures and stores agent activity data for all employees, regardless of whether the date falls on a working day or a week-off (e.g., Saturday, Sunday, or designated weekly off-days). This includes cases where an employee might log in briefly (e.g., 10 minutes or 1 hour) during their week-off.

When average calculations are performed — whether for a week, month, or custom date range — this week-off data is included in the calculation. As a result:

A small amount of activity on a week-off skews both individual and team averages.

The reported averages may not accurately reflect true working-day performance.


Proposed Change:
For all APIs that calculate averages in RPEM, we propose excluding week-off employee activity from the average computation. This means:

If a date is a week-off for a given employee, their data for that date will be ignored in average calculations.

Averages will be calculated only on valid working days for each employee.


Expected Benefits:

More accurate and meaningful average metrics for individuals and teams.

Avoids distortion of averages caused by minimal week-off activity.

Consistency across all APIs, regardless of the selected time period filter (week, month, or custom).


Scope:

Changes will be implemented in all RPEM APIs that involve average calculations.

The logic to exclude week-off data will be applied at the calculation stage.


Request:
We seek sign-off on this proposal so that development can proceed with updating all relevant APIs to exclude week-off data from average computations.


---

Do you want me to also add a visual example showing how averages change before vs. after this change? That would make the proposal even easier to approve.

