I wanted to share some context regarding the current issue and the upcoming change request scheduled for Monday or Tuesday.

At present, the login-logout recalculation is based on a delete and insert process. This approach leads to inaccurate data being returned if queries are made while the job is still running. The current process recalculates login-logout data for employees whose records fall within the past 72 hours, regardless of whether their actual login or logout times have changed. This is especially problematic with shift changes.

In the upcoming CR, this will be improved. The login-logout recalculation will occur only when there is an actual change in the employeeâ€™s login or logout time. Additionally, the process will be handled using an upsert approach instead of delete and insert. This means new data will be appended or updated without removing existing records, ensuring data consistency and accuracy.

With these changes, the issue of querying data during job execution will no longer occur.

Let me know if you have any questions.

import os
import logging
from dotenv import load_dotenv
import urllib.parse
import threading
from databricks import sql
from databricks.sdk.core import Config, oauth_service_principal
from queue import Queue
import time
from contextlib import contextmanager
from typing import Dict, Any
import json
from fastapi.responses import JSONResponse

# Load environment variables from .env file
load_dotenv()

# Configure Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Connection Pool Configuration
POOL_SIZE = 120  # Max number of connections in the pool
CONNECTION_TTL = 1800  # 30 minutes in seconds
connection_pool = Queue(maxsize=POOL_SIZE)
pool_lock = threading.Lock()

# Connection monitoring class
class ConnectionMonitor:
    def __init__(self):
        self.active_connections = 0
        self.total_connections_created = 0
        self.connections_reused = 0
        self.connections_closed = 0
        self.monitor_lock = threading.Lock()
    
    def connection_created(self):
        with self.monitor_lock:
            self.active_connections += 1
            self.total_connections_created += 1
            logger.info(f"New connection created. Active: {self.active_connections}, Total created: {self.total_connections_created}")
    
    def connection_reused(self):
        with self.monitor_lock:
            self.connections_reused += 1
            logger.info(f"Connection reused from pool. Reused count: {self.connections_reused}")
    
    def connection_returned(self):
        with self.monitor_lock:
            logger.info(f"Connection returned to pool. Pool size: {connection_pool.qsize()}")
    
    def connection_closed(self, reason: str):
        with self.monitor_lock:
            self.active_connections -= 1
            self.connections_closed += 1
            logger.info(f"Connection closed ({reason}). Active: {self.active_connections}, Closed: {self.connections_closed}")
    
    def get_stats(self) -> Dict[str, Any]:
        with self.monitor_lock:
            return {
                "active_connections": self.active_connections,
                "total_connections_created": self.total_connections_created,
                "connections_reused": self.connections_reused,
                "connections_closed": self.connections_closed,
                "pool_size": connection_pool.qsize(),
                "pool_capacity": POOL_SIZE
            }

# Initialize connection monitor
connection_monitor = ConnectionMonitor()

# Function to create a new Databricks connection with logging
def create_connection():
    try:
        server_hostname = os.getenv('DATABRICKS_SERVER_HOST')
        config = Config(
            host=f"https://{server_hostname}",
            client_id=os.getenv('DATABRICKS_CLIENT_ID'),
            client_secret=os.getenv('DATABRICKS_CLIENT_SECRET')
        )
        
        logger.info(f"Creating new connection to {server_hostname}")
        conn = sql.connect(
            server_hostname=server_hostname,
            http_path=os.getenv('DATABRICKS_HTTP_PATH'),
            credentials_provider=lambda: oauth_service_principal(config)
        )
        
        connection_monitor.connection_created()
        return conn
        
    except Exception as e:
        logger.error(f"Failed to create connection: {str(e)}")
        raise

# Context Manager for Connection Pooling with enhanced logging
class DatabricksSession:
    def __enter__(self):
        now = time.time()
        connection_obtained = False
        
        with pool_lock:
            # Try to get a connection from the pool
            while not connection_pool.empty() and not connection_obtained:
                conn, last_used = connection_pool.get()
                if now - last_used < CONNECTION_TTL:
                    self.conn = conn
                    connection_obtained = True
                    connection_monitor.connection_reused()
                    logger.info(f"Reused connection from pool. Time since last use: {now - last_used:.2f}s")
                else:
                    try:
                        conn.close()
                        connection_monitor.connection_closed("stale")
                        logger.info("Closed stale connection from pool")
                    except Exception as e:
                        logger.error(f"Error closing stale connection: {e}")
            
            # If no valid connection found in pool, create a new one
            if not connection_obtained:
                self.conn = create_connection()
                logger.info("Created new connection (pool empty or no valid connections)")
        
        logger.info(f"Connection obtained. Pool status: {connection_pool.qsize()}/{POOL_SIZE}")
        return self.conn

    def __exit__(self, exc_type, exc_val, exc_tb):
        try:
            # Check if connection is still valid before returning to pool
            if hasattr(self, 'conn') and self.conn:
                if exc_type is not None:
                    # If there was an exception, don't return to pool
                    logger.warning(f"Exception occurred in session, closing connection: {exc_val}")
                    self.conn.close()
                    connection_monitor.connection_closed("exception")
                else:
                    # Return connection to pool if it's not full
                    with pool_lock:
                        if connection_pool.full():
                            logger.warning("Connection pool full, closing connection")
                            self.conn.close()
                            connection_monitor.connection_closed("pool_full")
                        else:
                            connection_pool.put((self.conn, time.time()))
                            connection_monitor.connection_returned()
                            logger.debug(f"Connection returned to pool. New size: {connection_pool.qsize()}")
            else:
                logger.warning("No valid connection to return to pool")
                
        except Exception as e:
            logger.error(f"Error handling connection cleanup: {e}")
            try:
                if hasattr(self, 'conn') and self.conn:
                    self.conn.close()
                    connection_monitor.connection_closed("cleanup_error")
            except Exception as close_error:
                logger.error(f"Error closing connection during cleanup: {close_error}")

# Function to execute query with logging
def execute_query(conn, query: str):
    try:
        logger.info(f"Executing query: {query[:100]}...")  # Log first 100 chars
        start_time = time.time()
        
        with conn.cursor() as cursor:
            cursor.execute(query)
            result = cursor.fetchall()
        
        execution_time = time.time() - start_time
        logger.info(f"Query executed successfully in {execution_time:.2f}s. Rows returned: {len(result)}")
        return result
        
    except Exception as e:
        logger.error(f"Query execution failed: {str(e)}")
        raise

# Function to get connection pool statistics
def get_connection_stats() -> Dict[str, Any]:
    stats = connection_monitor.get_stats()
    logger.info(f"Connection pool statistics: {stats}")
    return stats

# Your existing usage pattern remains the same
def process_query(query: str):
    try:
        with DatabricksSession() as conn:
            data = execute_query(conn, query)
            
            if data and data[0]:
                json_string = data[0][0]
                json_data = json.loads(json_string)
            else:
                json_data = {"empSummary": {}, "teamSummary": {}, "empCount": None}
        
        # Log connection stats after operation
        stats = get_connection_stats()
        logger.info(f"Operation completed. Connection stats: {stats}")
        
        if not json_data:
            return JSONResponse(status_code=404, content={'message': "Execution Not Success"})
        return JSONResponse(status_code=200, content=json_data)
        
    except Exception as e:
        logger.error(f"Error in process_query: {str(e)}")
        return JSONResponse(status_code=500, content={'message': f"Internal Server Error: {str(e)}"})

# Optional: Add periodic logging of connection stats
def log_periodic_stats(interval: int = 300):  # 5 minutes
    """Periodically log connection pool statistics"""
    while True:
        time.sleep(interval)
        stats = get_connection_stats()
        logger.info(f"Periodic connection pool report: {stats}")

# Start periodic logging in a background thread (optional)
# stats_thread = threading.Thread(target=log_periodic_stats, daemon=True)
# stats_thread.start()







result_df = classified_df.groupBy("emp_id", "cal_date").agg(
    # Current login: Minimum start_time within current window and app_name != "Window Lock"
    F.min(F.when(
        (F.col("is_current_login")) & (F.col("app_name") != "WindowLock"),
        F.col("start_time")
    )).alias("current_login"),
    
    # Last activity: Maximum start_time within current window BUT exclude WindowLock
    F.max(F.when(
        (F.col("start_time") >= F.col("current_window_start")) &
        (F.col("start_time") <= F.col("current_window_end")) &
        (F.col("app_name") != "WindowLock"),  # EXCLUDE WindowLock
        F.col("start_time")
    )).alias("last_activity"),
    
    # ... keep other aggregations as they are
)

# For mouse/keyboard data, we already exclude WindowLock by nature
result_mkdf = classified_mkdf.groupBy("emp_id", "cal_date").agg(
    F.max(F.when(
        (F.col("event_time") >= F.col("current_window_start")) &
        (F.col("event_time") <= F.col("current_window_end")),
        F.col("event_time")
    )).alias("last_mk_activity"),
    # ... other aggregations
)




final_result = result_df.withColumn(
    "emp_login_time",
    F.when(
        F.col("is_week_off"),
        F.coalesce(F.col("week_off_login"), F.col("last_activity"))
    ).otherwise(
        F.coalesce(F.col("current_login"), F.col("last_activity"))
    )
).withColumn(
    "emp_logout_time",
    F.when(
        F.col("is_week_off"),
        F.coalesce(F.col("week_off_logout"), F.col("last_activity"))
    ).otherwise(
        # PREFER mouse/keyboard logout over app activities (which excludes WindowLock)
        F.coalesce(F.col("last_mk_activity"), F.col("last_activity"), F.col("current_login"))
    )
)




# Single combined merge with optimized logic
spark.sql("""
MERGE INTO gold_dashboard.analytics_emp_login_logout AS target
USING (
    -- Combined source data with operation type indicator
    SELECT ..., 1 as operation_type FROM temp_filtered_login_logout
    UNION ALL
    SELECT ..., 2 as operation_type FROM temp_prev_day_updates
) AS source
ON (
    -- Combined join conditions
    (operation_type = 1 AND target.EMP_ID = source.emp_id AND target.SHIFT_DATE = source.shift_date)
    OR
    (operation_type = 2 AND target.EMP_ID = source.update_emp_id AND target.SHIFT_DATE = source.update_date)
)
WHEN MATCHED AND source.operation_type = 1 AND (
    -- Only update if source values are better
    (unix_timestamp(source.emp_login_time) < unix_timestamp(target.EMP_LOGIN_TIME)) OR
    (unix_timestamp(source.emp_logout_time) > unix_timestamp(target.EMP_LOGOUT_TIME))
) THEN
    UPDATE SET 
        target.EMP_LOGIN_TIME = CASE 
            WHEN unix_timestamp(source.emp_login_time) < unix_timestamp(target.EMP_LOGIN_TIME)
            THEN source.emp_login_time 
            ELSE target.EMP_LOGIN_TIME 
        END,
        target.EMP_LOGOUT_TIME = CASE 
            WHEN unix_timestamp(source.emp_logout_time) > unix_timestamp(target.EMP_LOGOUT_TIME)
            THEN source.emp_logout_time 
            ELSE target.EMP_LOGOUT_TIME 
        END
WHEN MATCHED AND source.operation_type = 2 AND (
    -- Only update previous logout if source is later
    unix_timestamp(source.new_logout_time) > unix_timestamp(target.EMP_LOGOUT_TIME)
) THEN
    UPDATE SET target.EMP_LOGOUT_TIME = source.new_logout_time
WHEN NOT MATCHED AND source.operation_type = 1 THEN
    INSERT (EMP_ID, START_TIME, END_TIME, EMP_LOGIN_TIME, EMP_LOGOUT_TIME, SHIFT_DATE)
    VALUES (source.emp_id, source.start_time, source.end_time,
            source.emp_login_time, source.emp_logout_time, source.shift_date)
""")




WITH FilteredEmployees AS (
  SELECT
    emplid AS emp_id,
    COALESCE(country, 'Default') AS country
  FROM inbound.hr_employee_central
  WHERE func_mar_id = 43560069
    AND (TERMINATION_DT > CURRENT_TIMESTAMP() OR TERMINATION_DT IS NULL)
),

EmployeeShiftsWithConfig AS (
  SELECT
    l.emp_id,
    l.shift_date,
    TO_TIMESTAMP(l.start_time, 'yyyy-MM-dd HH:mm:ss') AS start_time,
    TO_TIMESTAMP(l.end_time, 'yyyy-MM-dd HH:mm:ss') AS end_time,
    TO_TIMESTAMP(l.emp_login_time, 'yyyy-MM-dd HH:mm:ss') AS emp_login_time,
    TO_TIMESTAMP(l.emp_logout_time, 'yyyy-MM-dd HH:mm:ss') AS emp_logout_time,
    (UNIX_TIMESTAMP(l.end_time) - UNIX_TIMESTAMP(l.start_time)) / 3600.0 AS pulse_shift_time,
    (COALESCE(o.shift_dur_hrs, 8) * (UNIX_TIMESTAMP(l.end_time) - UNIX_TIMESTAMP(l.start_time)) / 3600.0 / COALESCE(o.shift_length_hrs, 9)) AS adjusted_active_time,
    (COALESCE(o.break_dur_hrs, 1) * (UNIX_TIMESTAMP(l.end_time) - UNIX_TIMESTAMP(l.start_time)) / 3600.0 / COALESCE(o.shift_length_hrs, 9)) AS adjusted_lock_time
  FROM gold_dashboard.analytics_emp_login_logout l
  JOIN FilteredEmployees e ON l.emp_id = e.emp_id
  LEFT JOIN inbound.countrywise_shift o ON e.country = o.country
  WHERE (l.shift_date BETWEEN '{start_date}' AND '{end_date}' OR l.shift_date BETWEEN '{prev_start_date}' AND '{prev_end_date}')
    AND l.is_weekoff = false
    AND l.emp_login_time IS NOT NULL
    AND l.emp_logout_time IS NOT NULL
),

FilteredAppInfo AS (
  SELECT
    i.emp_id,
    i.cal_date AS shift_date,
    TO_TIMESTAMP(CONCAT(i.cal_date, ' ', SUBSTRING(i.interval, 1, 5), ':00'), 'yyyy-MM-dd HH:mm:ss') AS interval_start,
    TO_TIMESTAMP(CONCAT(i.cal_date, ' ', SUBSTRING(i.interval, 7, 5), ':00'), 'yyyy-MM-dd HH:mm:ss') AS interval_end,
    i.total_time_spent_active AS total_time_active,
    i.total_time_spent_idle AS total_time_idle,
    i.window_lock_time
  FROM gold_dashboard.analytics_emp_app_info i
  JOIN FilteredEmployees e ON i.emp_id = e.emp_id
  WHERE (i.cal_date BETWEEN '{start_date}' AND '{end_date}' OR i.cal_date BETWEEN '{prev_start_date}' AND '{prev_end_date}')
),

ActivityWithShiftBounds AS (
  SELECT
    i.emp_id,
    i.shift_date,
    s.start_time,
    s.end_time,
    i.interval_start,
    i.interval_end,
    s.emp_login_time,
    s.emp_logout_time,
    i.total_time_active,
    i.total_time_idle,
    i.window_lock_time,
    GREATEST(i.interval_start, s.start_time) AS overlap_start,
    LEAST(i.interval_end, s.end_time) AS overlap_end,
    GREATEST(i.interval_start, s.emp_login_time) AS activity_start,
    LEAST(i.interval_end, s.emp_logout_time) AS activity_end
  FROM FilteredAppInfo i
  JOIN EmployeeShiftsWithConfig s ON i.emp_id = s.emp_id AND i.shift_date = s.shift_date
  WHERE s.emp_login_time < i.interval_end
    AND s.emp_logout_time > i.interval_start
),

ActivityTimeAllocation AS (
  SELECT
    emp_id,
    shift_date,
    total_time_active,
    total_time_idle,
    window_lock_time,
    start_time,
    end_time,
    
    -- Calculate seconds within shift
    CASE 
      WHEN activity_start >= end_time OR activity_end <= start_time THEN 0
      WHEN activity_start >= start_time AND activity_end <= end_time THEN
        UNIX_TIMESTAMP(activity_end) - UNIX_TIMESTAMP(activity_start)
      ELSE
        UNIX_TIMESTAMP(LEAST(activity_end, end_time)) - UNIX_TIMESTAMP(GREATEST(activity_start, start_time))
    END AS seconds_within_shift,
    
    -- Calculate seconds outside shift
    CASE 
      WHEN activity_start >= end_time OR activity_end <= start_time THEN
        UNIX_TIMESTAMP(activity_end) - UNIX_TIMESTAMP(activity_start)
      WHEN activity_start >= start_time AND activity_end <= end_time THEN 0
      ELSE
        (UNIX_TIMESTAMP(activity_end) - UNIX_TIMESTAMP(activity_start)) - 
        (UNIX_TIMESTAMP(LEAST(activity_end, end_time)) - UNIX_TIMESTAMP(GREATEST(activity_start, start_time)))
    END AS seconds_outside_shift
  FROM ActivityWithShiftBounds
  WHERE activity_start < activity_end
),

CurrentShiftData AS (
  SELECT
    a.emp_id,
    a.shift_date,
    
    -- Total metrics
    SUM(total_time_active) AS total_active_time,
    SUM(total_time_idle) AS total_idle_time,
    SUM(window_lock_time) AS total_lock_time,
    SUM(total_time_active + total_time_idle + window_lock_time) AS total_work_time,
    
    -- Within shift allocation
    SUM(
      total_time_active * (seconds_within_shift / NULLIF((seconds_within_shift + seconds_outside_shift), 0))
    ) AS active_within_shift,
    
    SUM(
      total_time_idle * (seconds_within_shift / NULLIF((seconds_within_shift + seconds_outside_shift), 0))
    ) AS idle_within_shift,
    
    SUM(
      window_lock_time * (seconds_within_shift / NULLIF((seconds_within_shift + seconds_outside_shift), 0))
    ) AS lock_within_shift,
    
    -- Outside shift allocation
    SUM(
      total_time_active * (seconds_outside_shift / NULLIF((seconds_within_shift + seconds_outside_shift), 0))
    ) AS active_outside_shift,
    
    SUM(
      total_time_idle * (seconds_outside_shift / NULLIF((seconds_within_shift + seconds_outside_shift), 0))
    ) AS idle_outside_shift,
    
    -- Shift configuration
    MAX(s.pulse_shift_time) AS pulse_shift_time,
    MAX(s.adjusted_active_time) AS adjusted_active_time,
    MAX(s.adjusted_lock_time) AS adjusted_lock_time,
    
    -- Period indicator
    CASE 
      WHEN a.shift_date BETWEEN '{start_date}' AND '{end_date}' THEN 'current'
      WHEN a.shift_date BETWEEN '{prev_start_date}' AND '{prev_end_date}' THEN 'previous'
    END AS period
    
  FROM ActivityTimeAllocation a
  JOIN EmployeeShiftsWithConfig s ON a.emp_id = s.emp_id AND a.shift_date = s.shift_date
  GROUP BY a.emp_id, a.shift_date, 
    CASE 
      WHEN a.shift_date BETWEEN '{start_date}' AND '{end_date}' THEN 'current'
      WHEN a.shift_date BETWEEN '{prev_start_date}' AND '{prev_end_date}' THEN 'previous'
    END
),

PeriodAggregates AS (
  SELECT
    period,
    AVG(total_work_time) AS avg_total_work_time,
    AVG(total_active_time) AS avg_total_active_time,
    AVG(total_idle_time) AS avg_total_idle_time,
    AVG(total_lock_time) AS avg_total_lock_time,
    AVG(active_within_shift + idle_within_shift + lock_within_shift) AS avg_total_work_time_within_shift,
    AVG(active_outside_shift + idle_outside_shift) AS avg_total_work_time_outside_shift,
    AVG(pulse_shift_time * 3600) AS avg_pulse_shift_time_seconds,
    AVG(adjusted_active_time * 3600) AS avg_adjusted_active_time_seconds,
    AVG(adjusted_lock_time * 3600) AS avg_adjusted_lock_time_seconds
  FROM CurrentShiftData
  WHERE period IN ('current', 'previous')
  GROUP BY period
),

CurrentPeriod AS (
  SELECT * FROM PeriodAggregates WHERE period = 'current'
),

PreviousPeriod AS (
  SELECT * FROM PeriodAggregates WHERE period = 'previous'
)

SELECT
  -- Current period metrics (use 0 if no data exists)
  COALESCE(curr.avg_total_work_time, 0) AS current_total_work_time,
  COALESCE(curr.avg_total_active_time, 0) AS current_total_active_time,
  COALESCE(curr.avg_total_idle_time, 0) AS current_total_idle_time,
  COALESCE(curr.avg_total_lock_time, 0) AS current_total_lock_time,
  COALESCE(curr.avg_total_work_time_within_shift, 0) AS current_total_work_time_within_shift,
  COALESCE(curr.avg_total_work_time_outside_shift, 0) AS current_total_work_time_outside_shift,
  
  -- Schedule comparisons (only calculate if current data exists)
  CASE 
    WHEN curr.avg_total_work_time IS NOT NULL 
    THEN COALESCE(curr.avg_total_work_time, 0) - COALESCE(curr.avg_pulse_shift_time_seconds, 0)
    ELSE 0 
  END AS total_work_shift_schedule,
  
  CASE 
    WHEN curr.avg_total_work_time IS NULL THEN 'No Data'
    WHEN COALESCE(curr.avg_total_work_time, 0) - COALESCE(curr.avg_pulse_shift_time_seconds, 0) > 0 THEN 'Up'
    WHEN COALESCE(curr.avg_total_work_time, 0) - COALESCE(curr.avg_pulse_shift_time_seconds, 0) < 0 THEN 'Down'
    ELSE 'No Change'
  END AS total_work_shift_schedule_trend,
  
  CASE 
    WHEN curr.avg_total_active_time IS NOT NULL 
    THEN COALESCE(curr.avg_total_active_time, 0) - COALESCE(curr.avg_adjusted_active_time_seconds, 0)
    ELSE 0 
  END AS total_active_shift_schedule,
  
  CASE 
    WHEN curr.avg_total_active_time IS NULL THEN 'No Data'
    WHEN COALESCE(curr.avg_total_active_time, 0) - COALESCE(curr.avg_adjusted_active_time_seconds, 0) > 0 THEN 'Up'
    WHEN COALESCE(curr.avg_total_active_time, 0) - COALESCE(curr.avg_adjusted_active_time_seconds, 0) < 0 THEN 'Down'
    ELSE 'No Change'
  END AS total_active_shift_schedule_trend,
  
  CASE 
    WHEN curr.avg_total_lock_time IS NOT NULL 
    THEN COALESCE(curr.avg_total_lock_time, 0) - COALESCE(curr.avg_adjusted_lock_time_seconds, 0)
    ELSE 0 
  END AS total_lock_shift_schedule,
  
  CASE 
    WHEN curr.avg_total_lock_time IS NULL THEN 'No Data'
    WHEN COALESCE(curr.avg_total_lock_time, 0) - COALESCE(curr.avg_adjusted_lock_time_seconds, 0) > 0 THEN 'Up'
    WHEN COALESCE(curr.avg_total_lock_time, 0) - COALESCE(curr.avg_adjusted_lock_time_seconds, 0) < 0 THEN 'Down'
    ELSE 'No Change'
  END AS total_lock_shift_schedule_trend,
  
  -- Previous period metrics (use 0 if no data exists)
  COALESCE(prev.avg_total_work_time, 0) AS prev_total_work_time,
  COALESCE(prev.avg_total_active_time, 0) AS prev_total_active_time,
  COALESCE(prev.avg_total_idle_time, 0) AS prev_total_idle_time,
  COALESCE(prev.avg_total_lock_time, 0) AS prev_total_lock_time,
  
  -- Period comparison trends (handle missing data scenarios)
  CASE 
    WHEN curr.avg_total_work_time IS NULL OR prev.avg_total_work_time IS NULL THEN 'No Comparison Data'
    WHEN curr.avg_total_work_time > prev.avg_total_work_time THEN 'Up'
    WHEN curr.avg_total_work_time < prev.avg_total_work_time THEN 'Down'
    ELSE 'No Change'
  END AS total_work_trend,
  
  CASE 
    WHEN curr.avg_total_work_time IS NULL OR prev.avg_total_work_time IS NULL OR prev.avg_total_work_time = 0 THEN 0
    ELSE ROUND(((curr.avg_total_work_time - prev.avg_total_work_time) / prev.avg_total_work_time) * 100, 2)
  END AS total_work_percentage_change,
  
  CASE 
    WHEN curr.avg_total_active_time IS NULL OR prev.avg_total_active_time IS NULL THEN 'No Comparison Data'
    WHEN curr.avg_total_active_time > prev.avg_total_active_time THEN 'Up'
    WHEN curr.avg_total_active_time < prev.avg_total_active_time THEN 'Down'
    ELSE 'No Change'
  END AS total_active_trend,
  
  CASE 
    WHEN curr.avg_total_active_time IS NULL OR prev.avg_total_active_time IS NULL OR prev.avg_total_active_time = 0 THEN 0
    ELSE ROUND(((curr.avg_total_active_time - prev.avg_total_active_time) / prev.avg_total_active_time) * 100, 2)
  END AS total_active_percentage_change,
  
  CASE 
    WHEN curr.avg_total_idle_time IS NULL OR prev.avg_total_idle_time IS NULL THEN 'No Comparison Data'
    WHEN curr.avg_total_idle_time > prev.avg_total_idle_time THEN 'Up'
    WHEN curr.avg_total_idle_time < prev.avg_total_idle_time THEN 'Down'
    ELSE 'No Change'
  END AS total_idle_trend,
  
  CASE 
    WHEN curr.avg_total_idle_time IS NULL OR prev.avg_total_idle_time IS NULL OR prev.avg_total_idle_time = 0 THEN 0
    ELSE ROUND(((curr.avg_total_idle_time - prev.avg_total_idle_time) / prev.avg_total_idle_time) * 100, 2)
  END AS total_idle_percentage_change,
  
  CASE 
    WHEN curr.avg_total_lock_time IS NULL OR prev.avg_total_lock_time IS NULL THEN 'No Comparison Data'
    WHEN curr.avg_total_lock_time > prev.avg_total_lock_time THEN 'Up'
    WHEN curr.avg_total_lock_time < prev.avg_total_lock_time THEN 'Down'
    ELSE 'No Change'
  END AS total_lock_trend,
  
  CASE 
    WHEN curr.avg_total_lock_time IS NULL OR prev.avg_total_lock_time IS NULL OR prev.avg_total_lock_time = 0 THEN 0
    ELSE ROUND(((curr.avg_total_lock_time - prev.avg_total_lock_time) / prev.avg_total_lock_time) * 100, 2)
  END AS total_lock_percentage_change
  
FROM 
  (SELECT 1 AS join_key) AS dummy
LEFT JOIN CurrentPeriod curr ON 1=1
LEFT JOIN PreviousPeriod prev ON 1=1






WITH FilteredEmployees AS (
  SELECT
    emplid AS emp_id,
    COALESCE(country, 'Default') AS country
  FROM inbound.hr_employee_central
  WHERE func_mar_id = 43560069
    AND (TERMINATION_DT > CURRENT_TIMESTAMP() OR TERMINATION_DT IS NULL)
),

EmployeeShiftsWithConfig AS (
  SELECT
    l.emp_id,
    l.shift_date,
    TO_TIMESTAMP(l.start_time, 'yyyy-MM-dd HH:mm:ss') AS start_time,
    TO_TIMESTAMP(l.end_time, 'yyyy-MM-dd HH:mm:ss') AS end_time,
    TO_TIMESTAMP(l.emp_login_time, 'yyyy-MM-dd HH:mm:ss') AS emp_login_time,
    TO_TIMESTAMP(l.emp_logout_time, 'yyyy-MM-dd HH:mm:ss') AS emp_logout_time,
    (UNIX_TIMESTAMP(l.end_time) - UNIX_TIMESTAMP(l.start_time)) / 3600.0 AS pulse_shift_time,
    (COALESCE(o.shift_dur_hrs, 8) * (UNIX_TIMESTAMP(l.end_time) - UNIX_TIMESTAMP(l.start_time)) / 3600.0 / COALESCE(o.shift_length_hrs, 9)) AS adjusted_active_time,
    (COALESCE(o.break_dur_hrs, 1) * (UNIX_TIMESTAMP(l.end_time) - UNIX_TIMESTAMP(l.start_time)) / 3600.0 / COALESCE(o.shift_length_hrs, 9)) AS adjusted_lock_time
  FROM gold_dashboard.analytics_emp_login_logout l
  JOIN FilteredEmployees e ON l.emp_id = e.emp_id
  LEFT JOIN inbound.countrywise_shift o ON e.country = o.country
  WHERE (l.shift_date BETWEEN '{start_date}' AND '{end_date}' OR l.shift_date BETWEEN '{prev_start_date}' AND '{prev_end_date}')
    AND l.is_weekoff = false
    AND l.emp_login_time IS NOT NULL
    AND l.emp_logout_time IS NOT NULL
),

FilteredAppInfo AS (
  SELECT
    i.emp_id,
    i.cal_date AS shift_date,
    TO_TIMESTAMP(CONCAT(i.cal_date, ' ', SUBSTRING(i.interval, 1, 5), ':00'), 'yyyy-MM-dd HH:mm:ss') AS interval_start,
    TO_TIMESTAMP(CONCAT(i.cal_date, ' ', SUBSTRING(i.interval, 7, 5), ':00'), 'yyyy-MM-dd HH:mm:ss') AS interval_end,
    i.total_time_spent_active AS total_time_active,
    i.total_time_spent_idle AS total_time_idle,
    i.window_lock_time
  FROM gold_dashboard.analytics_emp_app_info i
  JOIN FilteredEmployees e ON i.emp_id = e.emp_id
  WHERE (i.cal_date BETWEEN '{start_date}' AND '{end_date}' OR i.cal_date BETWEEN '{prev_start_date}' AND '{prev_end_date}')
),

ActivityWithShiftBounds AS (
  SELECT
    i.emp_id,
    i.shift_date,
    s.start_time,
    s.end_time,
    i.interval_start,
    i.interval_end,
    s.emp_login_time,
    s.emp_logout_time,
    i.total_time_active,
    i.total_time_idle,
    i.window_lock_time,
    GREATEST(i.interval_start, s.start_time) AS overlap_start,
    LEAST(i.interval_end, s.end_time) AS overlap_end,
    GREATEST(i.interval_start, s.emp_login_time) AS activity_start,
    LEAST(i.interval_end, s.emp_logout_time) AS activity_end
  FROM FilteredAppInfo i
  JOIN EmployeeShiftsWithConfig s ON i.emp_id = s.emp_id AND i.shift_date = s.shift_date
  WHERE s.emp_login_time < i.interval_end
    AND s.emp_logout_time > i.interval_start
),

ActivityTimeAllocation AS (
  SELECT
    emp_id,
    shift_date,
    total_time_active,
    total_time_idle,
    window_lock_time,
    start_time,
    end_time,
    
    -- Calculate seconds within shift
    CASE 
      WHEN activity_start >= end_time OR activity_end <= start_time THEN 0
      WHEN activity_start >= start_time AND activity_end <= end_time THEN
        UNIX_TIMESTAMP(activity_end) - UNIX_TIMESTAMP(activity_start)
      ELSE
        UNIX_TIMESTAMP(LEAST(activity_end, end_time)) - UNIX_TIMESTAMP(GREATEST(activity_start, start_time))
    END AS seconds_within_shift,
    
    -- Calculate seconds outside shift
    CASE 
      WHEN activity_start >= end_time OR activity_end <= start_time THEN
        UNIX_TIMESTAMP(activity_end) - UNIX_TIMESTAMP(activity_start)
      WHEN activity_start >= start_time AND activity_end <= end_time THEN 0
      ELSE
        (UNIX_TIMESTAMP(activity_end) - UNIX_TIMESTAMP(activity_start)) - 
        (UNIX_TIMESTAMP(LEAST(activity_end, end_time)) - UNIX_TIMESTAMP(GREATEST(activity_start, start_time)))
    END AS seconds_outside_shift
  FROM ActivityWithShiftBounds
  WHERE activity_start < activity_end
),

CurrentShiftData AS (
  SELECT
    a.emp_id,
    a.shift_date,
    
    -- Total metrics
    SUM(total_time_active) AS total_active_time,
    SUM(total_time_idle) AS total_idle_time,
    SUM(window_lock_time) AS total_lock_time,
    SUM(total_time_active + total_time_idle + window_lock_time) AS total_work_time,
    
    -- Within shift allocation
    SUM(
      total_time_active * (seconds_within_shift / NULLIF((seconds_within_shift + seconds_outside_shift), 0))
    ) AS active_within_shift,
    
    SUM(
      total_time_idle * (seconds_within_shift / NULLIF((seconds_within_shift + seconds_outside_shift), 0))
    ) AS idle_within_shift,
    
    SUM(
      window_lock_time * (seconds_within_shift / NULLIF((seconds_within_shift + seconds_outside_shift), 0))
    ) AS lock_within_shift,
    
    -- Outside shift allocation
    SUM(
      total_time_active * (seconds_outside_shift / NULLIF((seconds_within_shift + seconds_outside_shift), 0))
    ) AS active_outside_shift,
    
    SUM(
      total_time_idle * (seconds_outside_shift / NULLIF((seconds_within_shift + seconds_outside_shift), 0))
    ) AS idle_outside_shift,
    
    -- Shift configuration
    MAX(s.pulse_shift_time) AS pulse_shift_time,
    MAX(s.adjusted_active_time) AS adjusted_active_time,
    MAX(s.adjusted_lock_time) AS adjusted_lock_time,
    
    -- Period indicator
    CASE 
      WHEN a.shift_date BETWEEN '{start_date}' AND '{end_date}' THEN 'current'
      WHEN a.shift_date BETWEEN '{prev_start_date}' AND '{prev_end_date}' THEN 'previous'
    END AS period
    
  FROM ActivityTimeAllocation a
  JOIN EmployeeShiftsWithConfig s ON a.emp_id = s.emp_id AND a.shift_date = s.shift_date
  GROUP BY a.emp_id, a.shift_date, 
    CASE 
      WHEN a.shift_date BETWEEN '{start_date}' AND '{end_date}' THEN 'current'
      WHEN a.shift_date BETWEEN '{prev_start_date}' AND '{prev_end_date}' THEN 'previous'
    END
),

PeriodAggregates AS (
  SELECT
    period,
    AVG(total_work_time) AS avg_total_work_time,
    AVG(total_active_time) AS avg_total_active_time,
    AVG(total_idle_time) AS avg_total_idle_time,
    AVG(total_lock_time) AS avg_total_lock_time,
    AVG(active_within_shift + idle_within_shift + lock_within_shift) AS avg_total_work_time_within_shift,
    AVG(active_outside_shift + idle_outside_shift) AS avg_total_work_time_outside_shift,
    AVG(pulse_shift_time * 3600) AS avg_pulse_shift_time_seconds,
    AVG(adjusted_active_time * 3600) AS avg_adjusted_active_time_seconds,
    AVG(adjusted_lock_time * 3600) AS avg_adjusted_lock_time_seconds
  FROM CurrentShiftData
  WHERE period IN ('current', 'previous')
  GROUP BY period
)

SELECT
  -- Current period metrics
  curr.avg_total_work_time AS current_total_work_time,
  curr.avg_total_active_time AS current_total_active_time,
  curr.avg_total_idle_time AS current_total_idle_time,
  curr.avg_total_lock_time AS current_total_lock_time,
  curr.avg_total_work_time_within_shift AS current_total_work_time_within_shift,
  curr.avg_total_work_time_outside_shift AS current_total_work_time_outside_shift,
  
  -- Schedule comparisons
  (curr.avg_total_work_time - curr.avg_pulse_shift_time_seconds) AS total_work_shift_schedule,
  CASE 
    WHEN (curr.avg_total_work_time - curr.avg_pulse_shift_time_seconds) > 0 THEN 'Up'
    WHEN (curr.avg_total_work_time - curr.avg_pulse_shift_time_seconds) < 0 THEN 'Down'
    ELSE 'No Change'
  END AS total_work_shift_schedule_trend,
  
  (curr.avg_total_active_time - curr.avg_adjusted_active_time_seconds) AS total_active_shift_schedule,
  CASE 
    WHEN (curr.avg_total_active_time - curr.avg_adjusted_active_time_seconds) > 0 THEN 'Up'
    WHEN (curr.avg_total_active_time - curr.avg_adjusted_active_time_seconds) < 0 THEN 'Down'
    ELSE 'No Change'
  END AS total_active_shift_schedule_trend,
  
  (curr.avg_total_lock_time - curr.avg_adjusted_lock_time_seconds) AS total_lock_shift_schedule,
  CASE 
    WHEN (curr.avg_total_lock_time - curr.avg_adjusted_lock_time_seconds) > 0 THEN 'Up'
    WHEN (curr.avg_total_lock_time - curr.avg_adjusted_lock_time_seconds) < 0 THEN 'Down'
    ELSE 'No Change'
  END AS total_lock_shift_schedule_trend,
  
  -- Previous period metrics
  prev.avg_total_work_time AS prev_total_work_time,
  prev.avg_total_active_time AS prev_total_active_time,
  prev.avg_total_idle_time AS prev_total_idle_time,
  prev.avg_total_lock_time AS prev_total_lock_time,
  
  -- Period comparison trends
  CASE 
    WHEN curr.avg_total_work_time > prev.avg_total_work_time THEN 'Up'
    WHEN curr.avg_total_work_time < prev.avg_total_work_time THEN 'Down'
    ELSE 'No Change'
  END AS total_work_trend,
  
  ROUND(((curr.avg_total_work_time - prev.avg_total_work_time) / NULLIF(prev.avg_total_work_time, 0)) * 100, 2) AS total_work_percentage_change,
  
  CASE 
    WHEN curr.avg_total_active_time > prev.avg_total_active_time THEN 'Up'
    WHEN curr.avg_total_active_time < prev.avg_total_active_time THEN 'Down'
    ELSE 'No Change'
  END AS total_active_trend,
  
  ROUND(((curr.avg_total_active_time - prev.avg_total_active_time) / NULLIF(prev.avg_total_active_time, 0)) * 100, 2) AS total_active_percentage_change,
  
  CASE 
    WHEN curr.avg_total_idle_time > prev.avg_total_idle_time THEN 'Up'
    WHEN curr.avg_total_idle_time < prev.avg_total_idle_time THEN 'Down'
    ELSE 'No Change'
  END AS total_idle_trend,
  
  ROUND(((curr.avg_total_idle_time - prev.avg_total_idle_time) / NULLIF(prev.avg_total_idle_time, 0)) * 100, 2) AS total_idle_percentage_change,
  
  CASE 
    WHEN curr.avg_total_lock_time > prev.avg_total_lock_time THEN 'Up'
    WHEN curr.avg_total_lock_time < prev.avg_total_lock_time THEN 'Down'
    ELSE 'No Change'
  END AS total_lock_trend,
  
  ROUND(((curr.avg_total_lock_time - prev.avg_total_lock_time) / NULLIF(prev.avg_total_lock_time, 0)) * 100, 2) AS total_lock_percentage_change
  
FROM 
  (SELECT * FROM PeriodAggregates WHERE period = 'current') curr
CROSS JOIN
  (SELECT * FROM PeriodAggregates WHERE period = 'previous') prev
WHERE curr.avg_total_work_time IS NOT NULL AND prev.avg_total_work_time IS NOT NULL






---------
-----------------
WITH ActivityWithShiftBounds AS (
  SELECT
    i.emp_id,
    i.shift_date,
    i.start_time,
    i.end_time,
    i.interval_start,
    i.interval_end,
    i.emp_login_time,
    i.emp_logout_time,
    i.total_time_active,
    i.total_time_idle,
    i.window_lock_time,
    greatest(i.interval_start, i.start_time) AS overlap_start,
    least(i.interval_end, i.end_time) AS overlap_end,
    greatest(i.interval_start, i.emp_login_time) AS activity_start,
    least(i.interval_end, i.emp_logout_time) AS activity_end
  FROM FilteredAppInfo i
  LEFT JOIN EmployeeShiftsWithConfig s
    ON i.emp_id = s.emp_id AND i.shift_date = s.shift_date
),

ActivityTimeAllocation AS (
  SELECT
    emp_id,
    shift_date,
    total_time_active,
    total_time_idle,
    window_lock_time,
    start_time,
    end_time,
    activity_start,
    activity_end,
    
    -- Calculate seconds within shift (considers BOTH login/logout AND shift boundaries)
    CASE 
      WHEN activity_start >= end_time OR activity_end <= start_time THEN
        0
      WHEN activity_start >= start_time AND activity_end <= end_time THEN
        unix_timestamp(activity_end) - unix_timestamp(activity_start)
      ELSE
        unix_timestamp(least(activity_end, end_time)) - unix_timestamp(greatest(activity_start, start_time))
    END AS seconds_within_shift,
    
    -- Calculate seconds outside shift
    CASE 
      WHEN activity_start >= end_time OR activity_end <= start_time THEN
        unix_timestamp(activity_end) - unix_timestamp(activity_start)
      WHEN activity_start >= start_time AND activity_end <= end_time THEN
        0
      ELSE
        (unix_timestamp(activity_end) - unix_timestamp(activity_start)) - 
        (unix_timestamp(least(activity_end, end_time)) - unix_timestamp(greatest(activity_start, start_time)))
    END AS seconds_outside_shift,
    
    -- Total activity seconds for validation
    unix_timestamp(activity_end) - unix_timestamp(activity_start) AS total_activity_seconds
    
  FROM ActivityWithShiftBounds
  WHERE activity_start < activity_end
),

CurrentShiftData AS (
  SELECT
    a.emp_id,
    a.shift_date,
    
    -- Total metrics
    SUM(total_time_active) AS total_active_time,
    SUM(total_time_idle) AS total_idle_time,
    SUM(window_lock_time) AS total_lock_time,
    SUM(total_time_active + total_time_idle + window_lock_time) AS total_work_time,
    
    -- Within shift allocation
    SUM(
      CASE 
        WHEN (seconds_within_shift + seconds_outside_shift) = 0 THEN 0
        ELSE total_time_active * (seconds_within_shift / (seconds_within_shift + seconds_outside_shift))
      END
    ) AS active_within_shift,
    
    SUM(
      CASE 
        WHEN (seconds_within_shift + seconds_outside_shift) = 0 THEN 0
        ELSE total_time_idle * (seconds_within_shift / (seconds_within_shift + seconds_outside_shift))
      END
    ) AS idle_within_shift,
    
    SUM(
      CASE 
        WHEN (seconds_within_shift + seconds_outside_shift) = 0 THEN 0
        ELSE window_lock_time * (seconds_within_shift / (seconds_within_shift + seconds_outside_shift))
      END
    ) AS lock_within_shift,
    
    -- Outside shift allocation (only active + idle, no lock time)
    SUM(
      CASE 
        WHEN (seconds_within_shift + seconds_outside_shift) = 0 THEN 0
        ELSE total_time_active * (seconds_outside_shift / (seconds_within_shift + seconds_outside_shift))
      END
    ) AS active_outside_shift,
    
    SUM(
      CASE 
        WHEN (seconds_within_shift + seconds_outside_shift) = 0 THEN 0
        ELSE total_time_idle * (seconds_outside_shift / (seconds_within_shift + seconds_outside_shift))
      END
    ) AS idle_outside_shift,
    
    -- Shift configuration
    MAX(s.pulse_shift_time) AS pulse_shift_time,
    MAX(s.adjusted_active_time) AS adjusted_active_time,
    MAX(s.adjusted_lock_time) AS adjusted_lock_time,
    MIN(s.start_time) AS shift_start_time,
    MAX(s.end_time) AS shift_end_time,
    
    -- Debug totals for validation
    SUM(seconds_within_shift) AS total_seconds_within_shift,
    SUM(seconds_outside_shift) AS total_seconds_outside_shift,
    SUM(total_activity_seconds) AS total_activity_seconds
    
  FROM ActivityTimeAllocation a
  JOIN EmployeeShiftsWithConfig s
    ON a.emp_id = s.emp_id AND a.shift_date = s.shift_date
  GROUP BY a.emp_id, a.shift_date
)

SELECT
  emp_id,
  shift_date,
  total_active_time,
  total_idle_time,
  total_lock_time,
  total_work_time,
  active_within_shift,
  idle_within_shift,
  lock_within_shift,
  active_outside_shift,
  idle_outside_shift,
  (active_within_shift + idle_within_shift + lock_within_shift) AS time_within_shift,
  (active_outside_shift + idle_outside_shift) AS time_outside_shift,
  pulse_shift_time,
  adjusted_active_time,
  adjusted_lock_time,
  shift_start_time,
  shift_end_time,
  -- Debug columns (can remove in production)
  total_seconds_within_shift,
  total_seconds_outside_shift,
  total_activity_seconds
FROM CurrentShiftData
ORDER BY emp_id, shift_date






Let me walk through your examples using Approach 1 logic:

Example 1: Shift 10:25:00 - 19:25:00, Login 10:45:12, Logout 20:42:45

For 10:00-11:00 interval:

Â· Activity: 10:45:12 - 11:00:00 (14m48s)
Â· seconds_within_shift = unix_timestamp('11:00:00') - unix_timestamp('10:45:12') = 888 seconds
Â· seconds_outside_shift = 0 seconds (all within shift)
Â· Ratio = 888/(888+0) = 1.0
Â· Result: 100% of active/idle/lock time allocated to WITHIN shift

For 19:00-20:00 interval:

Â· Activity: 19:00:00 - 20:00:00 (60m)
Â· Within shift portion: 19:00:00 - 19:25:00 (25m)
Â· Outside shift portion: 19:25:00 - 20:00:00 (35m)
Â· seconds_within_shift = unix_timestamp('19:25:00') - unix_timestamp('19:00:00') = 1500 seconds
Â· seconds_outside_shift = unix_timestamp('20:00:00') - unix_timestamp('19:25:00') = 2100 seconds
Â· Ratio = 1500/(1500+2100) = 0.4167 (within), 0.5833 (outside)
Â· Result: 41.67% of active/idle/lock to WITHIN shift, 58.33% of active/idle to OUTSIDE shift

For 20:00-21:00 interval:

Â· Activity: 20:00:00 - 20:42:45 (42m45s)
Â· seconds_within_shift = 0 seconds (completely after shift end)
Â· seconds_outside_shift = unix_timestamp('20:42:45') - unix_timestamp('20:00:00') = 2565 seconds
Â· Ratio = 0/(0+2565) = 0.0 (within), 1.0 (outside)
Â· Result: 100% of active/idle to OUTSIDE shift, 0% lock time

---

Example 2: Shift 10:25:00 - 19:25:00, Login 09:12:14, Logout 16:20:00

For 09:00-10:00 interval:

Â· Activity: 09:12:14 - 10:00:00 (47m46s)
Â· seconds_within_shift = 0 seconds (completely before shift start)
Â· seconds_outside_shift = unix_timestamp('10:00:00') - unix_timestamp('09:12:14') = 2866 seconds
Â· Ratio = 0/(0+2866) = 0.0 (within), 1.0 (outside)
Â· Result: 100% of active/idle to OUTSIDE shift

For 10:00-11:00 interval:

Â· Activity: 10:00:00 - 11:00:00 (60m)
Â· Within shift portion: 10:25:00 - 11:00:00 (35m)
Â· Outside shift portion: 10:00:00 - 10:25:00 (25m)
Â· seconds_within_shift = unix_timestamp('11:00:00') - unix_timestamp('10:25:00') = 2100 seconds
Â· seconds_outside_shift = unix_timestamp('10:25:00') - unix_timestamp('10:00:00') = 1500 seconds
Â· Ratio = 2100/(2100+1500) = 0.5833 (within), 0.4167 (outside)
Â· Result: 58.33% of active/idle/lock to WITHIN shift, 41.67% of active/idle to OUTSIDE shift

---

Example 3: Shift 09:30:00 - 18:30:00, Login 10:15:12, Logout 20:42:45

For 10:00-11:00 interval:

Â· Activity: 10:15:12 - 11:00:00 (44m48s)
Â· seconds_within_shift = unix_timestamp('11:00:00') - unix_timestamp('10:15:12') = 2688 seconds
Â· seconds_outside_shift = 0 seconds (all within shift)
Â· Ratio = 2688/(2688+0) = 1.0
Â· Result: 100% of active/idle/lock to WITHIN shift

For 18:00-19:00 interval:

Â· Activity: 18:00:00 - 19:00:00 (60m)
Â· Within shift portion: 18:00:00 - 18:30:00 (30m)
Â· Outside shift portion: 18:30:00 - 19:00:00 (30m)
Â· seconds_within_shift = unix_timestamp('18:30:00') - unix_timestamp('18:00:00') = 1800 seconds
Â· seconds_outside_shift = unix_timestamp('19:00:00') - unix_timestamp('18:30:00') = 1800 seconds
Â· Ratio = 1800/(1800+1800) = 0.5 (within), 0.5 (outside)
Â· Result: 50% of active/idle/lock to WITHIN shift, 50% of active/idle to OUTSIDE shift

Key Points:

1. Automatic handling: The ratio seconds_within_shift/(total_seconds) automatically gives correct allocation
2. Complete within: Ratio = 1.0 â†’ 100% within shift
3. Complete outside: Ratio = 0.0 â†’ 100% outside shift
4. Partial overlap: Exact proportional split based on time
5. Lock time: Only gets allocated to within-shift portion (as per business rule)

The logic is mathematically precise and handles all your scenarios correctly!






from datetime import datetime, date, timedelta
from typing import Optional
from fastapi import Query, Header, Depends
from fastapi.responses import JSONResponse
from fastapi import status
from auth.jwt_handler import AuthJWT

def get_last_day_of_month(dt: date) -> date:
    """Get the last day of the month for a given date"""
    if dt.month == 12:
        return date(dt.year + 1, 1, 1) - timedelta(days=1)
    return date(dt.year, dt.month + 1, 1) - timedelta(days=1)

@router.get('/get_team_active_idle_app_category')
async def get_team_app_activity(
    Authorize: AuthJWT = Depends(),
    authorization: str = Header(None),
    start_date: str = Query(...),
    end_date: str = Query(...),
    date_range: Optional[str] = Query("day")
):
    if not authorization or not authorization.startswith("Bearer"):
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"msg": "Authorization Token Missing"}
        )
    
    try:
        Authorize.jwt_required()
        user_identity = Authorize.get_jwt_subject()
        claims = Authorize.get_raw_jwt()
        manager_id = claims.get("user_id")
        
        if not manager_id:
            return JSONResponse(
                status_code=status.HTTP_401_UNAUTHORIZED,
                content={'message': "Invalid Token Claims"}
            )
        
        today = datetime.today().date()
        
        if start_date and end_date:
            try:
                start_date_dt = datetime.strptime(start_date, "%Y-%m-%d").date()
                end_date_dt = datetime.strptime(end_date, "%Y-%m-%d").date()
                
                period_days = (end_date_dt - start_date_dt).days + 1
                
                # Handle different date range types
                if date_range.lower() == "day":
                    # Previous week's same day
                    prev_start_date = start_date_dt - timedelta(days=7)
                    prev_end_date = prev_start_date
                    ext_end_date = end_date_dt + timedelta(days=1)
                    
                elif date_range.lower() == "week":
                    # Previous week's same days
                    prev_start_date = start_date_dt - timedelta(days=7)
                    prev_end_date = end_date_dt - timedelta(days=7)
                    ext_end_date = end_date_dt + timedelta(days=1)
                    
                elif date_range.lower() == "month":
                    # Check if selected dates are in current month
                    current_month = today.month
                    current_year = today.year
                    selected_month = start_date_dt.month
                    selected_year = start_date_dt.year
                    
                    if selected_month == current_month and selected_year == current_year:
                        # Current month: go back exactly 4 weeks (28 days)
                        prev_start_date = start_date_dt - timedelta(days=28)
                        prev_end_date = end_date_dt - timedelta(days=28)
                    else:
                        # Not current month: previous month's start and end
                        if start_date_dt.month == 1:
                            prev_start_date = date(start_date_dt.year - 1, 12, 1)
                        else:
                            prev_start_date = date(start_date_dt.year, start_date_dt.month - 1, 1)
                        
                        if end_date_dt.month == 1:
                            prev_end_date = date(end_date_dt.year - 1, 12, 31)
                        else:
                            prev_month = end_date_dt.month - 1
                            prev_year = end_date_dt.year
                            if prev_month == 0:
                                prev_month = 12
                                prev_year = end_date_dt.year - 1
                            prev_end_date = get_last_day_of_month(date(prev_year, prev_month, 1))
                    
                    ext_end_date = end_date_dt + timedelta(days=1)
                
                else:
                    # DEFAULT: Exact period days logic for unknown date_range values
                    prev_end_date = start_date_dt - timedelta(days=1)
                    prev_start_date = prev_end_date - timedelta(days=period_days - 1)
                    ext_end_date = end_date_dt + timedelta(days=1)
                
                return {
                    "start_date": start_date_dt.isoformat(),
                    "end_date": end_date_dt.isoformat(),
                    "previous_start_date": prev_start_date.isoformat(),
                    "previous_end_date": prev_end_date.isoformat(),
                    "extended_end_date": ext_end_date.isoformat(),
                    "date_range": date_range,
                    "period_days": period_days
                }
                
            except ValueError:
                return JSONResponse(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    content={"msg": "Invalid date format. Use YYYY-MM-DD."}
                )
        else:
            # Default to daily
            start_date_dt = today
            end_date_dt = today
            period_days = 1
            prev_end_date = start_date_dt - timedelta(days=1)
            prev_start_date = prev_end_date - timedelta(days=period_days - 1)
            ext_end_date = end_date_dt + timedelta(days=1)
            
            return {
                "start_date": start_date_dt.isoformat(),
                "end_date": end_date_dt.isoformat(),
                "previous_start_date": prev_start_date.isoformat(),
                "previous_end_date": prev_end_date.isoformat(),
                "extended_end_date": ext_end_date.isoformat(),
                "date_range": "day",
                "period_days": period_days
            }
            
    except Exception as e:
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={"msg": f"Internal server error: {str(e)}"}
        )




pip installs python-dateutil

from datetime import datetime, date, timedelta
from dateutil.relativedelta import relativedelta
from typing import Optional
from fastapi import Query, Header, Depends
from fastapi.responses import JSONResponse
from fastapi import status
from auth.jwt_handler import AuthJWT

@router.get('/get_team_active_idle_app_category')
async def get_team_app_activity(
    Authorize: AuthJWT = Depends(),
    authorization: str = Header(None),
    start_date: str = Query(...),
    end_date: str = Query(...),
    date_range: Optional[str] = Query("day")
):
    if not authorization or not authorization.startswith("Bearer"):
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"msg": "Authorization Token Missing"}
        )
    
    try:
        Authorize.jwt_required()
        user_identity = Authorize.get_jwt_subject()
        claims = Authorize.get_raw_jwt()
        manager_id = claims.get("user_id")
        
        if not manager_id:
            return JSONResponse(
                status_code=status.HTTP_401_UNAUTHORIZED,
                content={'message': "Invalid Token Claims"}
            )
        
        today = datetime.today().date()
        
        if start_date and end_date:
            try:
                start_date_dt = datetime.strptime(start_date, "%Y-%m-%d").date()
                end_date_dt = datetime.strptime(end_date, "%Y-%m-%d").date()
                
                period_days = (end_date_dt - start_date_dt).days + 1
                
                # Handle different date range types
                if date_range.lower() == "day":
                    # Previous week's same day
                    prev_start_date = start_date_dt - timedelta(days=7)
                    prev_end_date = prev_start_date  # Same day for day range
                    ext_end_date = end_date_dt + timedelta(days=1)
                    
                elif date_range.lower() == "week":
                    # Previous week's same days (exact number of days)
                    prev_start_date = start_date_dt - timedelta(days=7)
                    prev_end_date = end_date_dt - timedelta(days=7)
                    ext_end_date = end_date_dt + timedelta(days=1)
                    
                elif date_range.lower() == "month":
                    # Check if selected dates are in current month
                    current_month = today.month
                    current_year = today.year
                    selected_month = start_date_dt.month
                    selected_year = start_date_dt.year
                    
                    if selected_month == current_month and selected_year == current_year:
                        # Current month: go back exactly 4 weeks (28 days)
                        prev_start_date = start_date_dt - timedelta(days=28)
                        prev_end_date = end_date_dt - timedelta(days=28)
                    else:
                        # Not current month: previous month's start and end
                        # Get first day of previous month for start_date
                        prev_start_date = (start_date_dt.replace(day=1) - relativedelta(months=1)).replace(day=1)
                        
                        # Get last day of previous month for end_date
                        prev_month_end = (end_date_dt.replace(day=1) - relativedelta(months=1))
                        prev_end_date = (prev_month_end + relativedelta(months=1, days=-1))
                    
                    ext_end_date = end_date_dt + timedelta(days=1)
                
                else:
                    # Default to day logic for unknown ranges
                    prev_start_date = start_date_dt - timedelta(days=7)
                    prev_end_date = prev_start_date
                    ext_end_date = end_date_dt + timedelta(days=1)
                
                # Return the calculated dates
                return {
                    "start_date": start_date_dt.isoformat(),
                    "end_date": end_date_dt.isoformat(),
                    "previous_start_date": prev_start_date.isoformat(),
                    "previous_end_date": prev_end_date.isoformat(),
                    "extended_end_date": ext_end_date.isoformat(),
                    "date_range": date_range,
                    "period_days": period_days
                }
                
            except ValueError:
                return JSONResponse(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    content={"msg": "Invalid date format. Use YYYY-MM-DD."}
                )
        else:
            # Default to daily when no dates provided
            start_date_dt = today
            end_date_dt = today
            prev_start_date = today - timedelta(days=7)
            prev_end_date = prev_start_date
            ext_end_date = today + timedelta(days=1)
            
            return {
                "start_date": start_date_dt.isoformat(),
                "end_date": end_date_dt.isoformat(),
                "previous_start_date": prev_start_date.isoformat(),
                "previous_end_date": prev_end_date.isoformat(),
                "extended_end_date": ext_end_date.isoformat(),
                "date_range": "day",
                "period_days": 1
            }
            
    except Exception as e:
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={"msg": f"Internal server error: {str(e)}"}
        )







Hereâ€™s a cleaner, structured draft you can use for your proposal email or document:


---

Subject: Proposal to Exclude Week-Off Data from Average Calculations in RPEM APIs

Background:
Currently, our system captures and stores agent activity data for all employees, regardless of whether the date falls on a working day or a week-off (e.g., Saturday, Sunday, or designated weekly off-days). This includes cases where an employee might log in briefly (e.g., 10 minutes or 1 hour) during their week-off.

When average calculations are performed â€” whether for a week, month, or custom date range â€” this week-off data is included in the calculation. As a result:

A small amount of activity on a week-off skews both individual and team averages.

The reported averages may not accurately reflect true working-day performance.


Proposed Change:
For all APIs that calculate averages in RPEM, we propose excluding week-off employee activity from the average computation. This means:

If a date is a week-off for a given employee, their data for that date will be ignored in average calculations.

Averages will be calculated only on valid working days for each employee.


Expected Benefits:

More accurate and meaningful average metrics for individuals and teams.

Avoids distortion of averages caused by minimal week-off activity.

Consistency across all APIs, regardless of the selected time period filter (week, month, or custom).


Scope:

Changes will be implemented in all RPEM APIs that involve average calculations.

The logic to exclude week-off data will be applied at the calculation stage.


Request:
We seek sign-off on this proposal so that development can proceed with updating all relevant APIs to exclude week-off data from average computations.


---

Do you want me to also add a visual example showing how averages change before vs. after this change? That would make the proposal even easier to approve.

