WHERE file_description IS NOT NULL
  AND TRIM(file_description) <> ''
  AND UPPER(TRIM(file_description)) NOT IN ('NA', 'N/A')

  -- Exclude single-character values
  AND LENGTH(TRIM(file_description)) > 1

  -- Exclude values with only special characters
  AND NOT REGEXP_LIKE(
        file_description,
        '^[^A-Za-z0-9]+$'
      )

  -- Exclude URLs
  AND NOT REGEXP_LIKE(
        LOWER(file_description),
        '^(http://|https://|/)'
      )

  -- Exclude Windows / drive file paths (C:\, D:\, etc.)
  AND NOT REGEXP_LIKE(
        file_description,
        '^[A-Za-z]:\\'
      )

COMPLETE CODE: Day Employee Alerts

```python
# day_emp_alerts_complete.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, lit, udf, to_json, struct, array, countDistinct
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, BooleanType, TimestampType, MapType
import json
from datetime import datetime, timedelta
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class DayEmpAlertsProcessor:
    def __init__(self, spark):
        self.spark = spark
        self.alert_messages = {
            1: "Quick tip! You're not taking enough breaks. Try adding short pauses to stay productive.",
            2: "Heads up! Your active time is low. Review your tasks or engagement level.",
            3: "You've been working hard! Try balancing your workload to avoid burnout.",
            4: "Heads up! Your activity is low and idle time is high. Check for engagement or technical issues.",
            5: "Quick tip! Your session shows low active and idle time. Check for system issues or increase task engagement.",
            6: "Heads up! Your schedule shows high activity and idle time. This may indicate task balancing or tool issues.",
            7: "Quick tip! Your active time is low, idle time is high, and you're missing breaks. Check your setup to improve engagement.",
            8: "Quick tip! You're working long hours with idle stretches and missing breaks. Time to rebalance your workload.",
            9: "Quick tip! System interaction is low and breaks are limited. Check your setup or connectivity.",
            10: "Heads up! Your activity is low and idle time is high. Check for engagement or technical challenges.",
            11: "Quick tip! Your session had low activity and idle time. Check for system issues or increase engagement.",
            12: "Heads up! High activity mixed with idle periods suggests task or tool inefficiencies.",
            13: "Quick tip! You're missing breaks and activity is low. Check your setup to boost engagement and well-being.",
            14: "Quick tip! You've been working hard with few breaks. Try balancing your workload for a healthier routine."
        }
    
    def get_updated_records(self):
        """
        Get all emp_id and shift_date that need recalculation from loginlogout
        """
        logger.info("Fetching updated records from loginlogout...")
        
        four_hours_ago = (datetime.now() - timedelta(hours=4)).strftime('%Y-%m-%d %H:%M:%S')
        
        query = f"""
            SELECT DISTINCT 
                emp_id,
                shift_date,
                manager_id
            FROM loginlogout 
            WHERE updated_on > '{four_hours_ago}'
            AND logintime IS NOT NULL
            AND logouttime IS NOT NULL
            AND shift_date IS NOT NULL
        """
        
        updated_df = self.spark.sql(query)
        logger.info(f"Found {updated_df.count()} employee-date combinations to process")
        
        return updated_df
    
    def get_day_emp_data(self, trigger_df):
        """
        Get day_emp data for all triggered records
        """
        logger.info("Fetching day_emp data for triggered records...")
        
        # Create temp view
        trigger_df.createOrReplaceTempView("trigger_records")
        
        query = """
            SELECT 
                de.emp_id,
                de.shift_date,
                tr.manager_id,
                de.total_time,
                de.active_time,
                de.idle_time,
                de.lock_time,
                de.expected_shift_time,
                de.expected_break_time,
                de.updated_at
            FROM day_emp de
            INNER JOIN trigger_records tr 
                ON de.emp_id = tr.emp_id 
                AND de.shift_date = tr.shift_date
            WHERE de.active_time IS NOT NULL
        """
        
        day_data = self.spark.sql(query)
        logger.info(f"Found {day_data.count()} matching records in day_emp")
        
        return day_data
    
    def calculate_alerts(self, day_data_df):
        """
        Calculate all 14 alert conditions for batch of records
        """
        logger.info("Calculating alert conditions...")
        
        # Calculate Active Time Percentage Ratio
        df = day_data_df.withColumn(
            "total_interaction_time",
            col("active_time") + col("idle_time") + col("lock_time")
        ).withColumn(
            "active_time_percent_ratio",
            when(col("total_interaction_time") > 0,
                 col("active_time") * 100.0 / col("total_interaction_time")
            ).otherwise(0)
        )
        
        # ALERT 1: Few Breaks
        df = df.withColumn(
            "alert_1",
            (col("idle_time") + col("lock_time")) < col("expected_break_time")
        )
        
        # ALERT 2: Low Active Time
        df = df.withColumn(
            "alert_2",
            col("active_time") < col("expected_shift_time")
        )
        
        # ALERT 3: High Active Time
        df = df.withColumn(
            "alert_3",
            col("active_time") > col("expected_shift_time")
        )
        
        # ALERT 4: Low Active & High Idle Time
        df = df.withColumn(
            "alert_4",
            (col("active_time") < (0.9 * col("expected_shift_time"))) &
            (col("active_time_percent_ratio") < 90)
        )
        
        # ALERT 5: Low Active & Low Idle Time
        df = df.withColumn(
            "alert_5",
            (col("active_time") < (0.9 * col("expected_shift_time"))) &
            (col("active_time_percent_ratio") > 90)
        )
        
        # ALERT 6: High Active & High Idle Time
        df = df.withColumn(
            "alert_6",
            (col("active_time") > (1.1 * col("expected_shift_time"))) &
            (col("idle_time") > 1.0)
        )
        
        # ALERT 7: Few Breaks + Low Active & High Idle Time
        df = df.withColumn(
            "alert_7",
            ((col("idle_time") + col("lock_time")) < col("expected_break_time")) &
            (col("active_time") < (0.9 * col("expected_shift_time"))) &
            (col("active_time_percent_ratio") < 90)
        )
        
        # ALERT 8: Few Breaks + High Active & High Idle Time
        df = df.withColumn(
            "alert_8",
            ((col("idle_time") + col("lock_time")) < col("expected_break_time")) &
            (col("active_time") > (1.1 * col("expected_shift_time"))) &
            (col("idle_time") > 1.0)
        )
        
        # ALERT 9: Few Breaks + Low Active & Low Idle Time
        df = df.withColumn(
            "alert_9",
            ((col("idle_time") + col("lock_time")) < col("expected_break_time")) &
            (col("active_time") < (0.9 * col("expected_shift_time"))) &
            (col("active_time_percent_ratio") > 90)
        )
        
        # ALERT 10: Low Active + High Idle Time (Low Active combined)
        df = df.withColumn(
            "alert_10",
            (col("active_time") < col("expected_shift_time")) &
            (col("active_time") < (0.9 * col("expected_shift_time"))) &
            (col("active_time_percent_ratio") < 90)
        )
        
        # ALERT 11: Low Active + Low Idle Time (Low Active combined)
        df = df.withColumn(
            "alert_11",
            (col("active_time") < col("expected_shift_time")) &
            (col("active_time") < (0.9 * col("expected_shift_time"))) &
            (col("active_time_percent_ratio") > 90)
        )
        
        # ALERT 12: High Active + High Idle Time (High Active combined)
        df = df.withColumn(
            "alert_12",
            (col("active_time") > (1.1 * col("expected_shift_time"))) &
            (col("idle_time") > 1.0) &
            (col("active_time") > col("expected_shift_time"))
        )
        
        # ALERT 13: Low Active + Few Breaks
        df = df.withColumn(
            "alert_13",
            ((col("idle_time") + col("lock_time")) < col("expected_break_time")) &
            (col("active_time") < col("expected_shift_time"))
        )
        
        # ALERT 14: High Active + Few Breaks
        df = df.withColumn(
            "alert_14",
            ((col("idle_time") + col("lock_time")) < col("expected_break_time")) &
            (col("active_time") > col("expected_shift_time"))
        )
        
        return df
    
    def determine_final_alert(self, row):
        """
        UDF to determine final alert type with priority rules
        """
        # Check complex alerts first (7-14)
        for alert_id in range(7, 15):
            if getattr(row, f'alert_{alert_id}', False):
                return alert_id
        
        # Check basic alerts (1-6)
        for alert_id in range(1, 7):
            if getattr(row, f'alert_{alert_id}', False):
                return alert_id
        
        return None
    
    def consolidate_alerts(self, df):
        """
        Apply priority rules and determine final alert
        """
        logger.info("Consolidating alerts with priority rules...")
        
        # Register UDF
        from pyspark.sql.functions import pandas_udf, PandasUDFType
        import pandas as pd
        
        @pandas_udf(IntegerType(), PandasUDFType.SCALAR)
        def get_final_alert_pandas(
            a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14
        ):
            results = []
            for i in range(len(a1)):
                # Check complex alerts first (7-14)
                for alert_id in range(7, 15):
                    if locals()[f'a{alert_id}'][i]:
                        results.append(alert_id)
                        break
                else:
                    # Check basic alerts (1-6)
                    for alert_id in range(1, 7):
                        if locals()[f'a{alert_id}'][i]:
                            results.append(alert_id)
                            break
                    else:
                        results.append(None)
            return pd.Series(results)
        
        # Apply the pandas UDF
        df = df.withColumn(
            "final_alert_type",
            get_final_alert_pandas(
                *[col(f"alert_{i}") for i in range(1, 15)]
            )
        )
        
        # Alternative: Simple UDF if pandas not available
        # @udf(IntegerType())
        # def get_final_alert_simple(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14):
        #     alerts = [a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14]
        #     for i in range(6, 14):  # Check 7-14 first (index 6-13)
        #         if alerts[i]:
        #             return i + 1
        #     for i in range(6):  # Check 1-6 (index 0-5)
        #         if alerts[i]:
        #             return i + 1
        #     return None
        
        return df
    
    def generate_alert_json(self, row):
        """
        Generate complete alert JSON for a record
        """
        try:
            alert_type = int(row.final_alert_type) if row.final_alert_type else None
            
            # Build conditions checked dict
            conditions_checked = {}
            for i in range(1, 15):
                conditions_checked[f'alert_{i}'] = bool(getattr(row, f'alert_{i}', False))
            
            # Build condition details
            condition_details = {
                "active_time": float(row.active_time),
                "idle_time": float(row.idle_time),
                "lock_time": float(row.lock_time),
                "expected_shift_time": float(row.expected_shift_time),
                "expected_break_time": float(row.expected_break_time),
                "active_time_percent": float(row.active_time_percent_ratio) if hasattr(row, 'active_time_percent_ratio') else 0,
                "total_interaction": float(row.active_time + row.idle_time + row.lock_time)
            }
            
            # Add condition-specific details
            if alert_type == 1:
                condition_details["break_analysis"] = {
                    "actual_break": float(row.idle_time + row.lock_time),
                    "expected_break": float(row.expected_break_time),
                    "deficit": float(row.expected_break_time - (row.idle_time + row.lock_time))
                }
            
            # Build final JSON
            alert_json = {
                "alert_type": alert_type,
                "severity": "MEDIUM" if alert_type else None,
                "conditions_checked": conditions_checked,
                "condition_details": condition_details,
                "alert_message": self.alert_messages.get(alert_type) if alert_type else None,
                "calculated_at": datetime.now().isoformat(),
                "data_source": {
                    "emp_id": str(row.emp_id),
                    "shift_date": str(row.shift_date),
                    "manager_id": str(row.manager_id)
                }
            }
            
            return json.dumps(alert_json)
        except Exception as e:
            logger.error(f"Error generating alert JSON: {str(e)}")
            return json.dumps({"error": str(e)})
    
    def prepare_final_df(self, df):
        """
        Prepare final DataFrame for update
        """
        logger.info("Preparing final DataFrame...")
        
        # Register JSON generation UDF
        generate_json_udf = udf(self.generate_alert_json, StringType())
        
        final_df = df.withColumn(
            "alerts_json",
            generate_json_udf(struct([col(c) for c in df.columns]))
        ).withColumn(
            "alert_type",
            col("final_alert_type")
        ).withColumn(
            "alert_severity",
            when(col("final_alert_type").isNotNull(), "MEDIUM").otherwise(None)
        ).withColumn(
            "alert_updated_at",
            lit(datetime.now())
        ).select(
            "emp_id",
            "shift_date",
            "alert_type",
            "alert_severity",
            "alerts_json",
            "alert_updated_at"
        )
        
        return final_df
    
    def update_day_emp_table(self, final_df):
        """
        Update day_emp table with alerts
        """
        logger.info("Updating day_emp table...")
        
        # Create temp view
        final_df.createOrReplaceTempView("alerts_final")
        
        # First, count records to update
        count_query = """
            SELECT COUNT(*) as update_count
            FROM day_emp de
            INNER JOIN alerts_final af 
                ON de.emp_id = af.emp_id 
                AND de.shift_date = af.shift_date
        """
        
        update_count = self.spark.sql(count_query).first().update_count
        logger.info(f"Will update {update_count} existing records")
        
        # Update existing records
        update_query = """
            UPDATE day_emp de
            SET de.alert_type = af.alert_type,
                de.alert_severity = af.alert_severity,
                de.alerts_json = af.alerts_json,
                de.alert_updated_at = af.alert_updated_at
            FROM alerts_final af
            WHERE de.emp_id = af.emp_id
                AND de.shift_date = af.shift_date
        """
        
        self.spark.sql(update_query)
        
        # Insert new records (if any)
        insert_query = """
            INSERT INTO day_emp (emp_id, shift_date, alert_type, alert_severity, 
                               alerts_json, alert_updated_at)
            SELECT 
                af.emp_id,
                af.shift_date,
                af.alert_type,
                af.alert_severity,
                af.alerts_json,
                af.alert_updated_at
            FROM alerts_final af
            WHERE NOT EXISTS (
                SELECT 1 
                FROM day_emp de 
                WHERE de.emp_id = af.emp_id 
                    AND de.shift_date = af.shift_date
            )
        """
        
        insert_result = self.spark.sql(insert_query)
        
        # Get total affected count
        total_query = """
            SELECT COUNT(*) as total_updated
            FROM day_emp de
            INNER JOIN alerts_final af 
                ON de.emp_id = af.emp_id 
                AND de.shift_date = af.shift_date
            WHERE de.alert_updated_at = af.alert_updated_at
        """
        
        total_updated = self.spark.sql(total_query).first().total_updated
        
        logger.info(f"Successfully updated {total_updated} records in day_emp")
        return total_updated
    
    def run(self):
        """
        Main execution method
        """
        logger.info("Starting Day Employee Alerts Processing...")
        start_time = datetime.now()
        
        try:
            # Step 1: Get updated records
            trigger_df = self.get_updated_records()
            if trigger_df.count() == 0:
                logger.info("No updates found. Exiting.")
                return {"status": "skipped", "processed": 0}
            
            # Step 2: Get day_emp data
            day_data_df = self.get_day_emp_data(trigger_df)
            if day_data_df.count() == 0:
                logger.info("No day_emp data found. Exiting.")
                return {"status": "skipped", "processed": 0}
            
            # Step 3: Calculate alerts
            alerts_df = self.calculate_alerts(day_data_df)
            
            # Step 4: Consolidate alerts
            consolidated_df = self.consolidate_alerts(alerts_df)
            
            # Step 5: Prepare final DataFrame
            final_df = self.prepare_final_df(consolidated_df)
            
            # Step 6: Update table
            processed_count = self.update_day_emp_table(final_df)
            
            # Calculate duration
            duration = (datetime.now() - start_time).total_seconds()
            
            # Summary
            alert_distribution = final_df.groupBy("alert_type").count().collect()
            alert_summary = {str(row.alert_type): row["count"] for row in alert_distribution}
            
            logger.info(f"Processing completed in {duration:.2f} seconds")
            logger.info(f"Alert distribution: {alert_summary}")
            
            return {
                "status": "success",
                "processed": processed_count,
                "alert_distribution": alert_summary,
                "duration_seconds": duration,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error in Day Employee Alerts processing: {str(e)}", exc_info=True)
            return {
                "status": "failed",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

# Main execution
if __name__ == "__main__":
    # Initialize Spark
    spark = SparkSession.builder \
        .appName("DayEmpAlerts") \
        .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
        .config("spark.sql.shuffle.partitions", "50") \
        .enableHiveSupport() \
        .getOrCreate()
    
    # Run processor
    processor = DayEmpAlertsProcessor(spark)
    result = processor.run()
    
    # Print result
    print(json.dumps(result, indent=2))
    
    # Stop Spark
    spark.stop()
```

COMPLETE CODE: Day Manager Alerts

```python
# day_mgr_alerts_complete.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, lit, udf, to_json, struct, array, collect_list, countDistinct, sum as spark_sum
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, BooleanType, TimestampType, ArrayType
import json
from datetime import datetime, timedelta
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class DayMgrAlertsProcessor:
    def __init__(self, spark):
        self.spark = spark
        self.team_alert_messages = {
            1: "Quick tip! Your team could use more breaks. Encourage regular pauses to support well-being.",
            2: "Heads up! Team activity levels have dropped. Check for workload issues or blockers.",
            3: "Your team's been on overdrive. Check for workload surges or early burnout signs.",
            4: "Heads up! Your team shows low activity and high idle time. Review tasks or provide support to boost engagement.",
            5: "Heads up! Your team shows low activity and idle time. Investigate underutilisation or technical issues.",
            6: "Heads up! Your team shows high active and idle times. Assess process efficiency or tool responsiveness.",
            7: "Heads up! Your team shows low activity and high idle time with missed breaks. Review task clarity and tool efficiency.",
            8: "Heads up! Your team is overactive with idle spikes and limited breaks. Watch for burnout and inefficiencies.",
            9: "Heads up! Your team shows low interaction and limited breaks. Investigate tool or connectivity issues.",
            10: "Heads up! Your team shows low activity and high idle time. Review workloads and support needs.",
            11: "Heads up! Your team shows low activity and idle time. Check for underutilisation or technical concerns.",
            12: "Heads up! Your team shows high activity and idle time. Review process efficiency and tool usage.",
            13: "Heads up! Your team shows low activity with infrequent breaks. Address disruptions or unclear tasks.",
            14: "Heads up! Your team is on overdrive with minimal breaks. Watch for workload surges and burnout."
        }
    
    def get_updated_managers(self):
        """
        Get all managers that need recalculation based on updated day_emp records
        """
        logger.info("Fetching managers with updated day_emp data...")
        
        four_hours_ago = (datetime.now() - timedelta(hours=4)).strftime('%Y-%m-%d %H:%M:%S')
        
        query = f"""
            SELECT DISTINCT 
                hc.fun_mgr_id as manager_id,
                de.shift_date
            FROM day_emp de
            JOIN hrcentral hc ON de.emp_id = hc.emplid
            WHERE de.alert_updated_at > '{four_hours_ago}'
               OR de.alert_updated_at IS NULL  -- Include first-time processing
            GROUP BY hc.fun_mgr_id, de.shift_date
        """
        
        manager_dates_df = self.spark.sql(query)
        logger.info(f"Found {manager_dates_df.count()} manager-date combinations to process")
        
        return manager_dates_df
    
    def get_team_alert_data(self, manager_dates_df):
        """
        Get team alert data for all manager-date combinations
        """
        logger.info("Fetching team alert data...")
        
        # Create temp view
        manager_dates_df.createOrReplaceTempView("manager_dates")
        
        # Get team sizes
        team_sizes_query = """
            SELECT 
                fun_mgr_id as manager_id,
                COUNT(DISTINCT emplid) as team_size
            FROM hrcentral
            WHERE fun_mgr_id IN (SELECT DISTINCT manager_id FROM manager_dates)
            GROUP BY fun_mgr_id
        """
        
        team_sizes_df = self.spark.sql(team_sizes_query)
        
        # Get alert data for all team members
        team_alerts_query = """
            WITH team_members AS (
                SELECT 
                    hc.fun_mgr_id as manager_id,
                    hc.emplid as emp_id
                FROM hrcentral hc
                WHERE hc.fun_mgr_id IN (SELECT DISTINCT manager_id FROM manager_dates)
            )
            SELECT 
                tm.manager_id,
                de.shift_date,
                tm.emp_id,
                de.alert_type,
                de.alerts_json,
                de.alert_severity,
                de.active_time,
                de.idle_time,
                de.lock_time
            FROM team_members tm
            LEFT JOIN day_emp de ON tm.emp_id = de.emp_id
            INNER JOIN manager_dates md 
                ON tm.manager_id = md.manager_id
                AND de.shift_date = md.shift_date
        """
        
        team_alerts_df = self.spark.sql(team_alerts_query)
        
        # Join with team sizes
        team_data = team_alerts_df.join(
            team_sizes_df,
            team_alerts_df.manager_id == team_sizes_df.manager_id,
            "left"
        )
        
        logger.info(f"Found team data for {team_data.count()} employee-date-manager combinations")
        return team_data
    
    def calculate_team_metrics(self, team_data_df):
        """
        Calculate team-level metrics and severity
        """
        logger.info("Calculating team metrics...")
        
        # Group by manager, date, and alert type
        team_metrics = team_data_df.groupBy(
            "manager_id", "shift_date", "alert_type", "team_size"
        ).agg(
            countDistinct("emp_id").alias("affected_count"),
            collect_list("emp_id").alias("affected_employees"),
            spark_sum("active_time").alias("total_active_time"),
            spark_sum("idle_time").alias("total_idle_time"),
            spark_sum("lock_time").alias("total_lock_time")
        ).filter(
            col("alert_type").isNotNull()  # Only include records with alerts
        ).withColumn(
            "affected_percentage",
            when(col("team_size") > 0,
                 (col("affected_count") * 100.0 / col("team_size"))
            ).otherwise(0)
        )
        
        return team_metrics
    
    def assign_team_severity(self, team_metrics_df):
        """
        Assign severity based on team percentage thresholds
        """
        logger.info("Assigning team severity...")
        
        # Apply day timeframe team severity logic
        df = team_metrics_df.withColumn(
            "manager_severity_day",
            when(col("affected_percentage") >= 60, "HIGH")
            .when((col("affected_percentage") >= 40) & (col("affected_percentage") < 60), "MEDIUM")
            .when((col("affected_percentage") >= 20) & (col("affected_percentage") < 40), "LOW")
            .otherwise(None)
        ).filter(
            col("manager_severity_day").isNotNull()  # Only keep records meeting threshold
        )
        
        return df
    
    def generate_team_alert_json(self, row):
        """
        Generate team alert JSON for a manager-date-alert type combination
        """
        try:
            alert_type = int(row.alert_type) if row.alert_type else None
            
            # Parse affected employees list
            affected_employees = []
            if row.affected_employees:
                affected_employees = [str(emp_id) for emp_id in row.affected_employees]
            
            # Build team metrics
            team_metrics = {
                "team_size": int(row.team_size),
                "affected_count": int(row.affected_count),
                "affected_percentage": float(row.affected_percentage),
                "total_active_time": float(row.total_active_time) if row.total_active_time else 0,
                "total_idle_time": float(row.total_idle_time) if row.total_idle_time else 0,
                "total_lock_time": float(row.total_lock_time) if row.total_lock_time else 0
            }
            
            # Build alert JSON
            alert_json = {
                "alert_type": alert_type,
                "severity": str(row.manager_severity_day),
                "team_metrics": team_metrics,
                "affected_employees": affected_employees,
                "alert_message": self.team_alert_messages.get(alert_type),
                "calculated_at": datetime.now().isoformat(),
                "threshold_analysis": {
                    "threshold_applied": f"{row.affected_percentage:.2f}% affected",
                    "severity_thresholds": {
                        "LOW": "20-40%",
                        "MEDIUM": "40-60%", 
                        "HIGH": "â‰¥60%"
                    }
                }
            }
            
            return json.dumps(alert_json)
        except Exception as e:
            logger.error(f"Error generating team alert JSON: {str(e)}")
            return json.dumps({"error": str(e)})
    
    def prepare_manager_alerts(self, team_severity_df):
        """
        Prepare final manager alerts DataFrame
        """
        logger.info("Preparing manager alerts...")
        
        # Generate alert JSON for each alert type
        generate_json_udf = udf(self.generate_team_alert_json, StringType())
        
        alerts_with_json = team_severity_df.withColumn(
            "alert_json",
            generate_json_udf(struct([col(c) for c in team_severity_df.columns]))
        )
        
        # Group by manager and date to create array of alerts
        manager_alerts = alerts_with_json.groupBy(
            "manager_id", "shift_date", "team_size"
        ).agg(
            collect_list(
                struct(
                    col("alert_type"),
                    col("manager_severity_day").alias("severity"),
                    col("affected_count"),
                    col("affected_percentage"),
                    col("alert_json")
                )
            ).alias("alerts_array"),
            spark_sum("affected_count").alias("total_affected_employees")
        )
        
        # Generate final JSON for manager-date
        @udf(StringType())
        def generate_manager_json(alerts_array, manager_id, shift_date, team_size, total_affected):
            try:
                # Parse alerts array
                alerts_list = []
                for alert in alerts_array:
                    alerts_list.append({
                        "alert_type": int(alert.alert_type),
                        "severity": str(alert.severity),
                        "affected_count": int(alert.affected_count),
                        "affected_percentage": float(alert.affected_percentage),
                        "alert_details": json.loads(alert.alert_json)
                    })
                
                # Calculate overall severity (highest severity among alerts)
                severity_order = {"LOW": 1, "MEDIUM": 2, "HIGH": 3}
                overall_severity = None
                for alert in alerts_list:
                    if overall_severity is None or severity_order.get(alert["severity"], 0) > severity_order.get(overall_severity, 0):
                        overall_severity = alert["severity"]
                
                # Build manager JSON
                manager_json = {
                    "manager_id": str(manager_id),
                    "shift_date": str(shift_date),
                    "team_size": int(team_size),
                    "total_affected_employees": int(total_affected),
                    "overall_severity": overall_severity,
                    "alerts": alerts_list,
                    "alert_summary": {
                        "total_alerts": len(alerts_list),
                        "high_severity_count": sum(1 for a in alerts_list if a["severity"] == "HIGH"),
                        "medium_severity_count": sum(1 for a in alerts_list if a["severity"] == "MEDIUM"),
                        "low_severity_count": sum(1 for a in alerts_list if a["severity"] == "LOW")
                    },
                    "calculated_at": datetime.now().isoformat()
                }
                
                return json.dumps(manager_json)
            except Exception as e:
                logger.error(f"Error in generate_manager_json: {str(e)}")
                return json.dumps({"error": str(e)})
        
        final_df = manager_alerts.withColumn(
            "alerts_json",
            generate_manager_json(
                col("alerts_array"),
                col("manager_id"),
                col("shift_date"),
                col("team_size"),
                col("total_affected_employees")
            )
        ).withColumn(
            "alert_updated_at",
            lit(datetime.now())
        ).select(
            "manager_id",
            "shift_date",
            "team_size",
            "alerts_json",
            "alert_updated_at"
        )
        
        return final_df
    
    def update_day_mgr_table(self, final_df):
        """
        Update day_mgr table with team alerts
        """
        logger.info("Updating day_mgr table...")
        
        # Create temp view
        final_df.createOrReplaceTempView("manager_alerts_final")
        
        # Count existing records
        count_query = """
            SELECT COUNT(*) as update_count
            FROM day_mgr dm
            INNER JOIN manager_alerts_final maf 
                ON dm.manager_id = maf.manager_id 
                AND dm.shift_date = maf.shift_date
        """
        
        update_count = self.spark.sql(count_query).first().update_count
        logger.info(f"Will update {update_count} existing records")
        
        # Update existing records
        update_query = """
            UPDATE day_mgr dm
            SET dm.alerts_json = maf.alerts_json,
                dm.team_size = maf.team_size,
                dm.alert_updated_at = maf.alert_updated_at
            FROM manager_alerts_final maf
            WHERE dm.manager_id = maf.manager_id
                AND dm.shift_date = maf.shift_date
        """
        
        self.spark.sql(update_query)
        
        # Insert new records
        insert_query = """
            INSERT INTO day_mgr (manager_id, shift_date, team_size, alerts_json, alert_updated_at)
            SELECT 
                maf.manager_id,
                maf.shift_date,
                maf.team_size,
                maf.alerts_json,
                maf.alert_updated_at
            FROM manager_alerts_final maf
            WHERE NOT EXISTS (
                SELECT 1 
                FROM day_mgr dm 
                WHERE dm.manager_id = maf.manager_id 
                    AND dm.shift_date = maf.shift_date
            )
        """
        
        self.spark.sql(insert_query)
        
        # Get total updated count
        total_query = """
            SELECT COUNT(*) as total_updated
            FROM day_mgr dm
            INNER JOIN manager_alerts_final maf 
                ON dm.manager_id = maf.manager_id 
                AND dm.shift_date = maf.shift_date
            WHERE dm.alert_updated_at = maf.alert_updated_at
        """
        
        total_updated = self.spark.sql(total_query).first().total_updated
        logger.info(f"Successfully updated {total_updated} records in day_mgr")
        
        return total_updated
    
    def run(self):
        """
        Main execution method
        """
        logger.info("Starting Day Manager Alerts Processing...")
        start_time = datetime.now()
        
        try:
            # Step 1: Get updated managers
            manager_dates_df = self.get_updated_managers()
            if manager_dates_df.count() == 0:
                logger.info("No managers need updating. Exiting.")
                return {"status": "skipped", "processed": 0}
            
            # Step 2: Get team alert data
            team_data_df = self.get_team_alert_data(manager_dates_df)
            if team_data_df.count() == 0:
                logger.info("No team alert data found. Exiting.")
                return {"status": "skipped", "processed": 0}
            
            # Step 3: Calculate team metrics
            team_metrics_df = self.calculate_team_metrics(team_data_df)
            
            # Step 4: Assign team severity
            team_severity_df = self.assign_team_severity(team_metrics_df)
            
            # Step 5: Prepare manager alerts
            final_df = self.prepare_manager_alerts(team_severity_df)
            
            # Step 6: Update day_mgr table
            processed_count = self.update_day_mgr_table(final_df)
            
            # Calculate duration
            duration = (datetime.now() - start_time).total_seconds()
            
            # Summary
            manager_summary = final_df.groupBy("manager_id").count().collect()
            manager_count = len(manager_summary)
            
            logger.info(f"Processing completed in {duration:.2f} seconds")
            logger.info(f"Processed alerts for {manager_count} managers")
            
            return {
                "status": "success",
                "managers_processed": manager_count,
                "records_processed": processed_count,
                "duration_seconds": duration,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error in Day Manager Alerts processing: {str(e)}", exc_info=True)
            return {
                "status": "failed",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

# Main execution
if __name__ == "__main__":
    # Initialize Spark
    spark = SparkSession.builder \
        .appName("DayMgrAlerts") \
        .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
        .config("spark.sql.shuffle.partitions", "50") \
        .enableHiveSupport() \
        .getOrCreate()
    
    # Run processor
    processor = DayMgrAlertsProcessor(spark)
    result = processor.run()
    
    # Print result
    print(json.dumps(result, indent=2))
    
    # Stop Spark
    spark.stop()
```

ORCHESTRATION SCRIPT

```python
# run_daily_alerts.py
from pyspark.sql import SparkSession
import json
from datetime import datetime
import logging

# Import processors
from day_emp_alerts_complete import DayEmpAlertsProcessor
from day_mgr_alerts_complete import DayMgrAlertsProcessor

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def main():
    # Initialize Spark
    spark = SparkSession.builder \
        .appName("DailyAlertsPipeline") \
        .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
        .config("spark.sql.shuffle.partitions", "100") \
        .config("spark.dynamicAllocation.enabled", "true") \
        .config("spark.dynamicAllocation.maxExecutors", "10") \
        .enableHiveSupport() \
        .getOrCreate()
    
    overall_start = datetime.now()
    results = {}
    
    try:
        # Run Employee Alerts
        logger.info("=" * 60)
        logger.info("STARTING EMPLOYEE ALERTS PROCESSING")
        logger.info("=" * 60)
        
        emp_processor = DayEmpAlertsProcessor(spark)
        emp_result = emp_processor.run()
        results["employee_alerts"] = emp_result
        
        if emp_result.get("status") == "failed":
            logger.error("Employee alerts failed, skipping manager alerts")
            results["overall_status"] = "partial_failure"
        elif emp_result.get("processed", 0) > 0:
            # Run Manager Alerts only if employee alerts were processed
            logger.info("=" * 60)
            logger.info("STARTING MANAGER ALERTS PROCESSING")
            logger.info("=" * 60)
            
            mgr_processor = DayMgrAlertsProcessor(spark)
            mgr_result = mgr_processor.run()
            results["manager_alerts"] = mgr_result
            
            if mgr_result.get("status") == "success":
                results["overall_status"] = "success"
            else:
                results["overall_status"] = "partial_failure"
        else:
            logger.info("No employee alerts to process, skipping manager alerts")
            results["manager_alerts"] = {"status": "skipped", "reason": "no_employee_alerts"}
            results["overall_status"] = "success"
        
    except Exception as e:
        logger.error(f"Pipeline failed with error: {str(e)}", exc_info=True)
        results["overall_status"] = "failed"
        results["error"] = str(e)
    
    finally:
        # Calculate total duration
        total_duration = (datetime.now() - overall_start).total_seconds()
        results["total_duration_seconds"] = total_duration
        results["completed_at"] = datetime.now().isoformat()
        
        # Log summary
        logger.info("=" * 60)
        logger.info("PIPELINE SUMMARY")
        logger.info("=" * 60)
        logger.info(f"Overall Status: {results.get('overall_status')}")
        logger.info(f"Total Duration: {total_duration:.2f} seconds")
        
        if "employee_alerts" in results:
            emp = results["employee_alerts"]
            logger.info(f"Employee Alerts: {emp.get('processed', 0)} records, Status: {emp.get('status')}")
        
        if "manager_alerts" in results:
            mgr = results["manager_alerts"]
            logger.info(f"Manager Alerts: {mgr.get('records_processed', 0)} records, Status: {mgr.get('status')}")
        
        # Write results to file
        with open(f"/tmp/alert_pipeline_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json", "w") as f:
            json.dump(results, f, indent=2)
        
        # Print results
        print(json.dumps(results, indent=2))
        
        # Stop Spark
        spark.stop()
    
    return results

if __name__ == "__main__":
    main()
```

SCHEDULER CONFIGURATION

```bash
#!/bin/bash
# run_daily_alerts.sh

# Set environment
export SPARK_HOME=/opt/spark
export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9-src.zip:$PYTHONPATH

# Run the pipeline
$SPARK_HOME/bin/spark-submit \
    --master yarn \
    --deploy-mode cluster \
    --driver-memory 4G \
    --executor-memory 4G \
    --executor-cores 2 \
    --num-executors 4 \
    --conf spark.sql.adaptive.enabled=true \
    --conf spark.sql.adaptive.coalescePartitions.enabled=true \
    --conf spark.sql.adaptive.skewJoin.enabled=true \
    --py-files day_emp_alerts_complete.py,day_mgr_alerts_complete.py \
    run_daily_alerts.py
```

TABLE SCHEMA SETUP

```sql
-- Ensure day_emp has alert columns
ALTER TABLE day_emp 
ADD COLUMNS IF NOT EXISTS (
    alert_type INT COMMENT 'Final alert type (1-14)',
    alert_severity STRING COMMENT 'Severity level',
    alerts_json STRING COMMENT 'Complete alert details as JSON',
    alert_updated_at TIMESTAMP COMMENT 'When alert was last calculated'
);

-- Ensure day_mgr has alert columns  
ALTER TABLE day_mgr
ADD COLUMNS IF NOT EXISTS (
    team_size INT COMMENT 'Number of team members',
    alerts_json STRING COMMENT 'Team alert details as JSON',
    alert_updated_at TIMESTAMP COMMENT 'When alert was last calculated'
);

-- Create indexes for performance
CREATE INDEX IF NOT EXISTS idx_day_emp_alert_date 
ON day_emp (shift_date, alert_type);

CREATE INDEX IF NOT EXISTS idx_day_emp_alert_updated
ON day_emp (alert_updated_at);

CREATE INDEX IF NOT EXISTS idx_day_mgr_date
ON day_mgr (shift_date, manager_id);
```

This complete solution provides:

1. Employee alerts: Calculates all 14 conditions and stores in day_emp
2. Manager alerts: Aggregates team data and calculates severity
3. Batch processing: Efficient processing of all records at once
4. Error handling: Comprehensive logging and error recovery
5. Scalability: Spark-based distributed processing
6. Maintainability: Clean, modular code structure






_------___-----_________
def bulk_update_manager_ids():
    """
    Bulk update manager IDs using merge with joined DataFrame as source
    """
    print("Starting bulk update of manager IDs...")
    
    # Step 1: Create source DataFrame by joining login_logout with HR table
    source_df = spark.table("gold_dashboard.analytics_emp_login_logout") \
        .select(
            "EMP_ID",
            "SHIFT_DATE",
            "MANAGER_ID"  # This will be updated
        ) \
        .join(
            spark.table("inbound.hr_employee_central")
                .select(
                    F.col("emplid").alias("emp_id"),
                    F.col("func_mgr_id").alias("new_manager_id")
                ),
            F.col("EMP_ID") == F.col("emp_id"),
            "left"
        ) \
        .select(
            F.col("EMP_ID"),
            F.col("SHIFT_DATE"), 
            F.col("new_manager_id").alias("MANAGER_ID")  # Use the new manager_id from HR
        )
    
    # Step 2: Create temp view for source
    source_df.createOrReplaceTempView("temp_source")
    
    # Step 3: Perform merge using emp_id and shift_date as keys
    spark.sql("""
    MERGE INTO gold_dashboard.analytics_emp_login_logout AS target
    USING temp_source AS source
    ON target.EMP_ID = source.EMP_ID 
        AND target.SHIFT_DATE = source.SHIFT_DATE
    WHEN MATCHED THEN 
        UPDATE SET target.MANAGER_ID = source.MANAGER_ID
    """)
    
    print("Bulk update completed!")


Perfect! Here's the code for Option 1 (manager updates only during login/logout calculations) and a separate script for bulk updating existing data.

Option 1: Manager Updates Only During Login/Logout Calculations

No changes needed from the previous code I provided! The current implementation already follows Option 1. Here's the complete working code:

```python
from pyspark.sql import functions as F
from datetime import datetime, timedelta
from pyspark.sql.window import Window

# -------------------------
# Configurations
# -------------------------
dbutils.widgets.text("dynamic_hours", "24000")
try:
    dynamic_hours = int(dbutils.widgets.get("dynamic_hours"))
except Exception as e:
    print(f"Error parsing dynamic_hours: {e}, using default 1")
    dynamic_hours = 24000

DEFAULT_START = "09:00:00"
DEFAULT_END = "18:00:00"
ZERO_TIME = "00:00:00"
PROCESSING_WINDOW_MINUTES = 60 * dynamic_hours
print(f"Processing window: {PROCESSING_WINDOW_MINUTES} minutes")

# -------------------------
# Time Setup
# -------------------------
current_time = datetime.now()
ingestion_threshold = current_time - timedelta(minutes=PROCESSING_WINDOW_MINUTES)

# -------------------------
# Load HR Employee Central Data for Manager Information
# -------------------------
hr_central_df = spark.table("inbound.hr_employee_central") \
    .select(
        F.col("emplid").alias("emp_id"),
        F.col("func_mgr_id").alias("manager_id"),
        "func_mgr_name"
    ).distinct()

print("HR Central data loaded successfully")

# -------------------------
# Step 1: Recent activity and distinct emp_id, cal_date
# -------------------------
act_df = spark.table("app_trace.emp_activity") \
    .filter(F.col("ingestion_time") >= F.lit(ingestion_threshold))

# Device activities (for logouts)
mouse_df = spark.table("sys_trace.emp_mousedata") \
    .filter(F.col("ingestion_time") >= F.lit(ingestion_threshold)) \
    .select("emp_id", "cal_date", "event_time")

keyboard_df = spark.table("sys_trace.emp_keyboarddata") \
    .filter(F.col("ingestion_time") >= F.lit(ingestion_threshold)) \
    .select("emp_id", "cal_date", "event_time")

mousekey_df = mouse_df.union(keyboard_df)

activities_df = act_df.select("emp_id", "cal_date").distinct()

# -------------------------
# Add previous date
# -------------------------
emp_dates_df = activities_df.withColumn("prev_date", F.date_sub("cal_date", 1))

# -------------------------
# Step 2: Load shift data
# -------------------------
shift_df = spark.table("inbound.pulse_emp_shift_info") \
    .select("emp_id", "shift_date", "start_time", "end_time", "is_week_off")

# ... [REST OF YOUR ORIGINAL CODE UNTIL FINAL RESULT] ...

# ---------------------------------------------------
# Step 10. Determine final times with priority rules
# ---------------------------------------------------
final_result = result_df.withColumn(
    "emp_login_time",
    F.when(
        F.col("is_week_off"),
        F.coalesce(F.col("week_off_login"), F.col("last_activity"))
    ).otherwise(
        F.coalesce(F.col("current_login"), F.col("last_activity"))
    )
).withColumn(
    "emp_logout_time",
    F.when(
        F.col("is_week_off"),
        F.coalesce(F.col("week_off_logout"), F.col("last_activity"))
    ).otherwise(
        F.coalesce(F.col("last_activity"), F.col("current_login"))
    )
)

final_mkresult = result_mkdf.withColumn(
    "emp_logout_time_mk",
    F.when(
        F.col("is_week_off"),
        F.coalesce(F.col("week_off_mk_logout"), F.col("last_mk_activity"))
    ).otherwise(
        F.coalesce(F.col("last_mk_activity"), F.col("current_login"))
    )
)

# Join MK logout with main result
final_result_with_mk = final_result.join(
    final_mkresult.select("emp_id", "cal_date", "emp_logout_time_mk", "prev_logout_mk_update"),
    ["emp_id", "cal_date"],
    "left"
)

# ---------------------------------------------------
# Step 11. Generate previous day updates (both sources)
# ---------------------------------------------------
prev_day_updates = final_result_with_mk.filter(
    F.col("prev_logout_update").isNotNull() | F.col("prev_logout_mk_update").isNotNull()
).select(
    F.col("emp_id").alias("update_emp_id"),
    F.col("prev_cal_date").alias("update_date"),
    F.coalesce(F.col("prev_logout_mk_update"), F.col("prev_logout_update")).alias("new_logout_time")
)

# Final output: prefer MK logout if available and join manager information
final_output_with_manager = final_result_with_mk.join(
    hr_central_df,
    ["emp_id"],
    "left"  # LEFT JOIN ensures employees without HR records get NULL manager_id
).select(
    "emp_id",
    "manager_id",  # This will be NULL for employees not in HR table
    "cal_date",
    F.coalesce(F.col("emp_logout_time_mk"), F.col("emp_logout_time")).alias("emp_logout_time"),
    "emp_login_time",
    "shift_start_time",
    "shift_end_time",
    "is_week_off"
).orderBy("emp_id", "cal_date")

# Filter out null records
filtered_login_logout = final_output_with_manager.filter(
    (F.col("emp_id").isNotNull()) &
    (F.col("cal_date").isNotNull()) &
    (F.col("emp_login_time").isNotNull()) &
    (F.col("emp_logout_time").isNotNull()) &
    (F.col("emp_logout_time") >= F.col("emp_login_time"))
)

# Write to Delta table
spark.sql("""
CREATE TABLE IF NOT EXISTS gold_dashboard.analytics_emp_login_logout (
    EMP_ID int,
    MANAGER_ID int,  -- Add manager_id column
    EMP_CONTRACTED_HOURS string,
    START_TIME string,
    END_TIME string,
    START_TIME_THRESHOLD string,
    END_TIME_THRESHOLD string,
    EMP_LOGIN_TIME string,
    EMP_LOGOUT_TIME string,
    SHIFT_DATE date,
    SHIFT_COMPLETED string,
    ATTENDENCE_STATUS string,
    LOGIN_STATUS string,
    LOGOUT_STATUS string,
    WORKING_HOURS float,
    UPDATED_ON TIMESTAMP,
    IS_WEEK_OFF BOOLEAN GENERATED ALWAYS AS (
        CASE WHEN unix_timestamp(END_TIME) - unix_timestamp(START_TIME) <= 0 THEN TRUE ELSE FALSE END
    ),
    IS_HOLIDAY BOOLEAN
) USING DELTA
""")

spark.sql("""ALTER TABLE gold_dashboard.analytics_emp_login_logout ALTER COLUMN UPDATED_ON SET DEFAULT current_timestamp()""")
spark.sql("""ALTER TABLE gold_dashboard.analytics_emp_login_logout ALTER COLUMN IS_HOLIDAY SET DEFAULT FALSE""")

# Merge data into target table with manager_id
filtered_login_logout.createOrReplaceTempView("temp_filtered_login_logout")
spark.sql("""
MERGE INTO gold_dashboard.analytics_emp_login_logout AS target
USING temp_filtered_login_logout AS source
ON target.EMP_ID = source.emp_id AND target.SHIFT_DATE = source.cal_date
WHEN MATCHED AND target.EMP_LOGIN_TIME IS NOT NULL THEN
    UPDATE SET 
        target.EMP_LOGIN_TIME = CASE 
            WHEN source.emp_login_time < target.EMP_LOGIN_TIME THEN source.emp_login_time 
            ELSE target.EMP_LOGIN_TIME 
        END,
        target.EMP_LOGOUT_TIME = source.emp_logout_time,
        target.START_TIME = source.shift_start_time,
        target.END_TIME = source.shift_end_time,
        target.MANAGER_ID = source.manager_id  -- Update manager_id only during recalculation
WHEN NOT MATCHED THEN
    INSERT (EMP_ID, MANAGER_ID, START_TIME, END_TIME, EMP_LOGIN_TIME, EMP_LOGOUT_TIME, SHIFT_DATE)
    VALUES (source.emp_id, source.manager_id, source.shift_start_time, source.shift_end_time,
            source.emp_login_time, source.emp_logout_time, source.cal_date)
""")

# Update previous day's logout times with manager information
prev_day_updates_with_manager = prev_day_updates.join(
    hr_central_df,
    prev_day_updates.update_emp_id == hr_central_df.emp_id,
    "left"
).select(
    "update_emp_id",
    "update_date", 
    "new_logout_time",
    "manager_id"
)

prev_day_updates_with_manager.createOrReplaceTempView("temp_prev_day_updates")
spark.sql("""
MERGE INTO gold_dashboard.analytics_emp_login_logout AS target
USING temp_prev_day_updates AS source
ON target.EMP_ID = source.update_emp_id AND target.SHIFT_DATE = source.update_date
WHEN MATCHED THEN 
    UPDATE SET 
        target.EMP_LOGOUT_TIME = source.new_logout_time,
        target.MANAGER_ID = source.manager_id  -- Update manager_id for previous day too
""")
```

Separate Code for Bulk Updating Existing Manager IDs

Here's a separate script you can run whenever you need to update manager IDs for all existing historical data:

```python
# separate_manager_update.py
# Run this script independently when you need to bulk update manager IDs

from pyspark.sql import functions as F

def bulk_update_manager_ids():
    """
    Bulk update manager IDs for all existing records in analytics_emp_login_logout
    This updates historical records with current manager information from HR system
    """
    print("Starting bulk update of manager IDs...")
    
    # Load current HR data
    hr_central_df = spark.table("inbound.hr_employee_central") \
        .select(
            F.col("emplid").alias("emp_id"),
            F.col("func_mgr_id").alias("manager_id"),
            "func_mgr_name"
        ).distinct()
    
    print(f"Loaded HR data for {hr_central_df.count()} employees")
    
    # Create temp view for merge
    hr_central_df.createOrReplaceTempView("temp_hr_central")
    
    # Perform bulk update - update ALL records with current manager information
    update_result = spark.sql("""
    MERGE INTO gold_dashboard.analytics_emp_login_logout AS target
    USING temp_hr_central AS source
    ON target.EMP_ID = source.emp_id
    WHEN MATCHED THEN 
        UPDATE SET target.MANAGER_ID = source.manager_id
    """)
    
    # Get statistics
    total_records = spark.sql("SELECT COUNT(*) as total FROM gold_dashboard.analytics_emp_login_logout").collect()[0]['total']
    records_with_manager = spark.sql("SELECT COUNT(*) as with_manager FROM gold_dashboard.analytics_emp_login_logout WHERE MANAGER_ID IS NOT NULL").collect()[0]['with_manager']
    records_without_manager = spark.sql("SELECT COUNT(*) as without_manager FROM gold_dashboard.analytics_emp_login_logout WHERE MANAGER_ID IS NULL").collect()[0]['without_manager']
    
    print(f"Bulk update completed!")
    print(f"Total records: {total_records}")
    print(f"Records with manager ID: {records_with_manager}")
    print(f"Records without manager ID: {records_without_manager}")
    print(f"Coverage: {records_with_manager/total_records*100:.2f}%")
    
    return update_result

# Run the bulk update
if __name__ == "__main__":
    bulk_update_manager_ids()
```

How to Use:

1. Daily Process: Run the main code - it will update manager IDs only for records being inserted/updated
2. Bulk Updates: Run the separate script whenever you want to refresh ALL historical records with current manager information

Key Points:

Â· Main code: Manager IDs updated only during login/logout calculations (Option 1)
Â· Separate script: Use for one-time or periodic bulk updates of all historical data
Â· LEFT JOIN: Ensures employees without HR records get NULL manager_id
Â· No breaking changes: The main logic remains the same

This gives you the flexibility to have incremental updates during daily processing and bulk corrections when needed!















integrate leave update to existing login logout


to existing login logout --->

from pyspark.sql.window import Window
from pyspark.sql import functions as F

# 1. Extract distinct emp_id and cal_date from filtered login-logout data
emp_date_df = filtered_login_logout.select("emp_id", "cal_date").distinct()

# 2. Load leave data, filter to approved/cancelled, last 3 days updated (you may adjust time range)
leave_filtered = (
    spark.table("inbound.hr_lms")
    .filter(F.col("leave_status").isin("Approved", "Cancelled"))
    .select("employee_id", "leave_status", "leave_start_date", "leave_end_date", "create_time_stamp")
)

# 3. Explode leave date ranges into individual dates per employee with leave status
def explode_leave_dates(row):
    emp_id = row["employee_id"]
    status = row["leave_status"]
    start = row["leave_start_date"]
    end = row["leave_end_date"]
    dates = []
    current = start
    while current <= end:
        dates.append((emp_id, current, status))
        current += timedelta(days=1)
    return dates

exploded_leave_rdd = leave_filtered.rdd.flatMap(explode_leave_dates)
leave_dates_df = spark.createDataFrame(exploded_leave_rdd, ["emp_id", "cal_date", "leave_status"])

# 4. For duplicates (same emp_id & cal_date), keep the latest by create_time_stamp
window_spec = Window.partitionBy("emp_id", "cal_date").orderBy(F.col("create_time_stamp").desc())

# Join create_time_stamp info to leave_dates_df for windowing
leave_with_cts = leave_dates_df.alias("ld").join(
    leave_filtered.select(
        F.col("employee_id").alias("employee_id2"),
        "leave_start_date", "leave_end_date", "create_time_stamp"
    ),
    on=[(leave_dates_df.emp_id == F.col("employee_id2")) &
        (leave_dates_df.cal_date >= F.col("leave_start_date")) &
        (leave_dates_df.cal_date <= F.col("leave_end_date"))],
    how="left"
).select("emp_id", "cal_date", "leave_status", "create_time_stamp")

latest_leave_status_df = leave_with_cts.withColumn(
    "rn",
    F.row_number().over(window_spec)
).filter(F.col("rn") == 1).drop("rn")

# 5. Join latest_leave_status_df back to filtered_login_logout on emp_id and cal_date
filtered_login_logout_with_leave = filtered_login_logout.alias("flo").join(
    latest_leave_status_df.alias("lds"),
    (F.col("flo.emp_id") == F.col("lds.emp_id")) & (F.col("flo.cal_date") == F.col("lds.cal_date")),
    how="left"
).select(
    "flo.*",
    F.when(F.col("lds.leave_status") == "Approved", True)
     .when(F.col("lds.leave_status") == "Cancelled", False)
     .otherwise(F.col("is_holiday")).alias("is_holiday")
)

# 6. Use this dataframe for your merge:
filtered_login_logout_with_leave.createOrReplaceTempView("temp_filtered_login_logout")

# Then your existing MERGE continues, no change needed


Â£&#-_&Â£-_-&&-&&---&&_6&&&&_5_&&

daily job for leave update even for cancelled

from pyspark.sql import functions as F
from datetime import datetime, timedelta

current_time = datetime.now()
leave_threshold = current_time - timedelta(days=3)

# 1. Load leaves with status Approved or Cancelled, updated in last 3 days
leave_df = (
    spark.table("inbound.hr_lms")
    .filter(
        (F.col("updated_time_stamp") >= F.lit(leave_threshold)) &
        (F.col("leave_status").isin("Approved", "Cancelled"))
    )
    .select(
        "employee_id", "leave_status", "leave_start_date", "leave_end_date",
        "create_time_stamp"
    )
)

# 2. For duplicates on (employee_id, leave_start_date, leave_end_date), keep row with latest create_time_stamp
window_spec = Window.partitionBy(
    "employee_id", "leave_start_date", "leave_end_date"
).orderBy(F.col("create_time_stamp").desc())

distinct_leave_df = leave_df.withColumn(
    "rank", F.row_number().over(window_spec)
).filter(F.col("rank") == 1).drop("rank")

# 3. Explode leave intervals into (employee_id, date, leave_status)
def explode_leave_days(row):
    emp_id = row["employee_id"]
    leave_status = row["leave_status"]
    start = row["leave_start_date"]
    end = row["leave_end_date"]
    dates = []
    curr = start
    while curr <= end:
        dates.append((emp_id, curr.strftime("%Y-%m-%d"), leave_status))
        curr += timedelta(days=1)
    return dates

exploded = distinct_leave_df.rdd.flatMap(explode_leave_days)
leave_dates_df = spark.createDataFrame(exploded, ["emp_id", "shift_date", "leave_status"])

# 4. Create temp view for SQL merge
leave_dates_df.createOrReplaceTempView("leave_dates_to_update")

# 5. Update gold_dashboard.analytics_emp_login_logout based on leave_status
spark.sql("""
MERGE INTO gold_dashboard.analytics_emp_login_logout AS target
USING leave_dates_to_update AS source
ON target.EMP_ID = source.emp_id AND target.SHIFT_DATE = source.shift_date
WHEN MATCHED THEN
    UPDATE SET target.IS_HOLIDAY = CASE
        WHEN source.leave_status = 'Approved' THEN TRUE
        WHEN source.leave_status = 'Cancelled' THEN FALSE
        ELSE target.IS_HOLIDAY -- fallback, should not happen here
    END
""")

# 6. Print summary count
updated_count = leave_dates_df.count()
print(f"Processed leave updates (approved or cancelled) affecting {updated_count} employee-date records in last 3 days.")




Â£&Â£&___&&&&-+----

daily job to update leave details to emp_login_logout.

from pyspark.sql import functions as F
from datetime import datetime, timedelta

# ----- Config -----
current_time = datetime.now()
leave_threshold = current_time - timedelta(days=3)

# 1. Read leave records with status "Approved" updated in last 3 days
leave_df = (
    spark.table("inbound.hr_lms")
    .filter(
        (F.col("updated_time_stamp") >= F.lit(leave_threshold)) &
        (F.col("leave_status") == "Approved")
    )
    .select("employee_id", "leave_start_date", "leave_end_date")
    .dropDuplicates()
)

# 2. Explode leave date intervals into emp_id and every leave day
def explode_leave_days(row):
    emp_id = row["employee_id"]
    start = row["leave_start_date"]
    end = row["leave_end_date"]
    # Ensure types are datetime.date
    dates = []
    curr = start
    while curr <= end:
        dates.append((emp_id, curr.strftime("%Y-%m-%d")))
        curr += timedelta(days=1)
    return dates

exploded = leave_df.rdd.flatMap(explode_leave_days)
leave_dates_df = spark.createDataFrame(exploded, ["emp_id", "shift_date"])

# 3. Prepare view for update
leave_dates_df.createOrReplaceTempView("leave_dates_to_update")

# 4. Update gold_dashboard.analytics_emp_login_logout.is_holiday
spark.sql("""
MERGE INTO gold_dashboard.analytics_emp_login_logout AS target
USING leave_dates_to_update AS source
ON target.EMP_ID = source.emp_id AND target.SHIFT_DATE = source.shift_date
WHEN MATCHED THEN
    UPDATE SET target.IS_HOLIDAY = TRUE
""")

# 5. Print for audit
updated_count = leave_dates_df.count()
print(f"Updated holiday flag for {updated_count} emp_id/shift_date pairs.")


 &#-Â£&&Â£-Â£&Â£-Â£-Â£-Â£--Â£-Â£-Â£-Â£--Â£-Â£-Â£-Â£-Â£--Â£-Â£



daily job for any shift changes and emp-login-logout recalculation.


from datetime import datetime, timedelta
from pyspark.sql import functions as F
from pyspark.sql.window import Window
from pyspark.sql import Window

# -------------------------
# Configurations
# -------------------------

DEFAULT_START = "09:00:00"
DEFAULT_END = "18:00:00"
ZERO_TIME = "00:00:00"

current_time = datetime.now()
update_threshold = current_time - timedelta(days=3)

pulse_df = spark.table("inbound.pulse_emp_shift_info") \
    .filter(F.col("update_timestamp") >= F.lit(update_threshold)) \
    .select("emp_id", "shift_date", "start_time", "end_time")


def format_shift_times(df):
    # For week-off (start_time == end_time) -> "shift_date 00:00:00"
    # For normal shift (start < end): "shift_date start_time", "shift_date end_time"
    # For overnight: end_time on next date
    
    return df.withColumn(
        "start_time_full",
        F.concat_ws(" ", F.col("shift_date"), F.col("start_time"))
    ).withColumn(
        "end_time_full",
        F.when(F.col("start_time") == F.col("end_time"), 
               F.concat_ws(" ", F.col("shift_date"), F.lit("00:00:00")))
         .when(F.col("end_time") > F.col("start_time"),
               F.concat_ws(" ", F.col("shift_date"), F.col("end_time")))
         .otherwise(
             F.concat_ws(" ", F.date_add(F.col("shift_date"), 1), F.col("end_time"))
         )
    )

pulse_df_full = format_shift_times(pulse_df)

gold_df = spark.table("gold_dashboard.analytics_emp_login_logout") \
    .select("EMP_ID", "SHIFT_DATE", "START_TIME", "END_TIME")


from pyspark.sql import functions as F

# Step 1: Identify shifts that have changed start or end time
compare_df = pulse_df_full.join(
    gold_df,
    (pulse_df_full.emp_id == gold_df.EMP_ID) & (pulse_df_full.shift_date == gold_df.SHIFT_DATE),
    "inner"
).filter(
    (pulse_df_full.start_time_full != gold_df.START_TIME) |
    (pulse_df_full.end_time_full != gold_df.END_TIME)
).select(
    pulse_df_full.emp_id,
    pulse_df_full.shift_date
).distinct()

updated_shift_count = compare_df.count()
print(f"Number of shifts updated in last 3 days: {updated_shift_count}")


# Step xe: Filter activity tables based only on changed shifts (emp_id, shift_date)
act_df = spark.table("app_trace.emp_activity") \
    .join(
        compare_df.withColumnRenamed("shift_date", "cal_date"),
        (F.col("emp_id") == F.col("emp_id")) & (F.col("cal_date") == F.col("cal_date")),
        "inner"
    )

mouse_df = spark.table("sys_trace.emp_mousedata") \
    .join(
        compare_df.withColumnRenamed("shift_date", "cal_date"),
        (F.col("emp_id") == F.col("emp_id")) & (F.col("cal_date") == F.col("cal_date")),
        "inner"
    )

keyboard_df = spark.table("sys_trace.emp_keyboarddata") \
    .join(
        compare_df.withColumnRenamed("shift_date", "cal_date"),
        (F.col("emp_id") == F.col("emp_id")) & (F.col("cal_date") == F.col("cal_date")),
        "inner"
    )

mousekey_df = mouse_df.union(keyboard_df)

# Step x: Prepare distinct activity pairs from changed shifts only
activities_df = compare_df.select(
    F.col("emp_id"),
    F.col("shift_date").alias("cal_date")
).distinct()



# -------------------------
# Add previous date
# -------------------------
emp_dates_df = activities_df.withColumn("prev_date", F.date_sub("cal_date", 1))

# -------------------------
# Step 2: Load shift data
# -------------------------
shift_df = spark.table("inbound.pulse_emp_shift_info") \
    .select("emp_id", "shift_date", "start_time", "end_time", "is_week_off")

# -------------------------
# Step 3: Join current shift
# -------------------------
cur_shift = emp_dates_df.join(
    shift_df,
    (emp_dates_df.emp_id == shift_df.emp_id) & (emp_dates_df.cal_date == shift_df.shift_date),
    how='left'
).select(
    emp_dates_df.emp_id,
    emp_dates_df.cal_date,
    emp_dates_df.prev_date,
    shift_df.start_time.alias("cur_start_time_raw"),
    shift_df.end_time.alias("cur_end_time_raw"),
    shift_df.is_week_off
)

# Step 4: Join previous shift
prev_shift = shift_df \
    .withColumnRenamed("shift_date", "prev_cal_date") \
    .withColumnRenamed("start_time", "prev_start_time_raw") \
    .withColumnRenamed("end_time", "prev_end_time_raw") \
    .withColumnRenamed("emp_id", "emp_id_prev") \
    .withColumnRenamed("is_week_off", "prev_is_week_off")

cur_with_prev = cur_shift.join(
    prev_shift,
    (cur_shift.emp_id == prev_shift.emp_id_prev) & (cur_shift.prev_date == prev_shift.prev_cal_date),
    how="left"
).drop("emp_id_prev", "prev_cal_date")

# Step 5: Apply defaults based on missing data and day of week
final_df = cur_with_prev \
    .withColumn("dow", F.date_format("cal_date", "E")) \
    .withColumn("prev_dow", F.date_format("prev_date", "E")) \
    .withColumn("cur_start_time",
        F.when(F.col("cur_start_time_raw").isNotNull(), F.col("cur_start_time_raw"))
         .when(F.col("is_week_off") == True, F.lit(ZERO_TIME))
         .when(F.col("dow").isin("Sat", "Sun"), F.lit(ZERO_TIME))
         .otherwise(F.lit(DEFAULT_START))
    ) \
    .withColumn("cur_end_time",
        F.when(F.col("cur_end_time_raw").isNotNull(), F.col("cur_end_time_raw"))
         .when(F.col("is_week_off") == True, F.lit(ZERO_TIME))
         .when(F.col("dow").isin("Sat", "Sun"), F.lit(ZERO_TIME))
         .otherwise(F.lit(DEFAULT_END))
    ) \
    .withColumn("prev_start_time",
        F.when(F.col("prev_start_time_raw").isNotNull(), F.col("prev_start_time_raw"))
         .when(F.col("prev_is_week_off") == True, F.lit(ZERO_TIME))
         .when(F.col("prev_dow").isin("Sat", "Sun"), F.lit(ZERO_TIME))
         .otherwise(F.lit(DEFAULT_START))
    ) \
    .withColumn("prev_end_time",
        F.when(F.col("prev_end_time_raw").isNotNull(), F.col("prev_end_time_raw"))
         .when(F.col("prev_is_week_off") == True, F.lit(ZERO_TIME))
         .when(F.col("prev_dow").isin("Sat", "Sun"), F.lit(ZERO_TIME))
         .otherwise(F.lit(DEFAULT_END))
    ) \
    .withColumn("cur_end_time",
        F.when(F.col("cur_start_time") > F.col("cur_end_time"),
               F.date_format(F.expr("cast(cur_end_time as timestamp) + interval 1 day"), "HH:mm:ss")
        ).otherwise(F.col("cur_end_time"))
    ) \
    .withColumn("prev_end_time",
        F.when(F.col("prev_start_time") > F.col("prev_end_time"),
               F.date_format(F.expr("cast(prev_end_time as timestamp) + interval 1 day"), "HH:mm:ss")
        ).otherwise(F.col("prev_end_time"))
    ) \
    .withColumn("cur_start_time_ts", F.concat_ws(" ", F.col("cal_date"), F.col("cur_start_time"))) \
    .withColumn("cur_end_time_ts", F.concat_ws(" ", F.col("cal_date"), F.col("cur_end_time"))) \
    .withColumn("prev_start_time_ts", F.concat_ws(" ", F.col("prev_date"), F.col("prev_start_time"))) \
    .withColumn("prev_end_time_ts", F.concat_ws(" ", F.col("prev_date"), F.col("prev_end_time"))) \
    .withColumn("cur_end_time_ts",
        F.when(F.col("cur_start_time") > F.col("cur_end_time"),
               F.date_format(F.expr("cast(cur_end_time_ts as timestamp) + interval 1 day"), "yyyy-MM-dd HH:mm:ss")
        ).otherwise(F.col("cur_end_time_ts"))
    ) \
    .withColumn("prev_end_time_ts",
        F.when(F.col("prev_start_time") > F.col("prev_end_time"),
               F.date_format(F.expr("cast(prev_end_time_ts as timestamp) + interval 1 day"), "yyyy-MM-dd HH:mm:ss")
        ).otherwise(F.col("prev_end_time_ts"))
    ) \
    .withColumn("is_week_off",
        F.when(
            (F.col("cur_start_time") == ZERO_TIME) & (F.col("cur_end_time") == ZERO_TIME),
            F.lit(True)
        ).otherwise(F.lit(False))
    ) \
    .withColumn("prev_is_week_off",
        F.when(
            (F.col("prev_start_time") == ZERO_TIME) & (F.col("prev_end_time") == ZERO_TIME),
            F.lit(True)
        ).otherwise(F.lit(False))
    ) \
    .select(
        "emp_id",
        "cal_date",
        "prev_date",
        "cur_start_time_ts",
        "cur_end_time_ts",
        "prev_start_time_ts",
        "prev_end_time_ts",
        "is_week_off",
        "prev_is_week_off"
    )

# 6. Join with activities
joined_df = act_df.join(
    final_df,
    ["emp_id", "cal_date"],
    "left"
)
joined_mkdf = mousekey_df.join(
    final_df,
    ["emp_id", "cal_date"],
    "left"
)

# 7. Define time windows with special handling for week-offs

jdf_with_windows = joined_df.withColumn(
    "current_window_start",
    F.when(
    (F.col("is_week_off") == True) & (F.col("prev_is_week_off") == False),
    greatest(
        F.expr("cast(prev_end_time_ts as timestamp) + interval 8 hours"),  # previous day's extended shift end
        F.col("cur_start_time_ts")  # current day shift start (likely 00:00:00 on week off)
    )
).when(
    F.col("is_week_off") == True,
    F.col("cur_start_time_ts")  # consecutive week offs
).otherwise(
    F.expr("cast(cur_start_time_ts as timestamp) - interval 4 hours")  # normal working day
)

).withColumn(
    "current_window_end",
    F.when(~F.col("is_week_off"), F.expr("cast(cur_end_time_ts as timestamp) + interval 8 hours"))
	.when(
	(F.col("is_week_off") == True) & (F.col("prev_is_week_off") == False),
	F.concat_ws(" ", F.col("cal_date"),  lit("23:59:59"))
	) 
    .otherwise(F.col("cur_end_time_ts"))
).withColumn(
    "prev_window_start",
    F.when(
        (F.col("prev_date") == F.date_sub(F.col("cal_date"), 1)) &
        ~F.col("prev_is_week_off"),
        F.col("prev_end_time_ts")
    ).otherwise(F.col("prev_start_time_ts"))
).withColumn(
    "prev_window_end",
    F.when(
        (F.col("prev_date") == F.date_sub(F.col("cal_date"), 1)) &
        ~F.col("prev_is_week_off"),
        F.expr("cast(prev_end_time_ts as timestamp) + interval 8 hours")
    ).otherwise(F.col("prev_end_time_ts"))
)





jmkdf_with_windows = joined_mkdf.withColumn(
    "current_window_start",
    F.when(
    (F.col("is_week_off") == True) & (F.col("prev_is_week_off") == False),
    greatest(
        F.expr("cast(prev_end_time_ts as timestamp) + interval 8 hours"), 
        F.col("cur_start_time_ts")  # current day shift start (likely 00:00:00 on week off)
			)
	).when(
		F.col("is_week_off") == True,
		F.col("cur_start_time_ts")  # consecutive week offs
	).otherwise(
		F.expr("cast(cur_start_time_ts as timestamp) - interval 4 hours") )
).withColumn(
    "current_window_end",
    F.when(~F.col("is_week_off"), F.expr("cast(cur_end_time_ts as timestamp) + interval 8 hours"))
	.when(
	(F.col("is_week_off") == True) & (F.col("prev_is_week_off") == False),
	F.concat_ws(" ", F.col("cal_date"),  lit("23:59:59"))
	) 
    .otherwise(F.col("cur_end_time_ts"))
).withColumn(
    "prev_window_start",
    F.when(
        (F.col("prev_date") == F.date_sub(F.col("cal_date"), 1)) &
        ~F.col("prev_is_week_off"),
        F.col("prev_end_time_ts")
		).otherwise(F.col("prev_start_time_ts"))
).withColumn(
    "prev_window_end",
    F.when(
        (F.col("prev_date") == F.date_sub(F.col("cal_date"), 1)) &
        ~F.col("prev_is_week_off"),
        F.expr("cast(prev_end_time_ts as timestamp) + interval 8 hours")
    ).otherwise(F.col("prev_end_time_ts"))
)

# 8. Classify activities with priority to current day
classified_df = jdf_with_windows.withColumn(
    "is_current_login",
    ~F.col("is_week_off") &
    (F.col("start_time") >= F.col("current_window_start")) &
    (F.col("start_time") <= F.col("current_window_end")) &
    (F.col("app_name") != "WindowLock")
).withColumn(
    "is_prev_logout",
    F.col("prev_window_start").isNotNull() &
    (F.col("start_time") >= F.col("prev_window_start")) &
    (F.col("start_time") <= F.col("prev_window_end")) &
    (F.col("start_time") < F.col("current_window_start"))
).withColumn(
    "is_week_off_activity",
    F.col("is_week_off") &
    (F.col("prev_window_start").isNull() | 
     (F.col("start_time") > F.col("prev_window_end")))
)

classified_mkdf = jmkdf_with_windows.withColumn(
    "is_current_login",
    ~F.col("is_week_off") &
    (F.col("event_time") >= F.col("current_window_start")) &
    (F.col("event_time") <= F.col("current_window_end"))
).withColumn(
    "is_prev_logout",
    F.col("prev_window_start").isNotNull() &
    (F.col("event_time") >= F.col("prev_window_start")) &
    (F.col("event_time") <= F.col("prev_window_end")) &
    (F.col("event_time") < F.col("current_window_start"))
).withColumn(
    "is_week_off_activity",
    F.col("is_week_off") &
    (F.col("prev_window_start").isNull() | 
     (F.col("event_time") > F.col("prev_window_end")))
)

# ---------------------------------------------------
# Step 9. Calculate all potential times
# ---------------------------------------------------
result_df = classified_df.groupBy("emp_id", "cal_date").agg(
    # Current login: Minimum start_time within current window and app_name != "Window Lock"
    F.min(F.when(
        (F.col("is_current_login")) & (F.col("app_name") != "WindowLock"),
        F.col("start_time")
    )).alias("current_login"),
    # Last activity: Maximum start_time within current window
    F.max(F.when(
        (F.col("start_time") >= F.col("current_window_start")) &
        (F.col("start_time") <= F.col("current_window_end")),
        F.col("start_time")
    )).alias("last_activity"),
    # Previous logout candidate (normal): Max start_time in previous window
    F.max(F.when(
        (F.col("prev_window_start").isNotNull()) &
        (F.col("start_time") >= F.col("prev_window_start")) &
        (F.col("start_time") < F.col("prev_window_end")) &
        (F.col("start_time") < F.col("current_window_start")) &
        (~F.col("is_week_off_activity")),
        F.col("start_time")
    )).alias("prev_logout_update_norm"),
    # Previous logout candidate (week-off): Max start_time in prev window if week_off_activity
    F.max(F.when(
        (F.col("prev_window_start").isNotNull()) &
        (F.col("start_time") > F.col("prev_window_start")) &
        (F.col("start_time") < F.col("prev_window_end")) &
        (F.col("is_week_off_activity")),
        F.col("start_time")
    )).alias("prev_logout_update_weekoff"),
    # Week-off login: Min start_time if week_off_activity
    F.min(F.when(
        (F.col("is_week_off_activity")) &
        (F.col("app_name") != "WindowLock") &
        (F.col("start_time") > F.col("prev_window_end")) &
        (F.col("start_time") > F.col("current_window_start")),
        F.col("start_time")
    )).alias("week_off_login"),
    # Week-off logout: Max start_time if week_off_activity
    F.max(F.when(
        (F.col("is_week_off_activity")) &
        (F.col("start_time") > F.col("prev_window_end")),
        F.col("start_time")
    )).alias("week_off_logout"),
    # Shift info
    F.first("cur_start_time_ts").alias("shift_start_time"),
    F.first("cur_end_time_ts").alias("shift_end_time"),
    F.first("is_week_off").alias("is_week_off"),
    F.first("prev_date").alias("prev_cal_date")
).withColumn(
    "prev_logout_update",
    F.coalesce(F.col("prev_logout_update_norm"), F.col("prev_logout_update_weekoff"))
)

# Mouse/keyboard side
result_mkdf = classified_mkdf.groupBy("emp_id", "cal_date").agg(
    F.min(F.when(F.col("is_current_login") &
                 (F.col("event_time") >= F.col("current_window_start")) &
                 (F.col("event_time") <= F.col("current_window_end")),
                 F.col("event_time"))).alias("current_login"),
    F.max(F.when(
        (F.col("event_time") >= F.col("current_window_start")) &
        (F.col("event_time") <= F.col("current_window_end")),
        F.col("event_time")
    )).alias("last_mk_activity"),
    F.max(F.when(
        (F.col("prev_window_start").isNotNull()) &
        (F.col("event_time") >= F.col("prev_window_start")) &
        (F.col("event_time") < F.col("prev_window_end")) &
        (~F.col("is_week_off_activity")),
        F.col("event_time")
    )).alias("prev_logout_update_norm"),
    # Previous logout candidate (week-off): Max event_time in prev window if week_off_activity
    F.max(F.when(
        (F.col("prev_window_start").isNotNull()) &
        (F.col("event_time") > F.col("prev_window_start")) &
        (F.col("event_time") < F.col("prev_window_end")) &
        (F.col("is_week_off_activity")),
        F.col("event_time")
    )).alias("prev_logout_update_weekoff"),
    F.min(F.when(
        (F.col("is_week_off_activity")) &
        (F.col("event_time") > F.col("prev_window_end")) &
        (F.col("event_time") > F.col("current_window_start")),
        F.col("event_time")
    )).alias("week_off_login"),
    F.max(F.when(
        (F.col("is_week_off_activity")) &
        (F.col("event_time") > F.col("prev_window_end")),
        F.col("event_time")
    )).alias("week_off_mk_logout"),
    # Shift info
    F.first("cur_start_time_ts").alias("shift_start_time"),
    F.first("cur_end_time_ts").alias("shift_end_time"),
    F.first("is_week_off").alias("is_week_off"),
    F.first("prev_date").alias("prev_cal_date")
)

result_mkdf = result_mkdf.withColumn(
    "prev_logout_mk_update",
    F.coalesce(F.col("prev_logout_update_norm"), F.col("prev_logout_update_weekoff"))
)

# ---------------------------------------------------
# Step 10. Determine final times with priority rules
# ---------------------------------------------------
final_result = result_df.withColumn(
    "emp_login_time",
    F.when(
        F.col("is_week_off"),
        F.coalesce(F.col("week_off_login"), F.col("last_activity"))
    ).otherwise(
        F.coalesce(F.col("current_login"), F.col("last_activity"))
    )
).withColumn(
    "emp_logout_time",
    F.when(
        F.col("is_week_off"),
        F.coalesce(F.col("week_off_logout"), F.col("last_activity"))
    ).otherwise(
        F.coalesce(F.col("last_activity"), F.col("current_login"))
    )
)

final_mkresult = result_mkdf.withColumn(
    "emp_logout_time_mk",
    F.when(
        F.col("is_week_off"),
        F.coalesce(F.col("week_off_mk_logout"), F.col("last_mk_activity"))
    ).otherwise(
        F.coalesce(F.col("last_mk_activity"), F.col("current_login"))
    )
)

# Join MK logout with main result
final_result_with_mk = final_result.join(
    final_mkresult.select("emp_id", "cal_date", "emp_logout_time_mk", "prev_logout_mk_update"),
    ["emp_id", "cal_date"],
    "left"
)

# ---------------------------------------------------
# Step 11. Generate previous day updates (both sources)
# ---------------------------------------------------
prev_day_updates = final_result_with_mk.filter(
    F.col("prev_logout_update").isNotNull() | F.col("prev_logout_mk_update").isNotNull()
).select(
    F.col("emp_id").alias("update_emp_id"),
    F.col("prev_cal_date").alias("update_date"),
    F.coalesce(F.col("prev_logout_mk_update"), F.col("prev_logout_update")).alias("new_logout_time")
)

# Final output: prefer MK logout if available
final_output = final_result_with_mk.select(
    "emp_id",
    "cal_date",
    F.coalesce(F.col("emp_logout_time_mk"), F.col("emp_logout_time")).alias("emp_logout_time"),
    "emp_login_time",
    "shift_start_time",
    "shift_end_time",
    "is_week_off"
).orderBy("emp_id", "cal_date")

# Filter out null records
filtered_login_logout = final_output.filter(
    (F.col("emp_id").isNotNull()) &
    (F.col("cal_date").isNotNull()) &
    (F.col("emp_login_time").isNotNull()) &
    (F.col("emp_logout_time").isNotNull()) &
	(F.col("emp_logout_time") >= F.col("emp_login_time"))
)

# Write to Delta table
spark.sql("""
CREATE TABLE IF NOT EXISTS gold_dashboard.analytics_emp_login_logout (
    EMP_ID int,
    EMP_CONTRACTED_HOURS string,
    START_TIME string,
    END_TIME string,
    START_TIME_THRESHOLD string,
    END_TIME_THRESHOLD string,
    EMP_LOGIN_TIME string,
    EMP_LOGOUT_TIME string,
    SHIFT_DATE date,
    SHIFT_COMPLETED string,
    ATTENDENCE_STATUS string,
    LOGIN_STATUS string,
    LOGOUT_STATUS string,
    WORKING_HOURS float,
    UPDATED_ON TIMESTAMP,
    IS_WEEK_OFF BOOLEAN GENERATED ALWAYS AS (
        CASE WHEN unix_timestamp(END_TIME) - unix_timestamp(START_TIME) <= 0 THEN TRUE ELSE FALSE END
    ),
    IS_HOLIDAY BOOLEAN
) USING DELTA
""")

spark.sql("""ALTER TABLE gold_dashboard.analytics_emp_login_logout ALTER COLUMN UPDATED_ON SET DEFAULT current_timestamp()""")
spark.sql("""ALTER TABLE gold_dashboard.analytics_emp_login_logout ALTER COLUMN IS_HOLIDAY SET DEFAULT FALSE""")

# Merge data into target table
filtered_login_logout.createOrReplaceTempView("temp_filtered_login_logout")
spark.sql("""
MERGE INTO gold_dashboard.analytics_emp_login_logout AS target
USING temp_filtered_login_logout AS source
ON target.EMP_ID = source.emp_id AND target.SHIFT_DATE = source.cal_date
WHEN MATCHED AND target.EMP_LOGIN_TIME IS NOT NULL THEN
    UPDATE SET 
        target.EMP_LOGIN_TIME = CASE 
            WHEN source.emp_login_time < target.EMP_LOGIN_TIME THEN source.emp_login_time 
            ELSE target.EMP_LOGIN_TIME 
        END,
        target.EMP_LOGOUT_TIME = source.emp_logout_time,
        target.START_TIME = source.shift_start_time,
        target.END_TIME = source.shift_end_time
WHEN NOT MATCHED THEN
    INSERT (EMP_ID, START_TIME, END_TIME, EMP_LOGIN_TIME, EMP_LOGOUT_TIME, SHIFT_DATE)
    VALUES (source.emp_id, source.shift_start_time, source.shift_end_time,
            source.emp_login_time, source.emp_logout_time, source.cal_date)
""")

# Update previous day's logout times if needed
prev_day_updates.createOrReplaceTempView("temp_prev_day_updates")
spark.sql("""
MERGE INTO gold_dashboard.analytics_emp_login_logout AS target
USING temp_prev_day_updates AS source
ON target.EMP_ID = source.update_emp_id AND target.SHIFT_DATE = source.update_date
WHEN MATCHED THEN 
    UPDATE SET target.EMP_LOGOUT_TIME = source.new_logout_time
""")





##&#&#&&#&#&#&#&#&&Â£&#&#&#&#&&Â£&#&&Â£&Â£&Â£&Â£&&Â£&Â£&Â£&Â£&&Â£&Â£&Â£&Â£&&Â£&Â£&Â£


login logout modify. for proper login and weekoff changes.



from pyspark.sql import functions as F
from datetime import datetime, timedelta
from pyspark.sql.window import Window

# -------------------------
# Configurations
# -------------------------
dbutils.widgets.text("dynamic_hours", "24000")
try:
    dynamic_hours = int(dbutils.widgets.get("dynamic_hours"))
except Exception as e:
    print(f"Error parsing dynamic_hours: {e}, using default 1")
    dynamic_hours = 24000

DEFAULT_START = "09:00:00"
DEFAULT_END = "18:00:00"
ZERO_TIME = "00:00:00"
PROCESSING_WINDOW_MINUTES = 60 * dynamic_hours
print(f"Processing window: {PROCESSING_WINDOW_MINUTES} minutes")

# -------------------------
# Time Setup
# -------------------------
current_time = datetime.now()
ingestion_threshold = current_time - timedelta(minutes=PROCESSING_WINDOW_MINUTES)

# -------------------------
# Step 1: Recent activity and distinct emp_id, cal_date
# -------------------------
act_df = spark.table("app_trace.emp_activity") \
    .filter(F.col("ingestion_time") >= F.lit(ingestion_threshold))

# Device activities (for logouts)
mouse_df = spark.table("sys_trace.emp_mousedata") \
    .filter(F.col("ingestion_time") >= F.lit(ingestion_threshold)) \
    .select("emp_id", "cal_date", "event_time")

keyboard_df = spark.table("sys_trace.emp_keyboarddata") \
    .filter(F.col("ingestion_time") >= F.lit(ingestion_threshold)) \
    .select("emp_id", "cal_date", "event_time")

mousekey_df = mouse_df.union(keyboard_df)

activities_df = act_df.select("emp_id", "cal_date").distinct()

# -------------------------
# Add previous date
# -------------------------
emp_dates_df = activities_df.withColumn("prev_date", F.date_sub("cal_date", 1))

# -------------------------
# Step 2: Load shift data
# -------------------------
shift_df = spark.table("inbound.pulse_emp_shift_info") \
    .select("emp_id", "shift_date", "start_time", "end_time", "is_week_off")

# -------------------------
# Step 3: Join current shift
# -------------------------
cur_shift = emp_dates_df.join(
    shift_df,
    (emp_dates_df.emp_id == shift_df.emp_id) & (emp_dates_df.cal_date == shift_df.shift_date),
    how='left'
).select(
    emp_dates_df.emp_id,
    emp_dates_df.cal_date,
    emp_dates_df.prev_date,
    shift_df.start_time.alias("cur_start_time_raw"),
    shift_df.end_time.alias("cur_end_time_raw"),
    shift_df.is_week_off
)

# Step 4: Join previous shift
prev_shift = shift_df \
    .withColumnRenamed("shift_date", "prev_cal_date") \
    .withColumnRenamed("start_time", "prev_start_time_raw") \
    .withColumnRenamed("end_time", "prev_end_time_raw") \
    .withColumnRenamed("emp_id", "emp_id_prev") \
    .withColumnRenamed("is_week_off", "prev_is_week_off")

cur_with_prev = cur_shift.join(
    prev_shift,
    (cur_shift.emp_id == prev_shift.emp_id_prev) & (cur_shift.prev_date == prev_shift.prev_cal_date),
    how="left"
).drop("emp_id_prev", "prev_cal_date")

# Step 5: Apply defaults based on missing data and day of week
final_df = cur_with_prev \
    .withColumn("dow", F.date_format("cal_date", "E")) \
    .withColumn("prev_dow", F.date_format("prev_date", "E")) \
    .withColumn("cur_start_time",
        F.when(F.col("cur_start_time_raw").isNotNull(), F.col("cur_start_time_raw"))
         .when(F.col("is_week_off") == True, F.lit(ZERO_TIME))
         .when(F.col("dow").isin("Sat", "Sun"), F.lit(ZERO_TIME))
         .otherwise(F.lit(DEFAULT_START))
    ) \
    .withColumn("cur_end_time",
        F.when(F.col("cur_end_time_raw").isNotNull(), F.col("cur_end_time_raw"))
         .when(F.col("is_week_off") == True, F.lit(ZERO_TIME))
         .when(F.col("dow").isin("Sat", "Sun"), F.lit(ZERO_TIME))
         .otherwise(F.lit(DEFAULT_END))
    ) \
    .withColumn("prev_start_time",
        F.when(F.col("prev_start_time_raw").isNotNull(), F.col("prev_start_time_raw"))
         .when(F.col("prev_is_week_off") == True, F.lit(ZERO_TIME))
         .when(F.col("prev_dow").isin("Sat", "Sun"), F.lit(ZERO_TIME))
         .otherwise(F.lit(DEFAULT_START))
    ) \
    .withColumn("prev_end_time",
        F.when(F.col("prev_end_time_raw").isNotNull(), F.col("prev_end_time_raw"))
         .when(F.col("prev_is_week_off") == True, F.lit(ZERO_TIME))
         .when(F.col("prev_dow").isin("Sat", "Sun"), F.lit(ZERO_TIME))
         .otherwise(F.lit(DEFAULT_END))
    ) \
    .withColumn("cur_end_time",
        F.when(F.col("cur_start_time") > F.col("cur_end_time"),
               F.date_format(F.expr("cast(cur_end_time as timestamp) + interval 1 day"), "HH:mm:ss")
        ).otherwise(F.col("cur_end_time"))
    ) \
    .withColumn("prev_end_time",
        F.when(F.col("prev_start_time") > F.col("prev_end_time"),
               F.date_format(F.expr("cast(prev_end_time as timestamp) + interval 1 day"), "HH:mm:ss")
        ).otherwise(F.col("prev_end_time"))
    ) \
    .withColumn("cur_start_time_ts", F.concat_ws(" ", F.col("cal_date"), F.col("cur_start_time"))) \
    .withColumn("cur_end_time_ts", F.concat_ws(" ", F.col("cal_date"), F.col("cur_end_time"))) \
    .withColumn("prev_start_time_ts", F.concat_ws(" ", F.col("prev_date"), F.col("prev_start_time"))) \
    .withColumn("prev_end_time_ts", F.concat_ws(" ", F.col("prev_date"), F.col("prev_end_time"))) \
    .withColumn("cur_end_time_ts",
        F.when(F.col("cur_start_time") > F.col("cur_end_time"),
               F.date_format(F.expr("cast(cur_end_time_ts as timestamp) + interval 1 day"), "yyyy-MM-dd HH:mm:ss")
        ).otherwise(F.col("cur_end_time_ts"))
    ) \
    .withColumn("prev_end_time_ts",
        F.when(F.col("prev_start_time") > F.col("prev_end_time"),
               F.date_format(F.expr("cast(prev_end_time_ts as timestamp) + interval 1 day"), "yyyy-MM-dd HH:mm:ss")
        ).otherwise(F.col("prev_end_time_ts"))
    ) \
    .withColumn("is_week_off",
        F.when(
            (F.col("cur_start_time") == ZERO_TIME) & (F.col("cur_end_time") == ZERO_TIME),
            F.lit(True)
        ).otherwise(F.lit(False))
    ) \
    .withColumn("prev_is_week_off",
        F.when(
            (F.col("prev_start_time") == ZERO_TIME) & (F.col("prev_end_time") == ZERO_TIME),
            F.lit(True)
        ).otherwise(F.lit(False))
    ) \
    .select(
        "emp_id",
        "cal_date",
        "prev_date",
        "cur_start_time_ts",
        "cur_end_time_ts",
        "prev_start_time_ts",
        "prev_end_time_ts",
        "is_week_off",
        "prev_is_week_off"
    )

# 6. Join with activities
joined_df = act_df.join(
    final_df,
    ["emp_id", "cal_date"],
    "left"
)
joined_mkdf = mousekey_df.join(
    final_df,
    ["emp_id", "cal_date"],
    "left"
)

# 7. Define time windows with special handling for week-offs

jdf_with_windows = joined_df.withColumn(
    "current_window_start",
    F.when(
    (F.col("is_week_off") == True) & (F.col("prev_is_week_off") == False),
    greatest(
        F.expr("cast(prev_end_time_ts as timestamp) + interval 8 hours"),  # previous day's extended shift end
        F.col("cur_start_time_ts")  # current day shift start (likely 00:00:00 on week off)
    )
).when(
    F.col("is_week_off") == True,
    F.col("cur_start_time_ts")  # consecutive week offs
).otherwise(
    F.expr("cast(cur_start_time_ts as timestamp) - interval 4 hours")  # normal working day
)

).withColumn(
    "current_window_end",
    F.when(~F.col("is_week_off"), F.expr("cast(cur_end_time_ts as timestamp) + interval 8 hours"))
	.when(
	(F.col("is_week_off") == True) & (F.col("prev_is_week_off") == False),
	F.concat_ws(" ", F.col("cal_date"),  lit("23:59:59"))
	) 
    .otherwise(F.col("cur_end_time_ts"))
).withColumn(
    "prev_window_start",
    F.when(
        (F.col("prev_date") == F.date_sub(F.col("cal_date"), 1)) &
        ~F.col("prev_is_week_off"),
        F.col("prev_end_time_ts")
    ).otherwise(F.col("prev_start_time_ts"))
).withColumn(
    "prev_window_end",
    F.when(
        (F.col("prev_date") == F.date_sub(F.col("cal_date"), 1)) &
        ~F.col("prev_is_week_off"),
        F.expr("cast(prev_end_time_ts as timestamp) + interval 8 hours")
    ).otherwise(F.col("prev_end_time_ts"))
)





jmkdf_with_windows = joined_mkdf.withColumn(
    "current_window_start",
    F.when(
    (F.col("is_week_off") == True) & (F.col("prev_is_week_off") == False),
    greatest(
        F.expr("cast(prev_end_time_ts as timestamp) + interval 8 hours"), 
        F.col("cur_start_time_ts")  # current day shift start (likely 00:00:00 on week off)
			)
	).when(
		F.col("is_week_off") == True,
		F.col("cur_start_time_ts")  # consecutive week offs
	).otherwise(
		F.expr("cast(cur_start_time_ts as timestamp) - interval 4 hours") )
).withColumn(
    "current_window_end",
    F.when(~F.col("is_week_off"), F.expr("cast(cur_end_time_ts as timestamp) + interval 8 hours"))
	.when(
	(F.col("is_week_off") == True) & (F.col("prev_is_week_off") == False),
	F.concat_ws(" ", F.col("cal_date"),  lit("23:59:59"))
	) 
    .otherwise(F.col("cur_end_time_ts"))
).withColumn(
    "prev_window_start",
    F.when(
        (F.col("prev_date") == F.date_sub(F.col("cal_date"), 1)) &
        ~F.col("prev_is_week_off"),
        F.col("prev_end_time_ts")
		).otherwise(F.col("prev_start_time_ts"))
).withColumn(
    "prev_window_end",
    F.when(
        (F.col("prev_date") == F.date_sub(F.col("cal_date"), 1)) &
        ~F.col("prev_is_week_off"),
        F.expr("cast(prev_end_time_ts as timestamp) + interval 8 hours")
    ).otherwise(F.col("prev_end_time_ts"))
)

# 8. Classify activities with priority to current day
classified_df = jdf_with_windows.withColumn(
    "is_current_login",
    ~F.col("is_week_off") &
    (F.col("start_time") >= F.col("current_window_start")) &
    (F.col("start_time") <= F.col("current_window_end")) &
    (F.col("app_name") != "WindowLock")
).withColumn(
    "is_prev_logout",
    F.col("prev_window_start").isNotNull() &
    (F.col("start_time") >= F.col("prev_window_start")) &
    (F.col("start_time") <= F.col("prev_window_end")) &
    (F.col("start_time") < F.col("current_window_start"))
).withColumn(
    "is_week_off_activity",
    F.col("is_week_off") &
    (F.col("prev_window_start").isNull() | 
     (F.col("start_time") > F.col("prev_window_end")))
)

classified_mkdf = jmkdf_with_windows.withColumn(
    "is_current_login",
    ~F.col("is_week_off") &
    (F.col("event_time") >= F.col("current_window_start")) &
    (F.col("event_time") <= F.col("current_window_end"))
).withColumn(
    "is_prev_logout",
    F.col("prev_window_start").isNotNull() &
    (F.col("event_time") >= F.col("prev_window_start")) &
    (F.col("event_time") <= F.col("prev_window_end")) &
    (F.col("event_time") < F.col("current_window_start"))
).withColumn(
    "is_week_off_activity",
    F.col("is_week_off") &
    (F.col("prev_window_start").isNull() | 
     (F.col("event_time") > F.col("prev_window_end")))
)

# ---------------------------------------------------
# Step 9. Calculate all potential times
# ---------------------------------------------------
result_df = classified_df.groupBy("emp_id", "cal_date").agg(
    # Current login: Minimum start_time within current window and app_name != "Window Lock"
    F.min(F.when(
        (F.col("is_current_login")) & (F.col("app_name") != "WindowLock"),
        F.col("start_time")
    )).alias("current_login"),
    # Last activity: Maximum start_time within current window
    F.max(F.when(
        (F.col("start_time") >= F.col("current_window_start")) &
        (F.col("start_time") <= F.col("current_window_end")),
        F.col("start_time")
    )).alias("last_activity"),
    # Previous logout candidate (normal): Max start_time in previous window
    F.max(F.when(
        (F.col("prev_window_start").isNotNull()) &
        (F.col("start_time") >= F.col("prev_window_start")) &
        (F.col("start_time") < F.col("prev_window_end")) &
        (F.col("start_time") < F.col("current_window_start")) &
        (~F.col("is_week_off_activity")),
        F.col("start_time")
    )).alias("prev_logout_update_norm"),
    # Previous logout candidate (week-off): Max start_time in prev window if week_off_activity
    F.max(F.when(
        (F.col("prev_window_start").isNotNull()) &
        (F.col("start_time") > F.col("prev_window_start")) &
        (F.col("start_time") < F.col("prev_window_end")) &
        (F.col("is_week_off_activity")),
        F.col("start_time")
    )).alias("prev_logout_update_weekoff"),
    # Week-off login: Min start_time if week_off_activity
    F.min(F.when(
        (F.col("is_week_off_activity")) &
        (F.col("app_name") != "WindowLock") &
        (F.col("start_time") > F.col("prev_window_end")) &
        (F.col("start_time") > F.col("current_window_start")),
        F.col("start_time")
    )).alias("week_off_login"),
    # Week-off logout: Max start_time if week_off_activity
    F.max(F.when(
        (F.col("is_week_off_activity")) &
        (F.col("start_time") > F.col("prev_window_end")),
        F.col("start_time")
    )).alias("week_off_logout"),
    # Shift info
    F.first("cur_start_time_ts").alias("shift_start_time"),
    F.first("cur_end_time_ts").alias("shift_end_time"),
    F.first("is_week_off").alias("is_week_off"),
    F.first("prev_date").alias("prev_cal_date")
).withColumn(
    "prev_logout_update",
    F.coalesce(F.col("prev_logout_update_norm"), F.col("prev_logout_update_weekoff"))
)

# Mouse/keyboard side
result_mkdf = classified_mkdf.groupBy("emp_id", "cal_date").agg(
    F.min(F.when(F.col("is_current_login") &
                 (F.col("event_time") >= F.col("current_window_start")) &
                 (F.col("event_time") <= F.col("current_window_end")),
                 F.col("event_time"))).alias("current_login"),
    F.max(F.when(
        (F.col("event_time") >= F.col("current_window_start")) &
        (F.col("event_time") <= F.col("current_window_end")),
        F.col("event_time")
    )).alias("last_mk_activity"),
    F.max(F.when(
        (F.col("prev_window_start").isNotNull()) &
        (F.col("event_time") >= F.col("prev_window_start")) &
        (F.col("event_time") < F.col("prev_window_end")) &
        (~F.col("is_week_off_activity")),
        F.col("event_time")
    )).alias("prev_logout_update_norm"),
    # Previous logout candidate (week-off): Max event_time in prev window if week_off_activity
    F.max(F.when(
        (F.col("prev_window_start").isNotNull()) &
        (F.col("event_time") > F.col("prev_window_start")) &
        (F.col("event_time") < F.col("prev_window_end")) &
        (F.col("is_week_off_activity")),
        F.col("event_time")
    )).alias("prev_logout_update_weekoff"),
    F.min(F.when(
        (F.col("is_week_off_activity")) &
        (F.col("event_time") > F.col("prev_window_end")) &
        (F.col("event_time") > F.col("current_window_start")),
        F.col("event_time")
    )).alias("week_off_login"),
    F.max(F.when(
        (F.col("is_week_off_activity")) &
        (F.col("event_time") > F.col("prev_window_end")),
        F.col("event_time")
    )).alias("week_off_mk_logout"),
    # Shift info
    F.first("cur_start_time_ts").alias("shift_start_time"),
    F.first("cur_end_time_ts").alias("shift_end_time"),
    F.first("is_week_off").alias("is_week_off"),
    F.first("prev_date").alias("prev_cal_date")
)

result_mkdf = result_mkdf.withColumn(
    "prev_logout_mk_update",
    F.coalesce(F.col("prev_logout_update_norm"), F.col("prev_logout_update_weekoff"))
)

# ---------------------------------------------------
# Step 10. Determine final times with priority rules
# ---------------------------------------------------
final_result = result_df.withColumn(
    "emp_login_time",
    F.when(
        F.col("is_week_off"),
        F.coalesce(F.col("week_off_login"), F.col("last_activity"))
    ).otherwise(
        F.coalesce(F.col("current_login"), F.col("last_activity"))
    )
).withColumn(
    "emp_logout_time",
    F.when(
        F.col("is_week_off"),
        F.coalesce(F.col("week_off_logout"), F.col("last_activity"))
    ).otherwise(
        F.coalesce(F.col("last_activity"), F.col("current_login"))
    )
)

final_mkresult = result_mkdf.withColumn(
    "emp_logout_time_mk",
    F.when(
        F.col("is_week_off"),
        F.coalesce(F.col("week_off_mk_logout"), F.col("last_mk_activity"))
    ).otherwise(
        F.coalesce(F.col("last_mk_activity"), F.col("current_login"))
    )
)

# Join MK logout with main result
final_result_with_mk = final_result.join(
    final_mkresult.select("emp_id", "cal_date", "emp_logout_time_mk", "prev_logout_mk_update"),
    ["emp_id", "cal_date"],
    "left"
)

# ---------------------------------------------------
# Step 11. Generate previous day updates (both sources)
# ---------------------------------------------------
prev_day_updates = final_result_with_mk.filter(
    F.col("prev_logout_update").isNotNull() | F.col("prev_logout_mk_update").isNotNull()
).select(
    F.col("emp_id").alias("update_emp_id"),
    F.col("prev_cal_date").alias("update_date"),
    F.coalesce(F.col("prev_logout_mk_update"), F.col("prev_logout_update")).alias("new_logout_time")
)

# Final output: prefer MK logout if available
final_output = final_result_with_mk.select(
    "emp_id",
    "cal_date",
    F.coalesce(F.col("emp_logout_time_mk"), F.col("emp_logout_time")).alias("emp_logout_time"),
    "emp_login_time",
    "shift_start_time",
    "shift_end_time",
    "is_week_off"
).orderBy("emp_id", "cal_date")

# Filter out null records
filtered_login_logout = final_output.filter(
    (F.col("emp_id").isNotNull()) &
    (F.col("cal_date").isNotNull()) &
    (F.col("emp_login_time").isNotNull()) &
    (F.col("emp_logout_time").isNotNull()) &
	(F.col("emp_logout_time") >= F.col("emp_login_time"))
)

# Write to Delta table
spark.sql("""
CREATE TABLE IF NOT EXISTS gold_dashboard.analytics_emp_login_logout (
    EMP_ID int,
    EMP_CONTRACTED_HOURS string,
    START_TIME string,
    END_TIME string,
    START_TIME_THRESHOLD string,
    END_TIME_THRESHOLD string,
    EMP_LOGIN_TIME string,
    EMP_LOGOUT_TIME string,
    SHIFT_DATE date,
    SHIFT_COMPLETED string,
    ATTENDENCE_STATUS string,
    LOGIN_STATUS string,
    LOGOUT_STATUS string,
    WORKING_HOURS float,
    UPDATED_ON TIMESTAMP,
    IS_WEEK_OFF BOOLEAN GENERATED ALWAYS AS (
        CASE WHEN unix_timestamp(END_TIME) - unix_timestamp(START_TIME) <= 0 THEN TRUE ELSE FALSE END
    ),
    IS_HOLIDAY BOOLEAN
) USING DELTA
""")

spark.sql("""ALTER TABLE gold_dashboard.analytics_emp_login_logout ALTER COLUMN UPDATED_ON SET DEFAULT current_timestamp()""")
spark.sql("""ALTER TABLE gold_dashboard.analytics_emp_login_logout ALTER COLUMN IS_HOLIDAY SET DEFAULT FALSE""")

# Merge data into target table
filtered_login_logout.createOrReplaceTempView("temp_filtered_login_logout")
spark.sql("""
MERGE INTO gold_dashboard.analytics_emp_login_logout AS target
USING temp_filtered_login_logout AS source
ON target.EMP_ID = source.emp_id AND target.SHIFT_DATE = source.cal_date
WHEN MATCHED AND target.EMP_LOGIN_TIME IS NOT NULL THEN
    UPDATE SET 
        target.EMP_LOGIN_TIME = CASE 
            WHEN source.emp_login_time < target.EMP_LOGIN_TIME THEN source.emp_login_time 
            ELSE target.EMP_LOGIN_TIME 
        END,
        target.EMP_LOGOUT_TIME = source.emp_logout_time,
        target.START_TIME = source.shift_start_time,
        target.END_TIME = source.shift_end_time
WHEN NOT MATCHED THEN
    INSERT (EMP_ID, START_TIME, END_TIME, EMP_LOGIN_TIME, EMP_LOGOUT_TIME, SHIFT_DATE)
    VALUES (source.emp_id, source.shift_start_time, source.shift_end_time,
            source.emp_login_time, source.emp_logout_time, source.cal_date)
""")

# Update previous day's logout times if needed
prev_day_updates.createOrReplaceTempView("temp_prev_day_updates")
spark.sql("""
MERGE INTO gold_dashboard.analytics_emp_login_logout AS target
USING temp_prev_day_updates AS source
ON target.EMP_ID = source.update_emp_id AND target.SHIFT_DATE = source.update_date
WHEN MATCHED THEN 
    UPDATE SET target.EMP_LOGOUT_TIME = source.new_logout_time
""")