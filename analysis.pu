Subject: Urgent: Fix for Databricks Connection Memory Issues Causing Worker Crashes

Hi Team,

I'm sharing a detailed analysis of the memory issues that caused our workers to crash, along with the complete solution. Please read this carefully as it affects our production stability.

ğŸ” What Happened: Worker Crash Analysis

From our logs:

```
WORKER TIMEOUT (pid:1120)
Worker (pid:1121) was sent SIGKILL! Perhaps out of memory
Worker (pid:1125) was sent SIGKILL! Perhaps out of memory
```

Root Cause: Our Databricks connection pool was consuming excessive memory, triggering the OS Out-of-Memory (OOM) killer to terminate workers.

ğŸ§  The Perfect Storm: How We Got Here

1. Async Workers Misunderstanding

We're using Uvicorn async workers, which can handle 100+ requests concurrently per worker. This is great for performance but dangerous for memory.

2. Memory Math That Broke Us

```python
# OLD DANGEROUS SETUP:
WORKERS_PER_INSTANCE = 8
POOL_SIZE = 10
MEMORY_PER_CONNECTION = 75MB  # Databricks connections are heavy

# Calculation:
8 workers Ã— 10 connections Ã— 75MB = 6,000MB = 6GB
+ Python runtime: 500MB
+ Application: 200MB
+ Gunicorn: 300MB
+ OS: 500MB
---
TOTAL: 7.5GB+ â† Exceeded our container limits!
```

3. Async Multiplication Effect

Since each async worker handles 100+ requests, but we thought it was 1 request per worker:

Â· We expected: 8 workers Ã— 1 request = 8 connections
Â· Reality: 8 workers Ã— 100 requests Ã— 10 pool size = 8,000 potential connections!

ğŸ“Š The Technical Breakdown

Sync vs Async Workers:

 Sync Workers Async Workers
Requests per worker 1 100+
Connection needs Low Very High
Memory risk Low Very High

Our Mistaken Assumption:

We configured connection pools as if each worker only handled one request, but with async workers, each worker handles many requests simultaneously.

ğŸš€ The Complete Solution

1. New Safe Configuration

```python
# databricks_pool.py
POOL_SIZE = 3  # â†“ FROM 10 to 3 (70% reduction)
CONNECTION_TTL = 1800  # 30-minute reset

# gunicorn command
gunicorn -w 2 -k uvicorn.workers.UvicornWorker app:app  # â†“ FROM 8 to 2 workers
```

2. Memory Monitoring Added

```python
import psutil
import logging

def check_memory():
    """Monitor memory usage and prevent OOM kills"""
    memory = psutil.virtual_memory()
    if memory.percent > 80:
        logging.warning(f"ğŸš¨ High memory: {memory.percent}%")
        # Automatic safety measures can be added
```

3. Async-Aware Connection Limiting

```python
from asyncio import Semaphore

# Limit concurrent Databricks queries per instance
MAX_CONCURRENT_QUERIES = 20
query_semaphore = Semaphore(MAX_CONCURRENT_QUERIES)

@app.get("/api/endpoint")
async def api_endpoint():
    async with query_semaphore:  # â† Prevents async overload
        with DatabricksSession(api_name="endpoint") as conn:
            # ... your query
```

ğŸ“ˆ New Safe Memory Math

```python
# NEW SAFE SETUP:
INSTANCES = 6
WORKERS_PER_INSTANCE = 2    # â†“ From 8
POOL_SIZE = 3               # â†“ From 10
MEMORY_PER_CONNECTION = 75MB

# Total connections: 6 instances Ã— 3 connections = 18 connections
# Memory usage: 18 Ã— 75MB = 1.35GB â† Manageable!

# Concurrent requests: 6 instances Ã— 2 workers Ã— 100 async = 1,200 requests!
```

ğŸ¯ Why This Works:

1. Shared Pool Understanding: Connection pool is per instance, not per worker
2. Async Awareness: We now account for async concurrency in our calculations
3. Memory Safety: Drastic reduction in potential memory usage
4. Monitoring: Early warning system for memory issues

ğŸ”§ Implementation Plan

Phase 1: Immediate Fix (Today)

```python
# 1. Reduce pool size to 3
POOL_SIZE = 3

# 2. Reduce workers to 2 per instance
gunicorn -w 2 -k uvicorn.workers.UvicornWorker app:app

# 3. Add memory monitoring
```

Phase 2: Enhanced Protection (Next Week)

Â· Add automatic connection reduction during high memory
Â· Implement circuit breaker for Databricks
Â· Add detailed metrics and alerts

Phase 3: Optimization (Ongoing)

Â· Fine-tune based on actual usage metrics
Â· Implement connection pooling with health checks
Â· Add retry mechanisms with backoff

ğŸ“Š Expected Results

Before:

```
ğŸš¨ WORKER TIMEOUT (pid:1120)
ğŸš¨ Worker was sent SIGKILL! Perhaps out of memory
```

After:

```
âœ… CONN_EXIT: leadsummary â†’ pool (pool:2/3) Req:150 New:12 Hit:138
âœ… CONN_EXIT: userstats â†’ pool (pool:3/3) Req:151 New:12 Hit:139
```

ğŸš¨ Monitoring Checklist

1. Memory usage below 80%
2. No OOM kills in logs
3. Pool hit rate above 90%
4. Databricks response times stable
5. No HTTP 429 (throttling) errors

ğŸ’¡ Key Takeaways

1. Async workers multiply connection needs - they handle many requests concurrently
2. Connection pools are shared per instance - not per worker
3. Databricks connections are memory-heavy - ~75MB each
4. We must configure for worst-case scenarios - not ideal cases

ğŸ†˜ When to Escalate

If you see:

Â· Memory usage consistently above 85%
Â· Frequent OOM kills despite changes
Â· Databricks throttling (HTTP 429)
Â· Response times increasing dramatically

This fix should resolve our stability issues while maintaining performance. The changes are deployed and monitoring is in place.

Best regards,
[Your Name]
[Your Position]

P.S. This wasn't a code error but a configuration misunderstanding of how async workers interact with connection pooling. The solution makes us both stable and more efficient!