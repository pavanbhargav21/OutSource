Subject: Urgent: Fix for Databricks Connection Memory Issues Causing Worker Crashes

Hi Team,

I'm sharing a detailed analysis of the memory issues that caused our workers to crash, along with the complete solution. Please read this carefully as it affects our production stability.

🔍 What Happened: Worker Crash Analysis

From our logs:

```
WORKER TIMEOUT (pid:1120)
Worker (pid:1121) was sent SIGKILL! Perhaps out of memory
Worker (pid:1125) was sent SIGKILL! Perhaps out of memory
```

Root Cause: Our Databricks connection pool was consuming excessive memory, triggering the OS Out-of-Memory (OOM) killer to terminate workers.

🧠 The Perfect Storm: How We Got Here

1. Async Workers Misunderstanding

We're using Uvicorn async workers, which can handle 100+ requests concurrently per worker. This is great for performance but dangerous for memory.

2. Memory Math That Broke Us

```python
# OLD DANGEROUS SETUP:
WORKERS_PER_INSTANCE = 8
POOL_SIZE = 10
MEMORY_PER_CONNECTION = 75MB  # Databricks connections are heavy

# Calculation:
8 workers × 10 connections × 75MB = 6,000MB = 6GB
+ Python runtime: 500MB
+ Application: 200MB
+ Gunicorn: 300MB
+ OS: 500MB
---
TOTAL: 7.5GB+ ← Exceeded our container limits!
```

3. Async Multiplication Effect

Since each async worker handles 100+ requests, but we thought it was 1 request per worker:

· We expected: 8 workers × 1 request = 8 connections
· Reality: 8 workers × 100 requests × 10 pool size = 8,000 potential connections!

📊 The Technical Breakdown

Sync vs Async Workers:

 Sync Workers Async Workers
Requests per worker 1 100+
Connection needs Low Very High
Memory risk Low Very High

Our Mistaken Assumption:

We configured connection pools as if each worker only handled one request, but with async workers, each worker handles many requests simultaneously.

🚀 The Complete Solution

1. New Safe Configuration

```python
# databricks_pool.py
POOL_SIZE = 3  # ↓ FROM 10 to 3 (70% reduction)
CONNECTION_TTL = 1800  # 30-minute reset

# gunicorn command
gunicorn -w 2 -k uvicorn.workers.UvicornWorker app:app  # ↓ FROM 8 to 2 workers
```

2. Memory Monitoring Added

```python
import psutil
import logging

def check_memory():
    """Monitor memory usage and prevent OOM kills"""
    memory = psutil.virtual_memory()
    if memory.percent > 80:
        logging.warning(f"🚨 High memory: {memory.percent}%")
        # Automatic safety measures can be added
```

3. Async-Aware Connection Limiting

```python
from asyncio import Semaphore

# Limit concurrent Databricks queries per instance
MAX_CONCURRENT_QUERIES = 20
query_semaphore = Semaphore(MAX_CONCURRENT_QUERIES)

@app.get("/api/endpoint")
async def api_endpoint():
    async with query_semaphore:  # ← Prevents async overload
        with DatabricksSession(api_name="endpoint") as conn:
            # ... your query
```

📈 New Safe Memory Math

```python
# NEW SAFE SETUP:
INSTANCES = 6
WORKERS_PER_INSTANCE = 2    # ↓ From 8
POOL_SIZE = 3               # ↓ From 10
MEMORY_PER_CONNECTION = 75MB

# Total connections: 6 instances × 3 connections = 18 connections
# Memory usage: 18 × 75MB = 1.35GB ← Manageable!

# Concurrent requests: 6 instances × 2 workers × 100 async = 1,200 requests!
```

🎯 Why This Works:

1. Shared Pool Understanding: Connection pool is per instance, not per worker
2. Async Awareness: We now account for async concurrency in our calculations
3. Memory Safety: Drastic reduction in potential memory usage
4. Monitoring: Early warning system for memory issues

🔧 Implementation Plan

Phase 1: Immediate Fix (Today)

```python
# 1. Reduce pool size to 3
POOL_SIZE = 3

# 2. Reduce workers to 2 per instance
gunicorn -w 2 -k uvicorn.workers.UvicornWorker app:app

# 3. Add memory monitoring
```

Phase 2: Enhanced Protection (Next Week)

· Add automatic connection reduction during high memory
· Implement circuit breaker for Databricks
· Add detailed metrics and alerts

Phase 3: Optimization (Ongoing)

· Fine-tune based on actual usage metrics
· Implement connection pooling with health checks
· Add retry mechanisms with backoff

📊 Expected Results

Before:

```
🚨 WORKER TIMEOUT (pid:1120)
🚨 Worker was sent SIGKILL! Perhaps out of memory
```

After:

```
✅ CONN_EXIT: leadsummary → pool (pool:2/3) Req:150 New:12 Hit:138
✅ CONN_EXIT: userstats → pool (pool:3/3) Req:151 New:12 Hit:139
```

🚨 Monitoring Checklist

1. Memory usage below 80%
2. No OOM kills in logs
3. Pool hit rate above 90%
4. Databricks response times stable
5. No HTTP 429 (throttling) errors

💡 Key Takeaways

1. Async workers multiply connection needs - they handle many requests concurrently
2. Connection pools are shared per instance - not per worker
3. Databricks connections are memory-heavy - ~75MB each
4. We must configure for worst-case scenarios - not ideal cases

🆘 When to Escalate

If you see:

· Memory usage consistently above 85%
· Frequent OOM kills despite changes
· Databricks throttling (HTTP 429)
· Response times increasing dramatically

This fix should resolve our stability issues while maintaining performance. The changes are deployed and monitoring is in place.

Best regards,
[Your Name]
[Your Position]

P.S. This wasn't a code error but a configuration misunderstanding of how async workers interact with connection pooling. The solution makes us both stable and more efficient!